[
["index.html", "Exploring Enterprise Databases with R: A Tidyverse Approach Chapter 1 Introduction 1.1 Using R to query a DBMS in your organization 1.2 Docker as a tool for UseRs 1.3 Alternatives to Docker 1.4 Packages used in this book 1.5 Who are we? 1.6 How did this project come about? 1.7 Navigation", " Exploring Enterprise Databases with R: A Tidyverse Approach John David Smith, Sophie Yang, M. Edward (Ed) Borasky, Jim Tyhurst, Scott Came, Mary Anne Thygesen, and Ian Frantz 2019-12-06 Chapter 1 Introduction This chapter introduces: The motivation for this book and the strategies we have adopted Our approach to exploring issues “beyind the enterprise firewall” using Docker to demonstrate access to a service like PostgreSQL from R Our team and how this project came about 1.1 Using R to query a DBMS in your organization Many R users (or useRs) live a dual life: in the vibrant open-source R community where R is created, improved, discussed, and taught. And then they go to work in a secured, complex, closed organizational environment where they may be on their own. Here is a request on the Rstudio community site for help that has been lightly edited to emphasize the generality that we see: I’m trying to migrate some inherited scripts that […] to connect to a […] database to […] instead. I’ve reviewed the https://db.rstudio.com docs and tried a number of configurations but haven’t been able to connect. I’m in uncharted territory within my org, so haven’t been able to get much help internally. This book will help you create a hybrid environment on your machine that can mimic some of the uncharted territory in your organization. It goes far beyond the basic connection issues and covers issues that you face when you are finding your way around or writing queries to your organization’s databases, not just when maintaining inherited scripts. Technology hurdles. The interfaces (passwords, packages, etc.) and gaps between R and a back end database are hidden from public view as a matter of security, so pinpointing exactly where a problem is can be difficult. A simulated environment such as we offer here can be an important learning resource. Scale issues. We see at least two types of scale issues. Handling large volumes of data so that performance issues must be a consideration requires a basic understanding of what’s happening in “the back end” (which is necessarily hidden from view). Therefore mastering techniques for drawing samples or small batches of data are essential. In addition to their size, your organization’s databases will often have structural characteristics that are complex and obscure. Data documentation is often incomplete and emphasizes operational characteristics, rather than analytic opportunities. A careful useR often needs to confirm the documentation on the fly and de-normalize data carefully. Use cases. R users frequently need to make sense of an organization’s complex data structures and coding schemes to address incompletely formed questions so that informal exploratory data analysis has to be intuitive and fast. The technology details should not get in the way. Sharing and discussing exploratory and diagnostic retrieval techniquesis best in public, but is constrained by organizational requirements. We have found that PostgreSQL in a Docker container solves many of the foregoing problems. 1.2 Docker as a tool for UseRs Noam Ross’s “Docker for the UseR” (Ross 2018a) suggests that there are four distinct Docker use-cases for useRs. Make a fixed working environment for reproducible analysis Access a service outside of R (e.g., PostgreSQL) Create an R based service (e.g., with plumber) Send our compute jobs to the cloud with minimal reconfiguration or revision This book explores #2 because it allows us to work on the database access issues described above and to practice on an industrial-scale DBMS. Docker is a comparatively easy way to simulate the relationship between an R/RStudio session and a database – all on on your machine (provided you have Docker installed and running). Running PostgreSQL on a Docker container avoids OS or system dependencies or conflicts that cause confusion and limit reproducibility. A Docker environment consumes relatively few resources. Our sandbox does much less but only includes PostgreSQL and sample data, so it takes up about 5% of the space taken up by the Vagrant environment that inspired this project. (Makubuya 2018) A simple Docker container such as the one used in our sandbox is easy to use and could be extended for other uses. Docker is a widely used technology for deploying applications in the cloud, so for many useRs it’s worth mastering. 1.3 Alternatives to Docker We have found Docker to be a great tool for simulating the complexities of an enterprise environment. However, installing Docker can be challenging, especially for Windows users. Therefore the code in this book depends on PostgreSQL(Group 2019) in a Docker container, but it can all be readily adapted to either SQLite(Consortium 2019), PostgreSQL running natively on your computer, or even PostgreSQL running in the cloud. The technical details of these alternatives are all in separate chapters. 1.4 Packages used in this book The following packages are used in this book: bookdown DBI dbplyr devtools DiagrammeR downloader glue here knitr RPostgres skimr sqlpetr (installs with: remotes::install_github(&quot;smithjd/sqlpetr&quot;, force = TRUE, quiet = TRUE, build = TRUE, build_opts = &quot;&quot;)) tidyverse 1.5 Who are we? We have been collaborating on this book since the Summer of 2018, each of us chipping into the project as time permits: Ian Franz - @ianfrantz Jim Tyhurst - @jimtyhurst John David Smith - @smithjd M. Edward (Ed) Borasky - @znmeb Maryanne Thygesen @maryannet Scott Came - @scottcame Sophie Yang - @SophieMYang 1.6 How did this project come about? We trace this book back to the June 2, 2018 Cascadia R Conf where Aaron Makubuya gave a presentation using Vagrant hosting (Makubuya 2018). After that John Smith, Ian Franz, and Sophie Yang had discussions after the monthly Data Discussion Meetups about the difficulties around setting up Vagrant (a virtual environment), connecting to an enterprise database, and having realistic public environment to demo or practice the issues that come up behind corporate firewalls. Scott Came’s tutorial on R and Docker (Came 2018) (an alternative to Vagrant) at the 2018 UseR Conference in Melbourne was provocative and it turned out he lived nearby. We re-connected with M. Edward (Ed) Borasky who had done extensive development for a Hack Oregon data science containerization project (Borasky 2018). 1.7 Navigation If this is the first bookdown (Xie 2016) book you’ve read, here’s how to navigate the website. The controls on the upper left: there are four controls on the upper left. A “hamburger” menu: this toggles the table of contents on the left side of the page on or off. A magnifying glass: this toggles a search box on or off. A letter “A”: this lets you pick how you want the site to display. You have your choice of small or large text, a serif or sans-serif font, and a white, sepia or night theme. A pencil: this is the “Edit” button. This will take you to a GitHub edit dialog for the chapter you’re reading. If you’re a committer to the repository, you’ll be able to edit the source directly. If not, GitHub will fork a copy of the repository to your own account and you’ll be able to edit that version. Then you can make a pull request. The share buttons in the upper right hand corner. There’s one for Twitter, one for Facebook, and one that gives a menu of options, including LinkedIn. References "],
["chapter-how-to-use-this-book.html", "Chapter 2 How to use this book 2.1 Retrieve the code from GitHub 2.2 Read along, experiment as you go 2.3 Participating", " Chapter 2 How to use this book This chapter explains: Getting the code used in this book How you can contribute to the book project This book is full of examples that you can replicate on your computer. 2.1 Retrieve the code from GitHub The code to generate the book and the exercises it contains can be downloaded from this repo. 2.2 Read along, experiment as you go We have never been sure whether we’re writing an expository book or a massive tutorial. You may use it either way. The best way to learn the material we cover is to experiment. After the introductory chapters and the chapter that creates the persistent database, you can jump around and each chapter stands on its own. 2.3 Participating 2.3.1 Browsing the book If you just want to read the book and copy / paste code into your working environment, simply browse to https://smithjd.github.io/sql-pet. If you get stuck, or find things aren’t working, open an issue at https://github.com/smithjd/sql-pet/issues/new/. 2.3.2 Diving in If you want to experiment with the code in the book, run it in RStudio and interact with it, you’ll need to do two more things: Install the sqlpetr R package (Borasky et al. 2018). See https://smithjd.github.io/sqlpetr for the package documentation. Installation may take some time if it has to install or update packages not available on your computer. Clone the Git repository https://github.com/smithjd/sql-pet.git and open the project file sql-pet.Rproj in RStudio. Enjoy! References "],
["chapter-learning-goals.html", "Chapter 3 Chapter Learning Goals and Use Cases 3.1 The Book’s Challenge: goals, context and expectations 3.2 Making your way through the book 3.3 Adventure Works", " Chapter 3 Chapter Learning Goals and Use Cases This chapter sets the context for the book by: Describing our assumptions about the reader of this book: the challenges you face, your R skills, your learning goals, and context. Describing what the book offers in terms of: Problems that are addressed Learning objectives Sequence of topics, ranging from connecting to the database to exploring an issue in response to questions from an executive R packages used Describing the sample database used in the book 3.1 The Book’s Challenge: goals, context and expectations Working with the data that’s behind the enterprise firewall is challenging in a unique way. Most of us R users are accustomed to a vast learning community that shares resources, discusses methods in public, and that can help each other trouble-shoot a problem. The very necessary enterprise firewall makes all of that difficult, if not impossible. And yet enterprise database environment is very important because in so many cases that’s where the data (and possibly your paycheck) are coming from. Differences between production and data warehouse environments. We are simulating a production environment. There are many similarities. Data models are different. Performance is a bigger deal in the OLTP. Data in a organizational environment around the database. Learning to keep your DBAs happy: You are your own DBA in this simulation, so you can wreak havoc and learn from it, but you can learn to be DBA-friendly here. In the end it’s the subject-matter experts (people using the data every day) that really understand your data, but you have to work with your DBAs first. You can’t believe all the data you pull out of the database. 3.1.1 The Challenge: Investigating a question using an organization’s database Using an enterprise database to create meaningful management insights requires a combination of very different skills: Need both familiarity with the data and a focus question An iterative process where the data resource can shape your understanding of the question the question you need to answer will frame how you see the data resource You need to go back and forth between the two, asking do I understand the question? do I understand the data? A “good enough” understanding of the data resource (in the DBMS) Nobody knows everything about an entire organization’s data resources. We do, however, need to know what more we need to know and estimate what we don’t know yet. Use all available documentation and understand its limits Use your own tools and skills to examine the data resource What is missing from the database: (columns, records, cells) Why is the data missing? A “good enough” understanding of the question you seek to answer How general or specific is your question? How aligned is it with the purpose for which the database was designed and is being operated? How different are your assumptions and concerns from those of the people who enter and use the data on a day to day basis? Some cycles in this iteration between question refinement and reformulation on the one hand and data retrieval and investigation on the other feel like a waste time. That’s inevitable. Bringing R tools and skills to bear on these R is a powerful tool for data access, manipulation, modeling and presentation Different R packages and techniques are available for each of the elements involved in exploring, analyzing and reporting on enterprise behavior using the enterprise database. 3.1.2 Strategies Local, idiosyncratic optimization (entry and use of data). For example, different individuals might code a variable differently. Drifting use / bastardization of a column Turf wars and acquisitions Partial recollection / history: find the people who know where the skeletons are 3.1.3 Problems that we address in the book This book emphasizes database exploration and the R techniques that are needed. We are emphasizing a tidyverse approach. &amp; graphics to really makes sense of what we find. We can’t call on real people in the adventureworks company, obviously, but we invent some characters to illustrate the investigation process as we have experienced it in various organizational settings. 3.1.4 Signposts Practice Tips Here’s how we do it. + Conventions like always using the labs() function in ggplot + Specifying the package the first time a function is used 3.1.5 Book structure The book explores R techniques and and investigation strategies using progressively more complex queries, that lead to this scenario: There is a new Executive VP of Sales at Adventure Works. She wants an overview of sales and the sales organization’s performance at Adventure Works. Once her questions are satisfied, a monthly report is developed that can run automatically and appear in her mailbox. Early chapters demonstrate now to connect to a database and find your way around it, with a pause to discuss how to secure your credentials. Both Rstudio and R script methods are shown for the same database overview. The salesordedrheader table in the sales schema is used to demonstrate packages and functions that show what a single table contains. Then the same table is used but the investigation adopts a business perspective, demonstrating R techniques that are motivated by questions like “How sales for the Adventure Works company?” Starting with base tables, then use views (that contain knowledge about the application) More involved queries join three tables in three different schemas: salesperson, employee, and person. The relevant question might be “Who is my top salesperson? Are the 3 top salespersons older or younger?” Finally, we build a series of queries that explore the sales workflow: sales territories, sales people, top customers by product, product mixture that gives top 80% of sales. What are they producing in detail? Seasonal? Type of product, region, etc.? The book ends by demonstrating how R code can be used for standard reports from the database that are emailed to a list of recipients. 3.2 Making your way through the book After working through the code in this book, you can expect to be able to: R, SQL and PostgreSQL Run queries against PostgreSQL in an environment that simulates what is found in a enterprise setting. Understand techniques and some of the trade-offs between: queries aimed at exploration or informal investigation using dplyr (Wickham 2018); and queries that should be written in SQL, because performance is important due to the size of the database or the frequency with which a query is to be run. Understand the equivalence between dplyr and SQL queries, and how R translates one into the other. Gain familiarity with techniques that help you explore a database and verify its documentation. Gain familiarity with the standard metadata that a SQL database contains to describe its own contents. Understand some advanced SQL techniques. Gain some understanding of techniques for assessing query structure and performance. Docker related Set up a PostgreSQL database in a Docker environment. Gain familiarity with the various ways of interacting with the Docker and PostgreSQL environments Understand enough about Docker to swap databases, e.g. Sports DB for the DVD rental database used in this tutorial. Or swap the database management system (DBMS), e.g. MySQL for PostgreSQL. 3.2.1 R Packages These R packages are discussed or used in exercises: DBI dbplyr devtools downloader glue gt here knitr RPostgres skimr sqlpetr (installs with: remotes::install_github(&quot;smithjd/sqlpetr&quot;, force = TRUE, quiet = TRUE, build = TRUE, build_opts = &quot;&quot;)) tidyverse In addition, these are used to render the book: * bookdown * DiagrammeR 3.3 Adventure Works In this book we have adopted the Microsoft Adventure Works online transaction processing database for our examples. It is https://docs.microsoft.com/en-us/previous-versions/sql/sql-server-2008/ms124438(v=sql.100) See Sections 3 and 4 Journal of Information Systems Education, Vol. 26(3) Summer 2015. “Teaching Tip Active Learning via a Sample Database: The Case of Microsoft’s Adventure Works” by Michel Mitri http://jise.org/Volume26/n3/JISEv26n3p177.pdf See the AdventureWorks Data Dictionary and a sample table (employee). Here is a (link to an ERD diagram)[https://i.stack.imgur.com/LMu4W.gif] References "],
["chapter-setup-adventureworks-db.html", "Chapter 4 Create and connect to the adventureworks database in PostgreSQL 4.1 Overview 4.2 Verify that Docker is up, running, and clean up if necessary 4.3 Clean up if appropriate 4.4 Build the adventureworks Docker image 4.5 Run the adventureworks Docker Image 4.6 Connect to PostgreSQL 4.7 Adventureworks Schemas 4.8 Investigate the database using Rstudio 4.9 Cleaning up: diconnect from the database and stop Docker 4.10 Using the adventureworks container in the rest of the book", " Chapter 4 Create and connect to the adventureworks database in PostgreSQL This chapter demonstrates how to: Create and connect to the PostgreSQL adventureworks database in Docker Keep necessary credentials secret while being available to R when it executes. Leverage Rstudio features to get an overview of the database Set up the environment for subsequent chapters 4.1 Overview Docker commands can be run from a terminal (e.g., the Rstudio Terminal pane) or with a system2() command. The necessary functions to start, stop Docker containers and do other busy work are provided in the sqlpetr package. Note: The functions in the package are designed to help you focus on interacting with a dbms from R. You can ignore how they work until you are ready to delve into the details. They are all named to begin with sp_. The first time a function is called in the book, we provide a note explaining its use. Please install the sqlpetr package if not already installed: library(devtools) if (!require(sqlpetr)) { remotes::install_github( &quot;smithjd/sqlpetr&quot;, force = TRUE, build = FALSE, quiet = TRUE) } Note that when you install this package the first time, it will ask you to update the packages it uses and that may take some time. These packages are called in this Chapter: library(tidyverse) library(DBI) library(RPostgres) library(glue) require(knitr) library(dbplyr) library(sqlpetr) library(bookdown) library(here) 4.2 Verify that Docker is up, running, and clean up if necessary The sp_check_that_docker_is_up function from the sqlpetr package checks whether Docker is up and running. If it’s not, then you need to install, launch or re-install Docker. sp_check_that_docker_is_up() ## [1] &quot;Docker is up, running these containers:&quot; ## [2] &quot;CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES&quot; ## [3] &quot;f6f9da5ee4bc postgres:11 \\&quot;docker-entrypoint.s…\\&quot; 13 days ago Up 2 days 0.0.0.0:5432-&gt;5432/tcp adventureworks&quot; 4.3 Clean up if appropriate Force-remove the adventureworks container if it was left over (e.g., from a prior runs): sp_docker_remove_container(&quot;adventureworks&quot;) ## [1] 0 4.4 Build the adventureworks Docker image Now we set up a “realistic” database named adventureworks in Docker. NOTE: This chapter doesn’t go into the details of creating or restoring the adventureworks database. For more detail on what’s going on behind the scenes, you can examine the step-by-step code in: source('book-src/restore-adventureworks-postgres-on-docker.R') To save space here in the book, we’ve created a function in sqlpetr to build this image, called OUT OF DATE!! . Vignette Building the adventureworks Docker Image describes the build process. *Ignore the errors in the following step: source(here(&quot;book-src&quot;, &quot;restore-adventureworks-postgres-on-docker.R&quot;)) ## docker run --detach --name adventureworks --publish 5432:5432 --mount type=bind,source=&quot;/Users/jds/Documents/Library/R/r-system/sql-pet&quot;,target=/petdir postgres:11 4.5 Run the adventureworks Docker Image Now we can run the image in a container and connect to the database. To run the image we use an sqlpetr function called OUT OF DATE sp_pg_docker_run For the rest of the book we will assume that you have a Docker container called adventureworks that can be stopped and started. In that sense each chapter in the book is independent. sp_docker_start(&quot;adventureworks&quot;) 4.6 Connect to PostgreSQL *CHECK for sqlpetr update!Thesp_make_simple_pgfunction we called above created a container from thepostgres:11library image downloaded from Docker Hub. As part of the process, it set the password for the PostgreSQL database superuserpostgres` to the value “postgres”. For simplicity, we are using a weak password at this point and it’s shown here and in the code in plain text. That is bad practice because user credentials should not be shared in open code like that. A subsequent chapter demonstrates how to store and use credentials to access the DBMS so that they are kept private. The sp_get_postgres_connection function from the sqlpetr package gets a DBI connection string to a PostgreSQL database, waiting if it is not ready. This function connects to an instance of PostgreSQL and we assign it to a symbol, con, for subsequent use. The connctions_tab = TRUE parameter opens a connections tab that’s useful for navigating a database. Note that we are using port 5439 for PostgreSQL inside the container and published to localhost. Why? If you have PostgreSQL already running on the host or another container, it probably claimed port 5432, since that’s the default. So we need to use a different port for our PostgreSQL container. Use the DBI package to connect to the adventureworks database in PostgreSQL. Remember the settings discussion about [keeping passwords hidden][Pause for some security considerations] con &lt;- sp_get_postgres_connection( host = &quot;localhost&quot;, port = 5432, # this version still using 5432!!! user = &quot;postgres&quot;, password = &quot;postgres&quot;, dbname = &quot;adventureworks&quot;, seconds_to_test = 20, connection_tab = TRUE ) 4.7 Adventureworks Schemas Think of the Adventureworks database as a model of the Adventureworks business. The business is organized around different departments (humanresources, sales, and purchasing), business processes (production), and resources (person). Each schema is a container for the all the database objects needed to model the departments, business processes, and resources. As a data analyst, the connections tab has three of the five database objects of interest. These are schemas, tables and views. The other two database objects of interest not shown in the connetions tab are the table primary and foreign keys, PK and FK. Those database objects enforce the referential integrity of the data and the performance of the application. Let the DBA’s worry about them. The Connections tab has three icons. The node icon represents a schema. The schema helps organize the structure and design of the database. The schema contains the views, the grid with the glasses, and tables, the grids without the glasses, that are of interest to the data analyst. A table is a database object usually represents something useful to a business process. For example, a sales person may enter a new order. The first screen is typically called the sales order header screen which contains information about the customer placing the order. This information is captured in salesorderheader table. The customers ordered items are typically entered via multiple screens. These are captured in the salesorderdetail table. A view is a database object that maybe a subset of either the columns or rows of a single table. For example, the customer table has information on all the customers, but the customer view, c, shows only a single customer. Or a view may have data from a primary/driving table and joined to other tables to provide a better understanding/view of the information in the primary table. For example, the primary table typically has a primary key column, PK, and zero or more foreign key columns, FK. The PK and FK are usually an integer which is great for a computer, but not so nice us mere mortals. An extended view pulls information associated with the FK. For example a sales order view a customer foreign key, can show the actual customer name. 4.8 Investigate the database using Rstudio The Rstudio Connections tab shows that you are connected to Postgres and that the adventureworks database has a many schemas each of which has multiple tables and views in it. The drop-down icon to the left of a table lists the table’s columns. Connections tab - adventureworks Clicking on the icon to the left of a schema expands the list of tables and views in that schema. Clicking on the View or Table icon opens up Rstudio’s View pane to get a peek at the data: View of employee table The number of rows and columns shown in the View pane depends on the size of the window. Disconnect from the database: dbDisconnect(con) 4.9 Cleaning up: diconnect from the database and stop Docker Always have R disconnect from the database when you’re done. dbDisconnect(con) ## Warning in connection_release(conn@ptr): Already disconnected Stop the adventureworks container: sp_docker_stop(&quot;adventureworks&quot;) Show that the container still exists even though it’s not running sp_show_all_docker_containers() ## CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES ## 8c8345abba76 postgres:11 &quot;docker-entrypoint.s…&quot; 20 seconds ago Exited (0) Less than a second ago adventureworks Next time, you can just use this command to start the container: sp_docker_start(&quot;adventureworks&quot;) And once stopped, the container can be removed with: sp_check_that_docker_is_up(&quot;adventureworks&quot;) 4.10 Using the adventureworks container in the rest of the book After this point in the book, we assume that Docker is up and that we can always start up our adventureworks database with: sp_docker_start(&quot;adventureworks&quot;) "],
["chapter-dbms-login-credentials.html", "Chapter 5 Securing and using your dbms log-in credentials 5.1 Set up the adventureworks Docker container 5.2 Storing your dbms credentials 5.3 Disconnect from the database and stop Docker", " Chapter 5 Securing and using your dbms log-in credentials This chapter demonstrates how to: Keep necessary credentials secret or at least invisible Interact with PostgreSQL using your stored dbms credentials Connecting to a dbms can be very frustrating at first. In many organizations, simply getting access credentials takes time and may involve jumping through multiple hoops. In addition, a dbms is terse or deliberately inscrutable when your credetials are incorrect. That’s a security strategy, not a limitation of your understanding or of your software. When R can’t log you on to a dbms, you usually will have no information as to what went wrong. There are many different strategies for managing credentials. See Securing Credentials in RStudio’s Databases using R documentation for some alternatives to the method we adopt in this book. We provide more details about PostgreSQL Authentication in our sandbox environment in an appendix. The following packages are used in this chapter: library(tidyverse) library(DBI) library(RPostgres) require(knitr) library(sqlpetr) 5.1 Set up the adventureworks Docker container 5.1.1 Verify that Docker is running Check that Docker is up and running: sp_check_that_docker_is_up() ## [1] &quot;Docker is up but running no containers&quot; 5.1.2 Start the Docker container: Start the adventureworks Docker container: sp_docker_start(&quot;adventureworks&quot;) 5.2 Storing your dbms credentials In previous chapters the connection string for connecting to the dbms has used default credentials specified in plain text as follows: user= 'postgres', password = 'postgres' When we call sp_get_postgres_connection below we’ll use environment variables that R obtains from reading the .Renviron file when R starts up. This approach has two benefits: that file is not uploaded to GitHub and R looks for it in your default directory every time it loads. To see whether you have already created that file, use the R Studio Files tab to look at your home directory: That file should contain lines that look like the example below. Although in this example it contains the PostgreSQL default values for the username and password, they are obviously not secret. But this approach demonstrates where you should put secrets that R needs while not risking accidental uploaded to GitHub or some other public location.. Open your .Renviron file with this command: file.edit(&quot;~/.Renviron&quot;) Or you can execute define_postgresql_params.R to create the file or you could copy / paste the following into your .Renviron file: DEFAULT_POSTGRES_PASSWORD=postgres DEFAULT_POSTGRES_USER_NAME=postgres Once that file is created, restart R, and after that R reads it every time it comes up. 5.2.1 Connect with Postgres using the Sys.getenv function Connect to the postgrSQL using the sp_get_postgres_connection function: con &lt;- sp_get_postgres_connection(user = Sys.getenv(&quot;DEFAULT_POSTGRES_USER_NAME&quot;), password = Sys.getenv(&quot;DEFAULT_POSTGRES_PASSWORD&quot;), port = 5432, dbname = &quot;adventureworks&quot;, seconds_to_test = 30, connection_tab = TRUE) Once the connection object has been created, you can list all of the tables in one of the schemas: dbExecute(con, &quot;set search_path to humanresources, public;&quot;) # watch for duplicates! ## [1] 0 dbListTables(con) ## [1] &quot;employee&quot; &quot;shift&quot; ## [3] &quot;employeepayhistory&quot; &quot;jobcandidate&quot; ## [5] &quot;department&quot; &quot;vemployee&quot; ## [7] &quot;vemployeedepartment&quot; &quot;vemployeedepartmenthistory&quot; ## [9] &quot;vjobcandidate&quot; &quot;vjobcandidateeducation&quot; ## [11] &quot;vjobcandidateemployment&quot; &quot;employeedepartmenthistory&quot; 5.3 Disconnect from the database and stop Docker dbDisconnect(con) sp_docker_stop(&quot;adventureworks&quot;) "],
["chapter-connect-to-db-with-r-code.html", "Chapter 6 Connecting to the database with R code 6.1 Verify that Docker is up and running, and start the database 6.2 Connect to PostgreSQL 6.3 Set schema search path and list its contents 6.4 Anatomy of a dplyr connection object 6.5 Disconnect from the database and stop Docker", " Chapter 6 Connecting to the database with R code This chapter demonstrates how to: Connect to and disconnect R from the adventureworks database Use dplyr to get an overview of the database, replicating the facilities provided by RStudio These packages are called in this Chapter: library(tidyverse) library(DBI) library(RPostgres) library(glue) require(knitr) library(dbplyr) library(sqlpetr) library(bookdown) library(here) 6.1 Verify that Docker is up and running, and start the database The sp_check_that_docker_is_up function from the sqlpetr package checks whether Docker is up and running. If it’s not, then you need to install, launch or re-install Docker. sp_check_that_docker_is_up() ## [1] &quot;Docker is up but running no containers&quot; sp_docker_start(&quot;adventureworks&quot;) 6.2 Connect to PostgreSQL *CHECK for sqlpetr update!Thesp_make_simple_pgfunction we called above created a container from thepostgres:11library image downloaded from Docker Hub. As part of the process, it set the password for the PostgreSQL database superuserpostgres` to the value “postgres”. For simplicity, we are using a weak password at this point and it’s shown here and in the code in plain text. That is bad practice because user credentials should not be shared in open code like that. A subsequent chapter demonstrates how to store and use credentials to access the DBMS so that they are kept private. The sp_get_postgres_connection function from the sqlpetr package gets a DBI connection string to a PostgreSQL database, waiting if it is not ready. This function connects to an instance of PostgreSQL and we assign it to a symbol, con, for subsequent use. The connctions_tab = TRUE parameter opens a connections tab that’s useful for navigating a database. Note that we are using port 5439 for PostgreSQL inside the container and published to localhost. Why? If you have PostgreSQL already running on the host or another container, it probably claimed port 5432, since that’s the default. So we need to use a different port for our PostgreSQL container. Use the DBI package to connect to the adventureworks database in PostgreSQL. Remember the settings discussion about [keeping passwords hidden][Pause for some security considerations] con &lt;- sp_get_postgres_connection( host = &quot;localhost&quot;, port = 5432, # this version still using 5432!!! user = &quot;postgres&quot;, password = &quot;postgres&quot;, dbname = &quot;adventureworks&quot;, seconds_to_test = 20, connection_tab = TRUE ) 6.3 Set schema search path and list its contents Schemas will be discussed later on because multiple schemas are the norm in an enterprise database environment, but they are a side issue at this point. So we switch the order in which PostgreSQL searches for objects with the following SQL code: dbExecute(con, &quot;set search_path to sales;&quot;) ## [1] 0 With the custom search_path, the following command shows the tables in the sales schema. In the adventureworks database, there are no tables in the public schema. dbListTables(con) ## [1] &quot;countryregioncurrency&quot; &quot;customer&quot; ## [3] &quot;currencyrate&quot; &quot;creditcard&quot; ## [5] &quot;personcreditcard&quot; &quot;specialoffer&quot; ## [7] &quot;specialofferproduct&quot; &quot;salesorderheadersalesreason&quot; ## [9] &quot;shoppingcartitem&quot; &quot;salespersonquotahistory&quot; ## [11] &quot;salesperson&quot; &quot;currency&quot; ## [13] &quot;store&quot; &quot;salesorderheader&quot; ## [15] &quot;salesorderdetail&quot; &quot;salesreason&quot; ## [17] &quot;salesterritoryhistory&quot; &quot;vindividualcustomer&quot; ## [19] &quot;vpersondemographics&quot; &quot;vsalesperson&quot; ## [21] &quot;vsalespersonsalesbyfiscalyears&quot; &quot;vsalespersonsalesbyfiscalyearsdata&quot; ## [23] &quot;vstorewithaddresses&quot; &quot;vstorewithcontacts&quot; ## [25] &quot;vstorewithdemographics&quot; &quot;salestaxrate&quot; ## [27] &quot;salesterritory&quot; Notice there are several tables that start with the letter v: they are actually views which will turn out to be important. They are clearly distinguished in the connections tab, but the naming is a matter of convention. Same for dbListFields: dbListFields(con, &quot;salesorderheader&quot;) ## [1] &quot;salesorderid&quot; &quot;revisionnumber&quot; &quot;orderdate&quot; ## [4] &quot;duedate&quot; &quot;shipdate&quot; &quot;status&quot; ## [7] &quot;onlineorderflag&quot; &quot;purchaseordernumber&quot; &quot;accountnumber&quot; ## [10] &quot;customerid&quot; &quot;salespersonid&quot; &quot;territoryid&quot; ## [13] &quot;billtoaddressid&quot; &quot;shiptoaddressid&quot; &quot;shipmethodid&quot; ## [16] &quot;creditcardid&quot; &quot;creditcardapprovalcode&quot; &quot;currencyrateid&quot; ## [19] &quot;subtotal&quot; &quot;taxamt&quot; &quot;freight&quot; ## [22] &quot;totaldue&quot; &quot;comment&quot; &quot;rowguid&quot; ## [25] &quot;modifieddate&quot; Thus with this search order, the following two produce identical results: tbl(con, in_schema(&quot;sales&quot;, &quot;salesorderheader&quot;)) %&gt;% head() ## # Source: lazy query [?? x 25] ## # Database: postgres [postgres@localhost:5432/adventureworks] ## salesorderid revisionnumber orderdate duedate ## &lt;int&gt; &lt;int&gt; &lt;dttm&gt; &lt;dttm&gt; ## 1 43659 8 2011-05-31 00:00:00 2011-06-12 00:00:00 ## 2 43660 8 2011-05-31 00:00:00 2011-06-12 00:00:00 ## 3 43661 8 2011-05-31 00:00:00 2011-06-12 00:00:00 ## 4 43662 8 2011-05-31 00:00:00 2011-06-12 00:00:00 ## 5 43663 8 2011-05-31 00:00:00 2011-06-12 00:00:00 ## 6 43664 8 2011-05-31 00:00:00 2011-06-12 00:00:00 ## # … with 21 more variables: shipdate &lt;dttm&gt;, status &lt;int&gt;, ## # onlineorderflag &lt;lgl&gt;, purchaseordernumber &lt;chr&gt;, accountnumber &lt;chr&gt;, ## # customerid &lt;int&gt;, salespersonid &lt;int&gt;, territoryid &lt;int&gt;, ## # billtoaddressid &lt;int&gt;, shiptoaddressid &lt;int&gt;, shipmethodid &lt;int&gt;, ## # creditcardid &lt;int&gt;, creditcardapprovalcode &lt;chr&gt;, currencyrateid &lt;int&gt;, ## # subtotal &lt;dbl&gt;, taxamt &lt;dbl&gt;, freight &lt;dbl&gt;, totaldue &lt;dbl&gt;, comment &lt;chr&gt;, ## # rowguid &lt;chr&gt;, modifieddate &lt;dttm&gt; tbl(con, &quot;salesorderheader&quot;) %&gt;% head() ## # Source: lazy query [?? x 25] ## # Database: postgres [postgres@localhost:5432/adventureworks] ## salesorderid revisionnumber orderdate duedate ## &lt;int&gt; &lt;int&gt; &lt;dttm&gt; &lt;dttm&gt; ## 1 43659 8 2011-05-31 00:00:00 2011-06-12 00:00:00 ## 2 43660 8 2011-05-31 00:00:00 2011-06-12 00:00:00 ## 3 43661 8 2011-05-31 00:00:00 2011-06-12 00:00:00 ## 4 43662 8 2011-05-31 00:00:00 2011-06-12 00:00:00 ## 5 43663 8 2011-05-31 00:00:00 2011-06-12 00:00:00 ## 6 43664 8 2011-05-31 00:00:00 2011-06-12 00:00:00 ## # … with 21 more variables: shipdate &lt;dttm&gt;, status &lt;int&gt;, ## # onlineorderflag &lt;lgl&gt;, purchaseordernumber &lt;chr&gt;, accountnumber &lt;chr&gt;, ## # customerid &lt;int&gt;, salespersonid &lt;int&gt;, territoryid &lt;int&gt;, ## # billtoaddressid &lt;int&gt;, shiptoaddressid &lt;int&gt;, shipmethodid &lt;int&gt;, ## # creditcardid &lt;int&gt;, creditcardapprovalcode &lt;chr&gt;, currencyrateid &lt;int&gt;, ## # subtotal &lt;dbl&gt;, taxamt &lt;dbl&gt;, freight &lt;dbl&gt;, totaldue &lt;dbl&gt;, comment &lt;chr&gt;, ## # rowguid &lt;chr&gt;, modifieddate &lt;dttm&gt; 6.4 Anatomy of a dplyr connection object As introduced in the previous chapter, the dplyr::tbl function creates an object that might look like a data frame in that when you enter it on the command line, it prints a bunch of rows from the dbms table. But it is actually a list object that dplyr uses for constructing queries and retrieving data from the DBMS. The following code illustrates these issues. The dplyr::tbl function creates the connection object that we store in an object named salesorderheader_table: salesorderheader_table &lt;- dplyr::tbl(con, in_schema(&quot;sales&quot;, &quot;salesorderheader&quot;)) %&gt;% select(-rowguid) %&gt;% rename(salesorderheader_details_updated = modifieddate) At first glance, it acts like a data frame when you print it, although it only prints 10 of the table’s 31,465 rows: salesorderheader_table ## # Source: lazy query [?? x 24] ## # Database: postgres [postgres@localhost:5432/adventureworks] ## salesorderid revisionnumber orderdate duedate ## &lt;int&gt; &lt;int&gt; &lt;dttm&gt; &lt;dttm&gt; ## 1 43659 8 2011-05-31 00:00:00 2011-06-12 00:00:00 ## 2 43660 8 2011-05-31 00:00:00 2011-06-12 00:00:00 ## 3 43661 8 2011-05-31 00:00:00 2011-06-12 00:00:00 ## 4 43662 8 2011-05-31 00:00:00 2011-06-12 00:00:00 ## 5 43663 8 2011-05-31 00:00:00 2011-06-12 00:00:00 ## 6 43664 8 2011-05-31 00:00:00 2011-06-12 00:00:00 ## 7 43665 8 2011-05-31 00:00:00 2011-06-12 00:00:00 ## 8 43666 8 2011-05-31 00:00:00 2011-06-12 00:00:00 ## 9 43667 8 2011-05-31 00:00:00 2011-06-12 00:00:00 ## 10 43668 8 2011-05-31 00:00:00 2011-06-12 00:00:00 ## # … with more rows, and 20 more variables: shipdate &lt;dttm&gt;, status &lt;int&gt;, ## # onlineorderflag &lt;lgl&gt;, purchaseordernumber &lt;chr&gt;, accountnumber &lt;chr&gt;, ## # customerid &lt;int&gt;, salespersonid &lt;int&gt;, territoryid &lt;int&gt;, ## # billtoaddressid &lt;int&gt;, shiptoaddressid &lt;int&gt;, shipmethodid &lt;int&gt;, ## # creditcardid &lt;int&gt;, creditcardapprovalcode &lt;chr&gt;, currencyrateid &lt;int&gt;, ## # subtotal &lt;dbl&gt;, taxamt &lt;dbl&gt;, freight &lt;dbl&gt;, totaldue &lt;dbl&gt;, comment &lt;chr&gt;, ## # salesorderheader_details_updated &lt;dttm&gt; However, notice that the first output line shows ??, rather than providing the number of rows in the table. Similarly, the next to last line shows: … with more rows, and 20 more variables: whereas the output for a normal tbl of this salesorderheader data would say: … with 31,455 more rows, and 20 more variables: So even though salesorderheader_table is a tbl, it’s also a tbl_PqConnection: class(salesorderheader_table) ## [1] &quot;tbl_PqConnection&quot; &quot;tbl_dbi&quot; &quot;tbl_sql&quot; &quot;tbl_lazy&quot; ## [5] &quot;tbl&quot; It is not just a normal tbl of data. We can see that from the structure of salesorderheader_table: str(salesorderheader_table, max.level = 3) ## List of 2 ## $ src:List of 2 ## ..$ con :Formal class &#39;PqConnection&#39; [package &quot;RPostgres&quot;] with 3 slots ## ..$ disco: NULL ## ..- attr(*, &quot;class&quot;)= chr [1:4] &quot;src_PqConnection&quot; &quot;src_dbi&quot; &quot;src_sql&quot; &quot;src&quot; ## $ ops:List of 4 ## ..$ name: chr &quot;select&quot; ## ..$ x :List of 2 ## .. ..$ x : &#39;ident_q&#39; chr &quot;sales.salesorderheader&quot; ## .. ..$ vars: chr [1:25] &quot;salesorderid&quot; &quot;revisionnumber&quot; &quot;orderdate&quot; &quot;duedate&quot; ... ## .. ..- attr(*, &quot;class&quot;)= chr [1:3] &quot;op_base_remote&quot; &quot;op_base&quot; &quot;op&quot; ## ..$ dots: list() ## ..$ args:List of 1 ## .. ..$ vars:List of 24 ## ..- attr(*, &quot;class&quot;)= chr [1:3] &quot;op_select&quot; &quot;op_single&quot; &quot;op&quot; ## - attr(*, &quot;class&quot;)= chr [1:5] &quot;tbl_PqConnection&quot; &quot;tbl_dbi&quot; &quot;tbl_sql&quot; &quot;tbl_lazy&quot; ... It has only two rows! The first row contains all the information in the con object, which contains information about all the tables and objects in the database. Here is a sample: salesorderheader_table$src$con@typnames$typname[387:418] ## [1] &quot;AccountNumber&quot; &quot;_AccountNumber&quot; ## [3] &quot;Flag&quot; &quot;_Flag&quot; ## [5] &quot;Name&quot; &quot;_Name&quot; ## [7] &quot;NameStyle&quot; &quot;_NameStyle&quot; ## [9] &quot;OrderNumber&quot; &quot;_OrderNumber&quot; ## [11] &quot;Phone&quot; &quot;_Phone&quot; ## [13] &quot;department&quot; &quot;_department&quot; ## [15] &quot;pg_toast_16439&quot; &quot;d&quot; ## [17] &quot;_d&quot; &quot;employee&quot; ## [19] &quot;_employee&quot; &quot;pg_toast_16450&quot; ## [21] &quot;e&quot; &quot;_e&quot; ## [23] &quot;employeedepartmenthistory&quot; &quot;_employeedepartmenthistory&quot; ## [25] &quot;edh&quot; &quot;_edh&quot; ## [27] &quot;employeepayhistory&quot; &quot;_employeepayhistory&quot; ## [29] &quot;pg_toast_16482&quot; &quot;eph&quot; ## [31] &quot;_eph&quot; &quot;jobcandidate&quot; The second row contains a list of the columns in the salesorderheader table, among other things: salesorderheader_table$ops$x$vars ## [1] &quot;salesorderid&quot; &quot;revisionnumber&quot; &quot;orderdate&quot; ## [4] &quot;duedate&quot; &quot;shipdate&quot; &quot;status&quot; ## [7] &quot;onlineorderflag&quot; &quot;purchaseordernumber&quot; &quot;accountnumber&quot; ## [10] &quot;customerid&quot; &quot;salespersonid&quot; &quot;territoryid&quot; ## [13] &quot;billtoaddressid&quot; &quot;shiptoaddressid&quot; &quot;shipmethodid&quot; ## [16] &quot;creditcardid&quot; &quot;creditcardapprovalcode&quot; &quot;currencyrateid&quot; ## [19] &quot;subtotal&quot; &quot;taxamt&quot; &quot;freight&quot; ## [22] &quot;totaldue&quot; &quot;comment&quot; &quot;rowguid&quot; ## [25] &quot;modifieddate&quot; salesorderheader_table holds information needed to get the data from the ‘salesorderheader’ table, but salesorderheader_table does not hold the data itself. In the following sections, we will examine more closely this relationship between the salesorderheader_table object and the data in the database’s ‘salesorderheader’ table. 6.5 Disconnect from the database and stop Docker dbDisconnect(con) sp_docker_stop(&quot;adventureworks&quot;) "],
["chapter-dbms-queries-intro.html", "Chapter 7 Introduction to DBMS queries 7.1 Setup 7.2 Methods for downloading a single table 7.3 Translating dplyr code to SQL queries 7.4 Mixing dplyr and SQL 7.5 Examining a single table with R 7.6 Disconnect from the database and stop Docker 7.7 Additional reading", " Chapter 7 Introduction to DBMS queries This chapter demonstrates how to: Download all or part of a table from the DBMS, including different kinds of subsets See how dplyr code is translated into SQL commands and how they can be mixed Get acquainted with some useful functions and packages for investigating a single table Begin thinking about how to divide the work between your local R session and the DBMS 7.1 Setup The following packages are used in this chapter: library(tidyverse) library(DBI) library(RPostgres) library(dbplyr) require(knitr) library(bookdown) library(sqlpetr) library(skimr) Assume that the Docker container with PostgreSQL and the adventureworks database are ready to go. If not go back to [Chapter 6][#chapter_setup-adventureworks-db] sqlpetr::sp_docker_start(&quot;adventureworks&quot;) Connect to the database: con &lt;- sqlpetr::sp_get_postgres_connection( user = Sys.getenv(&quot;DEFAULT_POSTGRES_USER_NAME&quot;), password = Sys.getenv(&quot;DEFAULT_POSTGRES_PASSWORD&quot;), dbname = &quot;adventureworks&quot;, port = 5432, seconds_to_test = 20, connection_tab = TRUE ) 7.2 Methods for downloading a single table For the moment, assume you know something about the database and specifically what table you need to retrieve. We return to the topic of investigating the whole database later on. dbExecute(con, &quot;set search_path to sales, humanresources;&quot;) ## [1] 0 7.2.1 Read the entire table There are a few different methods of getting data from a DBMS, and we’ll explore the different ways of controlling each one of them. DBI::dbReadTable will download an entire table into an R tibble. salesorderheader_tibble &lt;- DBI::dbReadTable(con, &quot;salesorderheader&quot;) str(salesorderheader_tibble) ## &#39;data.frame&#39;: 31465 obs. of 25 variables: ## $ salesorderid : int 43659 43660 43661 43662 43663 43664 43665 43666 43667 43668 ... ## $ revisionnumber : int 8 8 8 8 8 8 8 8 8 8 ... ## $ orderdate : POSIXct, format: &quot;2011-05-31&quot; &quot;2011-05-31&quot; ... ## $ duedate : POSIXct, format: &quot;2011-06-12&quot; &quot;2011-06-12&quot; ... ## $ shipdate : POSIXct, format: &quot;2011-06-07&quot; &quot;2011-06-07&quot; ... ## $ status : int 5 5 5 5 5 5 5 5 5 5 ... ## $ onlineorderflag : logi FALSE FALSE FALSE FALSE FALSE FALSE ... ## $ purchaseordernumber : chr &quot;PO522145787&quot; &quot;PO18850127500&quot; &quot;PO18473189620&quot; &quot;PO18444174044&quot; ... ## $ accountnumber : chr &quot;10-4020-000676&quot; &quot;10-4020-000117&quot; &quot;10-4020-000442&quot; &quot;10-4020-000227&quot; ... ## $ customerid : int 29825 29672 29734 29994 29565 29898 29580 30052 29974 29614 ... ## $ salespersonid : int 279 279 282 282 276 280 283 276 277 282 ... ## $ territoryid : int 5 5 6 6 4 1 1 4 3 6 ... ## $ billtoaddressid : int 985 921 517 482 1073 876 849 1074 629 529 ... ## $ shiptoaddressid : int 985 921 517 482 1073 876 849 1074 629 529 ... ## $ shipmethodid : int 5 5 5 5 5 5 5 5 5 5 ... ## $ creditcardid : int 16281 5618 1346 10456 4322 806 15232 13349 10370 1566 ... ## $ creditcardapprovalcode: chr &quot;105041Vi84182&quot; &quot;115213Vi29411&quot; &quot;85274Vi6854&quot; &quot;125295Vi53935&quot; ... ## $ currencyrateid : int NA NA 4 4 NA NA NA NA NA 4 ... ## $ subtotal : num 20566 1294 32726 28833 419 ... ## $ taxamt : num 1971.5 124.2 3153.8 2775.2 40.3 ... ## $ freight : num 616.1 38.8 985.6 867.2 12.6 ... ## $ totaldue : num 23153 1457 36866 32475 472 ... ## $ comment : chr NA NA NA NA ... ## $ rowguid : chr &quot;79b65321-39ca-4115-9cba-8fe0903e12e6&quot; &quot;738dc42d-d03b-48a1-9822-f95a67ea7389&quot; &quot;d91b9131-18a4-4a11-bc3a-90b6f53e9d74&quot; &quot;4a1ecfc0-cc3a-4740-b028-1c50bb48711c&quot; ... ## $ modifieddate : POSIXct, format: &quot;2011-06-07&quot; &quot;2011-06-07&quot; ... That’s very simple, but if the table is very large it may not be a problem, since R is designed to keep the entire table in memory. The tables that are found in an enterprise database such as adventureworks may be large, they are most often records kept by people. That somewhat limits their size (relative to data generated by machines) and expands the possibilities for human error. Note that the first line of the str() output reports the total number of observations. Later on we’ll use this tibble to demonstrate several packages and functions, but use only the first 13 columns for simplicity. salesorderheader_tibble &lt;- salesorderheader_tibble[,1:13] 7.2.2 Create a pointer to a table that can be reused The dplyr::tbl function gives us more control over access to a table by enabling control over which columns and rows to download. It creates an object that might look like a data frame, but it’s actually a list object that dplyr uses for constructing queries and retrieving data from the DBMS. salesorderheader_table &lt;- dplyr::tbl(con, &quot;salesorderheader&quot;) class(salesorderheader_table) ## [1] &quot;tbl_PqConnection&quot; &quot;tbl_dbi&quot; &quot;tbl_sql&quot; &quot;tbl_lazy&quot; ## [5] &quot;tbl&quot; 7.2.3 Controlling the number of rows returned with collect() The collect function triggers the creation of a tibble and controls the number of rows that the DBMS sends to R. For more complex queries, the dplyr::collect() function provides a mechanism to indicate what’s processed on on the DBMS server and what’s processed by R on the local machine. The chapter on Lazy Evaluation and Execution Environment discusses this issue in detail. salesorderheader_table %&gt;% dplyr::collect(n = 3) %&gt;% dim() ## [1] 3 25 salesorderheader_table %&gt;% dplyr::collect(n = 500) %&gt;% dim() ## [1] 500 25 7.2.4 Retrieving random rows from the DBMS When the DBMS contains many rows, a sample of the data may be plenty for your purposes. Although dplyr has nice functions to sample a data frame that’s already in R (e.g., the sample_n and sample_frac functions), to get a sample from the DBMS we have to use dbGetQuery to send native SQL to the database. To peek ahead, here is one example of a query that retrieves 20 rows from a 1% sample: one_percent_sample &lt;- DBI::dbGetQuery( con, &quot;SELECT orderdate, subtotal, taxamt, freight, totaldue FROM salesorderheader TABLESAMPLE BERNOULLI(3) LIMIT 20; &quot; ) one_percent_sample ## orderdate subtotal taxamt freight totaldue ## 1 2011-05-31 12718.0844 1222.8838 382.1512 14323.1194 ## 2 2011-06-04 3578.2700 286.2616 89.4568 3953.9884 ## 3 2011-06-10 3578.2700 286.2616 89.4568 3953.9884 ## 4 2011-06-21 3399.9900 271.9992 84.9998 3756.9890 ## 5 2011-07-01 20628.5745 1977.1163 617.8489 23223.5397 ## 6 2011-07-01 75191.9599 7234.3070 2260.7209 84686.9878 ## 7 2011-07-01 19734.8665 1896.6545 592.7045 22224.2255 ## 8 2011-07-04 3578.2700 286.2616 89.4568 3953.9884 ## 9 2011-07-05 3578.2700 286.2616 89.4568 3953.9884 ## 10 2011-07-11 3578.2700 286.2616 89.4568 3953.9884 ## 11 2011-07-12 3578.2700 286.2616 89.4568 3953.9884 ## 12 2011-07-18 3374.9900 269.9992 84.3748 3729.3640 ## 13 2011-07-19 699.0982 55.9279 17.4775 772.5036 ## 14 2011-07-24 3399.9900 271.9992 84.9998 3756.9890 ## 15 2011-07-25 699.0982 55.9279 17.4775 772.5036 ## 16 2011-07-29 3578.2700 286.2616 89.4568 3953.9884 ## 17 2011-08-08 3578.2700 286.2616 89.4568 3953.9884 ## 18 2011-08-09 3578.2700 286.2616 89.4568 3953.9884 ## 19 2011-08-14 3578.2700 286.2616 89.4568 3953.9884 ## 20 2011-08-20 3578.2700 286.2616 89.4568 3953.9884 Exact sample of 100 records This technique depends on knowing the range of a record index, such as the businessentityid in the salesorderheader table of our adventureworks database. Start by finding the min and max values. DBI::dbListFields(con, &quot;salesorderheader&quot;) ## [1] &quot;salesorderid&quot; &quot;revisionnumber&quot; &quot;orderdate&quot; ## [4] &quot;duedate&quot; &quot;shipdate&quot; &quot;status&quot; ## [7] &quot;onlineorderflag&quot; &quot;purchaseordernumber&quot; &quot;accountnumber&quot; ## [10] &quot;customerid&quot; &quot;salespersonid&quot; &quot;territoryid&quot; ## [13] &quot;billtoaddressid&quot; &quot;shiptoaddressid&quot; &quot;shipmethodid&quot; ## [16] &quot;creditcardid&quot; &quot;creditcardapprovalcode&quot; &quot;currencyrateid&quot; ## [19] &quot;subtotal&quot; &quot;taxamt&quot; &quot;freight&quot; ## [22] &quot;totaldue&quot; &quot;comment&quot; &quot;rowguid&quot; ## [25] &quot;modifieddate&quot; salesorderheader_df &lt;- DBI::dbReadTable(con, &quot;salesorderheader&quot;) (max_id &lt;- max(salesorderheader_df$salesorderid)) ## [1] 75123 (min_id &lt;- min(salesorderheader_df$salesorderid)) ## [1] 43659 Set the random number seed and draw the sample. set.seed(123) sample_rows &lt;- sample(1:max(salesorderheader_df$salesorderid), 10) salesorderheader_table &lt;- dplyr::tbl(con, &quot;salesorderheader&quot;) Run query with the filter verb listing the randomly sampled rows to be retrieved: salesorderheader_sample &lt;- salesorderheader_table %&gt;% dplyr::filter(salesorderid %in% sample_rows) %&gt;% dplyr::collect() str(salesorderheader_sample) ## Classes &#39;tbl_df&#39;, &#39;tbl&#39; and &#39;data.frame&#39;: 7 obs. of 25 variables: ## $ salesorderid : int 45404 46435 51663 57870 62555 65161 68293 ## $ revisionnumber : int 8 8 8 8 8 8 8 ## $ orderdate : POSIXct, format: &quot;2012-01-10&quot; &quot;2012-05-06&quot; ... ## $ duedate : POSIXct, format: &quot;2012-01-22&quot; &quot;2012-05-18&quot; ... ## $ shipdate : POSIXct, format: &quot;2012-01-17&quot; &quot;2012-05-13&quot; ... ## $ status : int 5 5 5 5 5 5 5 ## $ onlineorderflag : logi TRUE TRUE TRUE TRUE TRUE FALSE ... ## $ purchaseordernumber : chr NA NA NA NA ... ## $ accountnumber : chr &quot;10-4030-011217&quot; &quot;10-4030-012251&quot; &quot;10-4030-016327&quot; &quot;10-4030-018572&quot; ... ## $ customerid : int 11217 12251 16327 18572 13483 29799 13239 ## $ salespersonid : int NA NA NA NA NA 281 NA ## $ territoryid : int 1 9 8 4 1 4 6 ## $ billtoaddressid : int 19321 24859 19265 16902 15267 997 27923 ## $ shiptoaddressid : int 19321 24859 19265 16902 15267 997 27923 ## $ shipmethodid : int 1 1 1 1 1 5 1 ## $ creditcardid : int 8241 13188 16357 1884 4409 12582 1529 ## $ creditcardapprovalcode: chr &quot;332581Vi42712&quot; &quot;635144Vi68383&quot; &quot;420152Vi84562&quot; &quot;1224478Vi9772&quot; ... ## $ currencyrateid : int NA 4121 NA NA NA NA 11581 ## $ subtotal : num 3578 3375 2466 14 57 ... ## $ taxamt : num 286.26 270 197.31 1.12 4.56 ... ## $ freight : num 89.457 84.375 61.658 0.349 1.424 ... ## $ totaldue : num 3954 3729.4 2725.3 15.4 63 ... ## $ comment : chr NA NA NA NA ... ## $ rowguid : chr &quot;358f91b2-dadd-4014-8d4f-7f9736cb664e&quot; &quot;eb312409-fcd5-4bac-bd3b-16d4bd7889db&quot; &quot;ddc60552-af98-4166-9249-d09d424d8430&quot; &quot;fe46e631-47b9-4e14-9da5-1e4a4a135364&quot; ... ## $ modifieddate : POSIXct, format: &quot;2012-01-17&quot; &quot;2012-05-13&quot; ... 7.2.5 Sub-setting variables A table in the DBMS may not only have many more rows than you want, but also many more columns. The select command controls which columns are retrieved. salesorderheader_table %&gt;% dplyr::select(orderdate, subtotal, taxamt, freight, totaldue) %&gt;% head() ## # Source: lazy query [?? x 5] ## # Database: postgres [postgres@localhost:5432/adventureworks] ## orderdate subtotal taxamt freight totaldue ## &lt;dttm&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 2011-05-31 00:00:00 20566. 1972. 616. 23153. ## 2 2011-05-31 00:00:00 1294. 124. 38.8 1457. ## 3 2011-05-31 00:00:00 32726. 3154. 986. 36866. ## 4 2011-05-31 00:00:00 28833. 2775. 867. 32475. ## 5 2011-05-31 00:00:00 419. 40.3 12.6 472. ## 6 2011-05-31 00:00:00 24433. 2345. 733. 27510. That’s exactly equivalent to submitting the following SQL commands directly: DBI::dbGetQuery( con, &#39;SELECT &quot;orderdate&quot;, &quot;subtotal&quot;, &quot;taxamt&quot;, &quot;freight&quot;, &quot;totaldue&quot; FROM &quot;salesorderheader&quot; LIMIT 6&#39;) ## orderdate subtotal taxamt freight totaldue ## 1 2011-05-31 20565.6206 1971.5149 616.0984 23153.2339 ## 2 2011-05-31 1294.2529 124.2483 38.8276 1457.3288 ## 3 2011-05-31 32726.4786 3153.7696 985.5530 36865.8012 ## 4 2011-05-31 28832.5289 2775.1646 867.2389 32474.9324 ## 5 2011-05-31 419.4589 40.2681 12.5838 472.3108 ## 6 2011-05-31 24432.6088 2344.9921 732.8100 27510.4109 We won’t discuss dplyr methods for sub-setting variables, deriving new ones, or sub-setting rows based on the values found in the table, because they are covered well in other places, including: Comprehensive reference: https://dplyr.tidyverse.org/ Good tutorial: https://suzan.rbind.io/tags/dplyr/ In practice we find that, renaming variables is often quite important because the names in an SQL database might not meet your needs as an analyst. In “the wild”, you will find names that are ambiguous or overly specified, with spaces in them, and other problems that will make them difficult to use in R. It is good practice to do whatever renaming you are going to do in a predictable place like at the top of your code. The names in the adventureworks database are simple and clear, but if they were not, you might rename them for subsequent use in this way: tbl(con, &quot;salesorderheader&quot;) %&gt;% dplyr::rename(order_date = orderdate, sub_total_amount = subtotal, tax_amount = taxamt, freight_amount = freight, total_due_amount = totaldue) %&gt;% dplyr::select(order_date, sub_total_amount, tax_amount, freight_amount, total_due_amount ) %&gt;% # head() show_query() ## &lt;SQL&gt; ## SELECT &quot;orderdate&quot; AS &quot;order_date&quot;, &quot;subtotal&quot; AS &quot;sub_total_amount&quot;, &quot;taxamt&quot; AS &quot;tax_amount&quot;, &quot;freight&quot; AS &quot;freight_amount&quot;, &quot;totaldue&quot; AS &quot;total_due_amount&quot; ## FROM &quot;salesorderheader&quot; That’s equivalent to the following SQL code: DBI::dbGetQuery( con, &#39;SELECT &quot;orderdate&quot; AS &quot;order_date&quot;, &quot;subtotal&quot; AS &quot;sub_total_amount&quot;, &quot;taxamt&quot; AS &quot;tax_amount&quot;, &quot;freight&quot; AS &quot;freight_amount&quot;, &quot;totaldue&quot; AS &quot;total_due_amount&quot; FROM &quot;salesorderheader&quot;&#39; ) %&gt;% head() ## order_date sub_total_amount tax_amount freight_amount total_due_amount ## 1 2011-05-31 20565.6206 1971.5149 616.0984 23153.2339 ## 2 2011-05-31 1294.2529 124.2483 38.8276 1457.3288 ## 3 2011-05-31 32726.4786 3153.7696 985.5530 36865.8012 ## 4 2011-05-31 28832.5289 2775.1646 867.2389 32474.9324 ## 5 2011-05-31 419.4589 40.2681 12.5838 472.3108 ## 6 2011-05-31 24432.6088 2344.9921 732.8100 27510.4109 The one difference is that the SQL code returns a regular data frame and the dplyr code returns a tibble. Notice that the seconds are grayed out in the tibble display. 7.3 Translating dplyr code to SQL queries Where did the translations we’ve shown above come from? The show_query function shows how dplyr is translating your query to the dialect of the target DBMS: salesorderheader_table %&gt;% dplyr::tally() %&gt;% dplyr::show_query() ## &lt;SQL&gt; ## SELECT COUNT(*) AS &quot;n&quot; ## FROM &quot;salesorderheader&quot; Here is an extensive discussion of how dplyr code is translated into SQL: https://dbplyr.tidyverse.org/articles/sql-translation.html If you prefer to use SQL directly, rather than dplyr, you can submit SQL code to the DBMS through the DBI::dbGetQuery function: DBI::dbGetQuery( con, &#39;SELECT COUNT(*) AS &quot;n&quot; FROM &quot;salesorderheader&quot; &#39; ) ## n ## 1 31465 When you create a report to run repeatedly, you might want to put that query into R markdown. That way you can also execute that SQL code in a chunk with the following header: {sql, connection=con, output.var = &quot;query_results&quot;} SELECT COUNT(*) AS &quot;n&quot; FROM &quot;salesorderheader&quot;; R markdown stores that query result in a tibble which can be printed by referring to it: query_results ## n ## 1 31465 7.4 Mixing dplyr and SQL When dplyr finds code that it does not know how to translate into SQL, it will simply pass it along to the DBMS. Therefore you can interleave native commands that your DBMS will understand in the middle of dplyr code. Consider this example that’s derived from (Ruiz 2019): salesorderheader_table %&gt;% dplyr::select_at(vars(subtotal, contains(&quot;date&quot;))) %&gt;% dplyr::mutate(today = now()) %&gt;% dplyr::show_query() ## &lt;SQL&gt; ## SELECT &quot;subtotal&quot;, &quot;orderdate&quot;, &quot;duedate&quot;, &quot;shipdate&quot;, &quot;modifieddate&quot;, CURRENT_TIMESTAMP AS &quot;today&quot; ## FROM &quot;salesorderheader&quot; That is native to PostgreSQL, not ANSI standard SQL. Verify that it works: salesorderheader_table %&gt;% dplyr::select_at(vars(subtotal, contains(&quot;date&quot;))) %&gt;% head() %&gt;% dplyr::mutate(today = now()) %&gt;% dplyr::collect() ## # A tibble: 6 x 6 ## subtotal orderdate duedate shipdate ## &lt;dbl&gt; &lt;dttm&gt; &lt;dttm&gt; &lt;dttm&gt; ## 1 20566. 2011-05-31 00:00:00 2011-06-12 00:00:00 2011-06-07 00:00:00 ## 2 1294. 2011-05-31 00:00:00 2011-06-12 00:00:00 2011-06-07 00:00:00 ## 3 32726. 2011-05-31 00:00:00 2011-06-12 00:00:00 2011-06-07 00:00:00 ## 4 28833. 2011-05-31 00:00:00 2011-06-12 00:00:00 2011-06-07 00:00:00 ## 5 419. 2011-05-31 00:00:00 2011-06-12 00:00:00 2011-06-07 00:00:00 ## 6 24433. 2011-05-31 00:00:00 2011-06-12 00:00:00 2011-06-07 00:00:00 ## # … with 2 more variables: modifieddate &lt;dttm&gt;, today &lt;dttm&gt; 7.5 Examining a single table with R Dealing with a large, complex database highlights the utility of specific tools in R. We include brief examples that we find to be handy: Base R structure: str Printing out some of the data: datatable, kable, and View Summary statistics: summary glimpse in the tibble package, which is included in the tidyverse skim in the skimr package 7.5.1 str - a base package workhorse str is a workhorse function that lists variables, their type and a sample of the first few variable values. str(salesorderheader_tibble) ## &#39;data.frame&#39;: 31465 obs. of 13 variables: ## $ salesorderid : int 43659 43660 43661 43662 43663 43664 43665 43666 43667 43668 ... ## $ revisionnumber : int 8 8 8 8 8 8 8 8 8 8 ... ## $ orderdate : POSIXct, format: &quot;2011-05-31&quot; &quot;2011-05-31&quot; ... ## $ duedate : POSIXct, format: &quot;2011-06-12&quot; &quot;2011-06-12&quot; ... ## $ shipdate : POSIXct, format: &quot;2011-06-07&quot; &quot;2011-06-07&quot; ... ## $ status : int 5 5 5 5 5 5 5 5 5 5 ... ## $ onlineorderflag : logi FALSE FALSE FALSE FALSE FALSE FALSE ... ## $ purchaseordernumber: chr &quot;PO522145787&quot; &quot;PO18850127500&quot; &quot;PO18473189620&quot; &quot;PO18444174044&quot; ... ## $ accountnumber : chr &quot;10-4020-000676&quot; &quot;10-4020-000117&quot; &quot;10-4020-000442&quot; &quot;10-4020-000227&quot; ... ## $ customerid : int 29825 29672 29734 29994 29565 29898 29580 30052 29974 29614 ... ## $ salespersonid : int 279 279 282 282 276 280 283 276 277 282 ... ## $ territoryid : int 5 5 6 6 4 1 1 4 3 6 ... ## $ billtoaddressid : int 985 921 517 482 1073 876 849 1074 629 529 ... 7.5.2 Always look at your data with head, View, or kable There is no substitute for looking at your data and R provides several ways to just browse it. The head function controls the number of rows that are displayed. Note that tail does not work against a database object. In every-day practice you would look at more than the default 6 rows, but here we wrap head around the data frame: sqlpetr::sp_print_df(head(salesorderheader_tibble)) 7.5.3 The summary function in base The base package’s summary function provides basic statistics that serve a unique diagnostic purpose in this context. For example, the following output shows that: * `businessentityid` is a number from 1 to 16,049. In a previous section, we ran the `str` function and saw that there are 16,044 observations in this table. Therefore, the `businessentityid` seems to be sequential from 1:16049, but there are 5 values missing from that sequence. _Exercise for the Reader_: Which 5 values from 1:16049 are missing from `businessentityid` values in the `salesorderheader` table? (_Hint_: In the chapter on SQL Joins, you will learn the functions needed to answer this question.) * The number of NA&#39;s in the `return_date` column is a good first guess as to the number of DVDs rented out or lost as of 2005-09-02 02:35:22. summary(salesorderheader_tibble) ## salesorderid revisionnumber orderdate ## Min. :43659 Min. :8.000 Min. :2011-05-31 00:00:00 ## 1st Qu.:51525 1st Qu.:8.000 1st Qu.:2013-06-20 00:00:00 ## Median :59391 Median :8.000 Median :2013-11-03 00:00:00 ## Mean :59391 Mean :8.001 Mean :2013-08-21 12:05:04 ## 3rd Qu.:67257 3rd Qu.:8.000 3rd Qu.:2014-02-28 00:00:00 ## Max. :75123 Max. :9.000 Max. :2014-06-30 00:00:00 ## ## duedate shipdate status ## Min. :2011-06-12 00:00:00 Min. :2011-06-07 00:00:00 Min. :5 ## 1st Qu.:2013-07-02 00:00:00 1st Qu.:2013-06-27 00:00:00 1st Qu.:5 ## Median :2013-11-15 00:00:00 Median :2013-11-10 00:00:00 Median :5 ## Mean :2013-09-02 12:05:41 Mean :2013-08-28 12:06:06 Mean :5 ## 3rd Qu.:2014-03-13 00:00:00 3rd Qu.:2014-03-08 00:00:00 3rd Qu.:5 ## Max. :2014-07-12 00:00:00 Max. :2014-07-07 00:00:00 Max. :5 ## ## onlineorderflag purchaseordernumber accountnumber customerid ## Mode :logical Length:31465 Length:31465 Min. :11000 ## FALSE:3806 Class :character Class :character 1st Qu.:14432 ## TRUE :27659 Mode :character Mode :character Median :19452 ## Mean :20170 ## 3rd Qu.:25994 ## Max. :30118 ## ## salespersonid territoryid billtoaddressid ## Min. :274.0 Min. : 1.000 Min. : 405 ## 1st Qu.:277.0 1st Qu.: 4.000 1st Qu.:14080 ## Median :279.0 Median : 6.000 Median :19449 ## Mean :280.6 Mean : 6.091 Mean :18263 ## 3rd Qu.:284.0 3rd Qu.: 9.000 3rd Qu.:24678 ## Max. :290.0 Max. :10.000 Max. :29883 ## NA&#39;s :27659 So the summary function is surprisingly useful as we first start to look at the table contents. 7.5.4 The glimpse function in the tibble package The tibble package’s glimpse function is a more compact version of str: tibble::glimpse(salesorderheader_tibble) ## Observations: 31,465 ## Variables: 13 ## $ salesorderid &lt;int&gt; 43659, 43660, 43661, 43662, 43663, 43664, 43665, … ## $ revisionnumber &lt;int&gt; 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8… ## $ orderdate &lt;dttm&gt; 2011-05-31, 2011-05-31, 2011-05-31, 2011-05-31, … ## $ duedate &lt;dttm&gt; 2011-06-12, 2011-06-12, 2011-06-12, 2011-06-12, … ## $ shipdate &lt;dttm&gt; 2011-06-07, 2011-06-07, 2011-06-07, 2011-06-07, … ## $ status &lt;int&gt; 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5… ## $ onlineorderflag &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, … ## $ purchaseordernumber &lt;chr&gt; &quot;PO522145787&quot;, &quot;PO18850127500&quot;, &quot;PO18473189620&quot;, … ## $ accountnumber &lt;chr&gt; &quot;10-4020-000676&quot;, &quot;10-4020-000117&quot;, &quot;10-4020-0004… ## $ customerid &lt;int&gt; 29825, 29672, 29734, 29994, 29565, 29898, 29580, … ## $ salespersonid &lt;int&gt; 279, 279, 282, 282, 276, 280, 283, 276, 277, 282,… ## $ territoryid &lt;int&gt; 5, 5, 6, 6, 4, 1, 1, 4, 3, 6, 1, 3, 1, 6, 2, 6, 3… ## $ billtoaddressid &lt;int&gt; 985, 921, 517, 482, 1073, 876, 849, 1074, 629, 52… 7.5.5 The skim function in the skimr package The skimr package has several functions that make it easy to examine an unknown data frame and assess what it contains. It is also extensible. skimr::skim(salesorderheader_tibble) Table 7.1: Data summary Name salesorderheader_tibble Number of rows 31465 Number of columns 13 _______________________ Column type frequency: character 2 logical 1 numeric 7 POSIXct 3 ________________________ Group variables None Variable type: character skim_variable n_missing complete_rate min max empty n_unique whitespace purchaseordernumber 27659 0.12 10 13 0 3806 0 accountnumber 0 1.00 14 14 0 19119 0 Variable type: logical skim_variable n_missing complete_rate mean count onlineorderflag 0 1 0.88 TRU: 27659, FAL: 3806 Variable type: numeric skim_variable n_missing complete_rate mean sd p0 p25 p50 p75 p100 hist salesorderid 0 1.00 59391.00 9083.31 43659 51525 59391 67257 75123 ▇▇▇▇▇ revisionnumber 0 1.00 8.00 0.03 8 8 8 8 9 ▇▁▁▁▁ status 0 1.00 5.00 0.00 5 5 5 5 5 ▁▁▇▁▁ customerid 0 1.00 20170.18 6261.73 11000 14432 19452 25994 30118 ▇▆▅▅▇ salespersonid 27659 0.12 280.61 4.85 274 277 279 284 290 ▇▅▅▂▃ territoryid 0 1.00 6.09 2.96 1 4 6 9 10 ▃▅▃▅▇ billtoaddressid 0 1.00 18263.15 8210.07 405 14080 19449 24678 29883 ▃▁▇▇▇ Variable type: POSIXct skim_variable n_missing complete_rate min max median n_unique orderdate 0 1 2011-05-31 2014-06-30 2013-11-03 1124 duedate 0 1 2011-06-12 2014-07-12 2013-11-15 1124 shipdate 0 1 2011-06-07 2014-07-07 2013-11-10 1124 skimr::skim_to_wide(salesorderheader_tibble) #skimr doesn&#39;t like certain kinds of columns ## Warning: &#39;skimr::skim_to_wide&#39; is deprecated. ## Use &#39;skim()&#39; instead. ## See help(&quot;Deprecated&quot;) Table 7.1: Data summary Name .data Number of rows 31465 Number of columns 13 _______________________ Column type frequency: character 2 logical 1 numeric 7 POSIXct 3 ________________________ Group variables None Variable type: character skim_variable n_missing complete_rate min max empty n_unique whitespace purchaseordernumber 27659 0.12 10 13 0 3806 0 accountnumber 0 1.00 14 14 0 19119 0 Variable type: logical skim_variable n_missing complete_rate mean count onlineorderflag 0 1 0.88 TRU: 27659, FAL: 3806 Variable type: numeric skim_variable n_missing complete_rate mean sd p0 p25 p50 p75 p100 hist salesorderid 0 1.00 59391.00 9083.31 43659 51525 59391 67257 75123 ▇▇▇▇▇ revisionnumber 0 1.00 8.00 0.03 8 8 8 8 9 ▇▁▁▁▁ status 0 1.00 5.00 0.00 5 5 5 5 5 ▁▁▇▁▁ customerid 0 1.00 20170.18 6261.73 11000 14432 19452 25994 30118 ▇▆▅▅▇ salespersonid 27659 0.12 280.61 4.85 274 277 279 284 290 ▇▅▅▂▃ territoryid 0 1.00 6.09 2.96 1 4 6 9 10 ▃▅▃▅▇ billtoaddressid 0 1.00 18263.15 8210.07 405 14080 19449 24678 29883 ▃▁▇▇▇ Variable type: POSIXct skim_variable n_missing complete_rate min max median n_unique orderdate 0 1 2011-05-31 2014-06-30 2013-11-03 1124 duedate 0 1 2011-06-12 2014-07-12 2013-11-15 1124 shipdate 0 1 2011-06-07 2014-07-07 2013-11-10 1124 7.6 Disconnect from the database and stop Docker dbDisconnect(con) sp_docker_stop(&quot;adventureworks&quot;) 7.7 Additional reading (Wickham 2018) (Baumer 2018) References "],
["chapter-exploring-a-single-table.html", "Chapter 8 Asking Business Questions From a Single Table 8.1 Setup our standard working environment 8.2 A word on naming 8.3 The overall AdventureWorks sales picture 8.4 Annual sales 8.5 Monthly Sales 8.6 The effect of online sales 8.7 Impact of order type on monthly sales 8.8 Detect and diagnose the day of the month problem 8.9 Correcting the order date for Sales Reps 8.10 Disconnect from the database and stop Docker", " Chapter 8 Asking Business Questions From a Single Table This chapter demonstrates how to: Investigate a database from a business perspective Dig into a single Adventureworks table containing sales data Show the multiple data anomalies found in a single AdventureWorks table Suggest the interplay between “data questions” and “business questions” The previous chapter has demonstrated some of the automated techniques for showing what’s in a table using specific R functions and packages. Now we demonstrate a step-by-step process of making sense of what’s in one table with more of a business perspective. We illustrate the kind of detective work that’s often involved as we investigate the meaning of the data in a table. We’ll investigate the salesorderheader table in the sales schema in this example with an eye on the AdventureWorks business’ sales. We show that there are quite a few interpretation issues even when we are examining just 3 out of the 25 columns in the salesorderheader table. For this kind of detective work we are seeking to understand the following elements separately and as they interact with each other (and they all do): The data that’s stored in the database and how information is represented How the data is entered at a day-to-day level to represent business activities How the business itself is changing 8.1 Setup our standard working environment Use these libraries: library(tidyverse) library(DBI) library(RPostgres) library(glue) require(knitr) library(dbplyr) library(sqlpetr) library(bookdown) library(here) library(lubridate) library(gt) library(scales) # ggplot xy scales theme_set(theme_light()) Connect to adventureworks: sp_docker_start(&quot;adventureworks&quot;) Sys.sleep(sleep_default) con &lt;- sp_get_postgres_connection( host = &quot;localhost&quot;, port = 5432, user = &quot;postgres&quot;, password = &quot;postgres&quot;, dbname = &quot;adventureworks&quot;, seconds_to_test = sleep_default, connection_tab = TRUE ) 8.2 A word on naming You will find that many columns have the same name in an enterprise database. For example, in the adventureworks database, almost all tables have columns named rowguid and modifieddate and there are many other examples of names that are reused. The meaning of a column depends on the table that contains it, so as you pull a column out of a table, naming its provenance is important. Naming columns carefully (whether retrieved from the database or calculated) will pay off, especially as our queries become more complex. Using soh to tag statistics that are derived from the salesorderheader table as we do in this book is one example of an intentional naming strategy: it reminds you of the original source of a column. You, future you, and your collaborators will appreciate the effort although different naming conventions are completely valid. And a naming convention when rigidly applied can yield some long and ugly names. In the following example soh appears in different positions in the column name but it is easy to guess at a glance that the data comes from the salesorderheader table. Naming derived tables is just as important as naming derived columns. 8.3 The overall AdventureWorks sales picture 8.4 Annual sales On an annual basis, are sales dollars trending up, down or flat? We begin with total revenue and number of orders at different levels of detail. annual_sales &lt;- tbl(con, in_schema(&quot;sales&quot;, &quot;salesorderheader&quot;)) %&gt;% mutate(year = substr(as.character(orderdate), 1, 4)) %&gt;% group_by(year) %&gt;% summarize( min_soh_orderdate = min(orderdate, na.rm = TRUE), max_soh_orderdate = max(orderdate, na.rm = TRUE), total_soh_dollars = round(sum(subtotal, na.rm = TRUE), 2), avg_total_soh_dollars = round(mean(subtotal, na.rm = TRUE), 2), soh_count = n() ) %&gt;% arrange(year) %&gt;% select( year, min_soh_orderdate, max_soh_orderdate, total_soh_dollars, avg_total_soh_dollars, soh_count ) %&gt;% show_query() %&gt;% collect() ## &lt;SQL&gt; ## SELECT &quot;year&quot;, &quot;min_soh_orderdate&quot;, &quot;max_soh_orderdate&quot;, &quot;total_soh_dollars&quot;, &quot;avg_total_soh_dollars&quot;, &quot;soh_count&quot; ## FROM (SELECT * ## FROM (SELECT &quot;year&quot;, MIN(&quot;orderdate&quot;) AS &quot;min_soh_orderdate&quot;, MAX(&quot;orderdate&quot;) AS &quot;max_soh_orderdate&quot;, ROUND((SUM(&quot;subtotal&quot;)) :: numeric, 2) AS &quot;total_soh_dollars&quot;, ROUND((AVG(&quot;subtotal&quot;)) :: numeric, 2) AS &quot;avg_total_soh_dollars&quot;, COUNT(*) AS &quot;soh_count&quot; ## FROM (SELECT &quot;salesorderid&quot;, &quot;revisionnumber&quot;, &quot;orderdate&quot;, &quot;duedate&quot;, &quot;shipdate&quot;, &quot;status&quot;, &quot;onlineorderflag&quot;, &quot;purchaseordernumber&quot;, &quot;accountnumber&quot;, &quot;customerid&quot;, &quot;salespersonid&quot;, &quot;territoryid&quot;, &quot;billtoaddressid&quot;, &quot;shiptoaddressid&quot;, &quot;shipmethodid&quot;, &quot;creditcardid&quot;, &quot;creditcardapprovalcode&quot;, &quot;currencyrateid&quot;, &quot;subtotal&quot;, &quot;taxamt&quot;, &quot;freight&quot;, &quot;totaldue&quot;, &quot;comment&quot;, &quot;rowguid&quot;, &quot;modifieddate&quot;, SUBSTR(CAST(&quot;orderdate&quot; AS TEXT), 1, 4) AS &quot;year&quot; ## FROM sales.salesorderheader) &quot;dbplyr_001&quot; ## GROUP BY &quot;year&quot;) &quot;dbplyr_002&quot; ## ORDER BY &quot;year&quot;) &quot;dbplyr_003&quot; annual_sales ## # A tibble: 4 x 6 ## year min_soh_orderdate max_soh_orderdate total_soh_dolla… ## &lt;chr&gt; &lt;dttm&gt; &lt;dttm&gt; &lt;dbl&gt; ## 1 2011 2011-05-31 00:00:00 2011-12-31 00:00:00 12641672. ## 2 2012 2012-01-01 00:00:00 2012-12-31 00:00:00 33524301. ## 3 2013 2013-01-01 00:00:00 2013-12-31 00:00:00 43622479. ## 4 2014 2014-01-01 00:00:00 2014-06-30 00:00:00 20057929. ## # … with 2 more variables: avg_total_soh_dollars &lt;dbl&gt;, soh_count &lt;int64&gt; min_soh_dt &lt;- min(annual_sales$min_soh_orderdate) max_soh_dt &lt;- max(annual_sales$max_soh_orderdate) Both 2011 and 2014 are shorter time spans than the other two years, making comparison across the years more difficult. We might normalize the totals based on the number of months in each year, but we first graph total dollars. 8.4.1 Total sales by year ggplot(data = annual_sales, aes(x = year, y = total_soh_dollars)) + geom_col() + scale_y_continuous(labels = scales::dollar_format()) + labs( title = &quot;Adventure Works Sales Dollars by Year&quot;, x = glue(&quot;Year - between &quot;, min_soh_dt, &quot; - &quot;, max_soh_dt), y = &quot;Sales $&quot; ) From 2011 through 2013, sales are trending up. Are sales dollars for 2014 really down? We only have a half year of data, but the 2014 total is less than half of the 2013 total. Could it be that sales are seasonal? Maybe AdventureWorks has larger sales volumes in the fourth quarter. To see if the sales dollars are seasonal, we drill down and look at the monthly sales. But first, let’s look at the number of orders and whether there’s a pattern in the sales data. 8.4.2 Total order volume Look at number of orders per year: ggplot(data = annual_sales, aes(x = year, y = as.numeric(soh_count))) + geom_col() + geom_text(aes(label = round(as.numeric(soh_count), digits = 0)), vjust = -0.25) + labs( title = &quot;Number of orders per year&quot;, x = glue(&quot;Years between &quot;, min_soh_dt, &quot; - &quot;, max_soh_dt), y = &quot;Total Number of Orders&quot; ) That’s a huge jump in the number of orders between 2012 and 2013. Given the total annual dollars, we ask whether the size of a sale has changed. 8.4.3 Average dollars per sale ggplot(data = annual_sales, aes(x = year, y = avg_total_soh_dollars)) + geom_col() + scale_y_continuous(labels = scales::dollar_format()) + geom_text(aes(label = round(avg_total_soh_dollars, digits = 0)), vjust = -0.25) + labs( title = &quot;Average Dollars per Sale&quot;, x = glue(&quot;Year - between &quot;, min_soh_dt, &quot; - &quot;, max_soh_dt), y = &quot;Average Sale Amount&quot; ) That’s a remarkable drop between average sale of more than $7,000 to less than $3,000. Some kind of remarkable change has taken place in this business. From 2012 to 2013 the average dollars per order dropped from more than $8,500 to nearly $3,000 while the total number of orders shot up from less than 4,000 to more than 14,000. Why are the number of orders increasing, but the average dollar amount of a sale is dropping? We need to drill down to look at monthly sales, adapting the first query to group by month and year. 8.5 Monthly Sales Our next investigation drills down from annual sales dollars to monthly sales dollars. For that we download the orderdate, rather than a character variable for the year. R handles the conversion from the PostgreSQL date-time to an R date-time. We then convert it to a simple date with a lubridate function. monthly_sales &lt;- tbl(con, in_schema(&quot;sales&quot;, &quot;salesorderheader&quot;)) %&gt;% select(orderdate, subtotal) %&gt;% collect() %&gt;% # From here on we&#39;re in R mutate( orderdate = date(orderdate), orderdate = round_date(orderdate, &quot;month&quot;) ) %&gt;% group_by(orderdate) %&gt;% summarize( min_soh_orderdate = min(orderdate, na.rm = TRUE), max_soh_orderdate = max(orderdate, na.rm = TRUE), total_soh_dollars = round(sum(subtotal, na.rm = TRUE), 2), avg_total_soh_dollars = round(mean(subtotal, na.rm = TRUE), 2), soh_count = n() ) Plotting the monthly sales data: ggplot(data = monthly_sales, aes(x = orderdate, y = total_soh_dollars)) + geom_col() + scale_y_continuous(labels = dollar) + theme(plot.title = element_text(hjust = 0.5)) + # Center the title labs( title = glue(&quot;Sales by Month\\n&quot;, min_soh_dt, &quot; - &quot;, max_soh_dt), x = &quot;Month&quot;, y = &quot;Sales Dollars&quot; ) 8.5.1 Check lagged monthly data The total sales are trending up but suspiciously uneven. Looking at lags might confirm just how much month-to-month difference there is: monthly_sales_lagged &lt;- monthly_sales %&gt;% mutate(monthly_sales_change = (lag(total_soh_dollars, 1)) - total_soh_dollars) (sum_lags &lt;- summary(monthly_sales_lagged$monthly_sales_change)) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## -1667368.56 -1082792.47 52892.65 18287.07 816048.02 4399378.90 ## NA&#39;s ## 1 The trend is positive on average 18,287 but half of the months have swings greater than 1,898,840! ggplot(monthly_sales_lagged, aes(x = orderdate, y = monthly_sales_change)) + scale_x_date(date_breaks = &quot;year&quot;, date_labels = &quot;%Y&quot;, date_minor_breaks = &quot;3 months&quot;) + geom_line() + scale_y_continuous(labels = scales::dollar_format()) + theme(plot.title = element_text(hjust = .5)) + # Center ggplot title labs( title = glue( &quot;Monthly Sales Change \\n&quot;, &quot;Between &quot;, min_soh_dt, &quot; and &quot;, max_soh_dt ), x = &quot;Month&quot;, y = &quot;Dollar Change&quot; ) ## Warning: Removed 1 rows containing missing values (geom_path). AdventureWorks sales are very uneven. 8.5.2 Comparing dollars and orders to a base year To look at dollars and the number of orders together, we compare the monthly data to the yearly average for 2011. start_year &lt;- monthly_sales %&gt;% mutate(yr = year(orderdate)) %&gt;% group_by(yr) %&gt;% summarize( total_soh_dollars = sum(total_soh_dollars), soh_count = sum(soh_count), n_months = n(), avg_dollars = total_soh_dollars / n_months, avg_count = soh_count / n_months ) %&gt;% filter(yr == min(yr)) Use 2011 as a baseline: start_year ## # A tibble: 1 x 6 ## yr total_soh_dollars soh_count n_months avg_dollars avg_count ## &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 2011 12354206. 1513 7 1764887. 216. Re express monthly data in terms of the baseline and plot: monthly_sales_base_year_normalized_to_2011 &lt;- monthly_sales %&gt;% mutate( dollars = (100 * total_soh_dollars) / start_year$avg_dollars, number_of_orders = (100 * soh_count) / start_year$avg_count ) %&gt;% ungroup() monthly_sales_base_year_normalized_to_2011 &lt;- monthly_sales_base_year_normalized_to_2011 %&gt;% select(orderdate, dollars, number_of_orders) %&gt;% pivot_longer(-orderdate, names_to = &quot;relative_to_2011_average&quot;, values_to = &quot;amount&quot; ) monthly_sales_base_year_normalized_to_2011 %&gt;% ggplot(aes(orderdate, amount, color = relative_to_2011_average)) + geom_line() + geom_hline(yintercept = 100) + scale_x_date(date_labels = &quot;%Y-%m&quot;, date_breaks = &quot;6 months&quot;) + labs( title = glue( &quot;Adventureworks Normalized Monthly Sales\\n&quot;, &quot;Number of Sales Orders and Dollar Totals\\n&quot;, min_soh_dt, &quot; to &quot;, max_soh_dt ), x = &quot;Date&quot;, y = &quot;&quot;, color = &quot;% change from\\n 2011 average&quot; ) 8.6 The effect of online sales We suspect that the business has changed a lot with the advent of online orders so we check the impact of onlineorderflag on annual sales. The onlineorderflag indicates which sales channel accounted for the sale, Sales Reps or Online. 8.6.1 Add onlineorderflag to our annual sales query annual_sales_w_channel &lt;- tbl(con, in_schema(&quot;sales&quot;, &quot;salesorderheader&quot;)) %&gt;% select(orderdate, subtotal, onlineorderflag) %&gt;% collect() %&gt;% mutate( orderdate = date(orderdate), orderdate = round_date(orderdate, &quot;year&quot;), onlineorderflag = if_else(onlineorderflag == FALSE, &quot;Sales Rep&quot;, &quot;Online&quot; ), onlineorderflag = as.factor(onlineorderflag) ) %&gt;% group_by(orderdate, onlineorderflag) %&gt;% summarize( min_soh_orderdate = min(orderdate, na.rm = TRUE), max_soh_orderdate = max(orderdate, na.rm = TRUE), total_soh_dollars = round(sum(subtotal, na.rm = TRUE), 2), avg_total_soh_dollars = round(mean(subtotal, na.rm = TRUE), 2), soh_count = n() ) %&gt;% select( orderdate, onlineorderflag, min_soh_orderdate, max_soh_orderdate, total_soh_dollars, avg_total_soh_dollars, soh_count ) 8.6.2 Annual Sales comparison Start by looking at total sales. ggplot(data = annual_sales_w_channel, aes(x = orderdate, y = total_soh_dollars)) + geom_col() + scale_y_continuous(labels = scales::dollar_format()) + facet_wrap(&quot;onlineorderflag&quot;) + labs( title = &quot;Adventure Works Sales Dollars by Year&quot;, caption = glue(&quot;Between&quot;, min_soh_dt, &quot; and &quot;, max_soh_dt), subtitle = &quot;Comparing Online and Sales Rep sales channels&quot;, x = &quot;Year&quot;, y = &quot;Sales $&quot; ) Indeed the total sales are quite different as are the number of orders and the average order size! 8.6.3 Order volume comparison Look at number of orders per year: ggplot(data = annual_sales_w_channel, aes(x = orderdate, y = as.numeric(soh_count))) + geom_col() + facet_wrap(&quot;onlineorderflag&quot;) + labs( title = &quot;Adventure Works Number of orders per Year&quot;, caption = glue(&quot;Between&quot;, min_soh_dt, &quot; and &quot;, max_soh_dt), subtitle = &quot;Comparing Online and Sales Rep sales channels&quot;, x = &quot;Year&quot;, y = &quot;Total number of orders&quot; ) 8.6.4 Comparing Sales Rep sales to Online Orders ggplot(data = annual_sales_w_channel, aes(x = orderdate, y = avg_total_soh_dollars)) + geom_col() + facet_wrap(&quot;onlineorderflag&quot;) + scale_y_continuous(labels = scales::dollar_format()) + labs( title = &quot;Average Dollars per Sale&quot;, x = glue(&quot;Year, between&quot;, min_soh_dt, &quot; and &quot;, max_soh_dt), y = &quot;Average sale amount&quot; ) 8.7 Impact of order type on monthly sales Digging into the difference between Sales Rep and Online sales. 8.7.1 Retrieve monthly sales with the onlineorderflag This query puts the collect statement earlier than the previous queries. monthly_sales_w_channel &lt;- tbl(con, in_schema(&quot;sales&quot;, &quot;salesorderheader&quot;)) %&gt;% select(orderdate, subtotal, onlineorderflag) %&gt;% collect() %&gt;% # From here on we&#39;re in R mutate( orderdate = date(orderdate), orderdate_rounded = round_date(orderdate, &quot;month&quot;), onlineorderflag = if_else(onlineorderflag == FALSE, &quot;Sales Rep&quot;, &quot;Online&quot; ), ) %&gt;% # group_by(orderdate, onlineorderflag) %&gt;% summarize( min_soh_orderdate = min(orderdate, na.rm = TRUE), max_soh_orderdate = max(orderdate, na.rm = TRUE), total_soh_dollars = round(sum(subtotal, na.rm = TRUE), 2), avg_total_soh_dollars = round(mean(subtotal, na.rm = TRUE), 2), soh_count = n() ) %&gt;% ungroup() monthly_sales_w_channel %&gt;% rename(`Sales Channel` = onlineorderflag) %&gt;% group_by(`Sales Channel`) %&gt;% summarize( unique_dates = n(), start_date = min(min_soh_orderdate), end_date = max(max_soh_orderdate), total_sales = sum(total_soh_dollars) ) %&gt;% gt() html { font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, 'Helvetica Neue', 'Fira Sans', 'Droid Sans', Arial, sans-serif; } #pzveloiefp .gt_table { display: table; border-collapse: collapse; margin-left: auto; /* table.margin.left */ margin-right: auto; /* table.margin.right */ color: #333333; font-size: 16px; /* table.font.size */ background-color: #FFFFFF; /* table.background.color */ width: auto; /* table.width */ border-top-style: solid; /* table.border.top.style */ border-top-width: 2px; /* table.border.top.width */ border-top-color: #A8A8A8; /* table.border.top.color */ border-bottom-style: solid; /* table.border.bottom.style */ border-bottom-width: 2px; /* table.border.bottom.width */ border-bottom-color: #A8A8A8; /* table.border.bottom.color */ } #pzveloiefp .gt_heading { background-color: #FFFFFF; /* heading.background.color */ border-bottom-color: #FFFFFF; /* table.background.color */ border-left-style: hidden; /* heading.border.lr.style */ border-left-width: 1px; /* heading.border.lr.width */ border-left-color: #D3D3D3; /* heading.border.lr.color */ border-right-style: hidden; /* heading.border.lr.style */ border-right-width: 1px; /* heading.border.lr.width */ border-right-color: #D3D3D3; /* heading.border.lr.color */ } #pzveloiefp .gt_title { color: #333333; font-size: 125%; /* heading.title.font.size */ font-weight: initial; /* heading.title.font.weight */ padding-top: 4px; /* heading.top.padding - not yet used */ padding-bottom: 4px; border-bottom-color: #FFFFFF; /* table.background.color */ border-bottom-width: 0; } #pzveloiefp .gt_subtitle { color: #333333; font-size: 85%; /* heading.subtitle.font.size */ font-weight: initial; /* heading.subtitle.font.weight */ padding-top: 0; padding-bottom: 4px; /* heading.bottom.padding - not yet used */ border-top-color: #FFFFFF; /* table.background.color */ border-top-width: 0; } #pzveloiefp .gt_bottom_border { border-bottom-style: solid; /* heading.border.bottom.style */ border-bottom-width: 2px; /* heading.border.bottom.width */ border-bottom-color: #D3D3D3; /* heading.border.bottom.color */ } #pzveloiefp .gt_column_spanner { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; padding-top: 4px; padding-bottom: 4px; } #pzveloiefp .gt_col_headings { border-top-style: solid; /* column_labels.border.top.style */ border-top-width: 2px; /* column_labels.border.top.width */ border-top-color: #D3D3D3; /* column_labels.border.top.color */ border-bottom-style: solid; /* column_labels.border.bottom.style */ border-bottom-width: 2px; /* column_labels.border.bottom.width */ border-bottom-color: #D3D3D3; /* column_labels.border.bottom.color */ border-left-style: none; /* column_labels.border.lr.style */ border-left-width: 1px; /* column_labels.border.lr.width */ border-left-color: #D3D3D3; /* column_labels.border.lr.color */ border-right-style: none; /* column_labels.border.lr.style */ border-right-width: 1px; /* column_labels.border.lr.width */ border-right-color: #D3D3D3; /* column_labels.border.lr.color */ } #pzveloiefp .gt_col_heading { color: #333333; background-color: #FFFFFF; /* column_labels.background.color */ font-size: 100%; /* column_labels.font.size */ font-weight: initial; /* column_labels.font.weight */ text-transform: inherit; /* column_labels.text_transform */ vertical-align: middle; padding: 5px; margin: 10px; overflow-x: hidden; } #pzveloiefp .gt_sep_right { border-right: 5px solid #FFFFFF; } #pzveloiefp .gt_group_heading { padding: 8px; /* row_group.padding */ color: #333333; background-color: #FFFFFF; /* row_group.background.color */ font-size: 100%; /* row_group.font.size */ font-weight: initial; /* row_group.font.weight */ text-transform: inherit; /* row_group.text_transform */ border-top-style: solid; /* row_group.border.top.style */ border-top-width: 2px; /* row_group.border.top.width */ border-top-color: #D3D3D3; /* row_group.border.top.color */ border-bottom-style: solid; /* row_group.border.bottom.style */ border-bottom-width: 2px; /* row_group.border.bottom.width */ border-bottom-color: #D3D3D3; /* row_group.border.bottom.color */ border-left-style: none; /* row_group.border.left.style */ border-left-width: 1px; /* row_group.border.left.width */ border-left-color: #D3D3D3; /* row_group.border.left.color */ border-right-style: none; /* row_group.border.right.style */ border-right-width: 1px; /* row_group.border.right.width */ border-right-color: #D3D3D3; /* row_group.border.right.color */ vertical-align: middle; } #pzveloiefp .gt_empty_group_heading { padding: 0.5px; color: #333333; background-color: #FFFFFF; /* row_group.background.color */ font-size: 100%; /* row_group.font.size */ font-weight: initial; /* row_group.font.weight */ border-top-style: solid; /* row_group.border.top.style */ border-top-width: 2px; /* row_group.border.top.width */ border-top-color: #D3D3D3; /* row_group.border.top.color */ border-bottom-style: solid; /* row_group.border.bottom.style */ border-bottom-width: 2px; /* row_group.border.bottom.width */ border-bottom-color: #D3D3D3; /* row_group.border.bottom.color */ vertical-align: middle; } #pzveloiefp .gt_striped { background-color: #8080800D; /* row.striping.background_color */ } #pzveloiefp .gt_from_md > :first-child { margin-top: 0; } #pzveloiefp .gt_from_md > :last-child { margin-bottom: 0; } #pzveloiefp .gt_row { padding-top: 8px; /* data_row.padding */ padding-bottom: 8px; /* data_row.padding */ padding-left: 5px; padding-right: 5px; margin: 10px; border-top-style: solid; /* table_body.hlines.style */ border-top-width: 1px; /* table_body.hlines.width */ border-top-color: #D3D3D3; /* table_body.hlines.color */ border-left-style: none; /* table_body.vlines.style */ border-left-width: 1px; /* table_body.vlines.width */ border-left-color: #D3D3D3; /* table_body.vlines.color */ border-right-style: none; /* table_body.vlines.style */ border-right-width: 1px; /* table_body.vlines.width */ border-right-color: #D3D3D3; /* table_body.vlines.color */ vertical-align: middle; overflow-x: hidden; } #pzveloiefp .gt_stub { color: #333333; background-color: #FFFFFF; /* stub.background.color */ font-weight: initial; /* stub.font.weight */ text-transform: inherit; /* stub.text_transform */ border-right-style: solid; /* stub.border.style */ border-right-width: 2px; /* stub.border.width */ border-right-color: #D3D3D3; /* stub.border.color */ padding-left: 12px; } #pzveloiefp .gt_summary_row { color: #333333; background-color: #FFFFFF; /* summary_row.background.color */ text-transform: inherit; /* summary_row.text_transform */ padding-top: 8px; /* summary_row.padding */ padding-bottom: 8px; /* summary_row.padding */ padding-left: 5px; padding-right: 5px; } #pzveloiefp .gt_first_summary_row { padding-top: 8px; /* summary_row.padding */ padding-bottom: 8px; /* summary_row.padding */ padding-left: 5px; padding-right: 5px; border-top-style: solid; /* summary_row.border.style */ border-top-width: 2px; /* summary_row.border.width */ border-top-color: #D3D3D3; /* summary_row.border.color */ } #pzveloiefp .gt_grand_summary_row { color: #333333; background-color: #FFFFFF; /* grand_summary_row.background.color */ text-transform: inherit; /* grand_summary_row.text_transform */ padding-top: 8px; /* grand_summary_row.padding */ padding-bottom: 8px; /* grand_summary_row.padding */ padding-left: 5px; padding-right: 5px; } #pzveloiefp .gt_first_grand_summary_row { padding-top: 8px; /* grand_summary_row.padding */ padding-bottom: 8px; /* grand_summary_row.padding */ padding-left: 5px; padding-right: 5px; border-top-style: double; /* grand_summary_row.border.style */ border-top-width: 6px; /* grand_summary_row.border.width */ border-top-color: #D3D3D3; /* grand_summary_row.border.color */ } #pzveloiefp .gt_table_body { border-top-style: solid; /* table_body.border.top.style */ border-top-width: 2px; /* table_body.border.top.width */ border-top-color: #D3D3D3; /* table_body.border.top.color */ border-bottom-style: solid; /* table_body.border.bottom.style */ border-bottom-width: 2px; /* table_body.border.bottom.width */ border-bottom-color: #D3D3D3; /* table_body.border.bottom.color */ } #pzveloiefp .gt_footnotes { color: #333333; background-color: #FFFFFF; /* footnotes.background.color */ border-bottom-style: none; /* footnotes.border.bottom.style */ border-bottom-width: 2px; /* footnotes.border.bottom.width */ border-bottom-color: #D3D3D3; /* footnotes.border.bottom.color */ border-left-style: none; /* footnotes.border.lr.color */ border-left-width: 2px; /* footnotes.border.lr.color */ border-left-color: #D3D3D3; /* footnotes.border.lr.color */ border-right-style: none; /* footnotes.border.lr.color */ border-right-width: 2px; /* footnotes.border.lr.color */ border-right-color: #D3D3D3; /* footnotes.border.lr.color */ } #pzveloiefp .gt_footnote { margin: 0px; font-size: 90%; /* footnotes.font.size */ padding: 4px; /* footnotes.padding */ } #pzveloiefp .gt_sourcenotes { color: #333333; background-color: #FFFFFF; /* source_notes.background.color */ border-bottom-style: none; /* source_notes.border.bottom.style */ border-bottom-width: 2px; /* source_notes.border.bottom.width */ border-bottom-color: #D3D3D3; /* source_notes.border.bottom.color */ border-left-style: none; /* source_notes.border.lr.style */ border-left-width: 2px; /* source_notes.border.lr.style */ border-left-color: #D3D3D3; /* source_notes.border.lr.style */ border-right-style: none; /* source_notes.border.lr.style */ border-right-width: 2px; /* source_notes.border.lr.style */ border-right-color: #D3D3D3; /* source_notes.border.lr.style */ } #pzveloiefp .gt_sourcenote { font-size: 90%; /* source_notes.font.size */ padding: 4px; /* source_notes.padding */ } #pzveloiefp .gt_left { text-align: left; } #pzveloiefp .gt_center { text-align: center; } #pzveloiefp .gt_right { text-align: right; font-variant-numeric: tabular-nums; } #pzveloiefp .gt_font_normal { font-weight: normal; } #pzveloiefp .gt_font_bold { font-weight: bold; } #pzveloiefp .gt_font_italic { font-style: italic; } #pzveloiefp .gt_super { font-size: 65%; } #pzveloiefp .gt_footnote_marks { font-style: italic; font-size: 65%; } Sales Channel unique_dates start_date end_date total_sales Online 1124 2011-05-31 2014-06-30 29358677.46 Sales Rep 40 2011-05-31 2014-05-01 80487704.18 As we will see later on, the Sales Rep data doesn’t match the Online data. The Online data includes 2 months that are not included in the main sales channel. 8.7.2 Monthly variation compared to a trend line Jumping to the trend line comparison, we see the source of the variation. # sp_print_df(monthly_sales_w_channel) ggplot( data = monthly_sales_w_channel, aes( x = orderdate, y = total_soh_dollars ) ) + geom_line() + geom_smooth(se = FALSE) + facet_wrap(&quot;onlineorderflag&quot;) + scale_y_continuous(labels = dollar) + scale_x_date(date_breaks = &quot;year&quot;, date_labels = &quot;%Y&quot;, date_minor_breaks = &quot;3 months&quot;) + theme(plot.title = element_text(hjust = .5)) + # Center ggplot title labs( title = glue( &quot;Sales by Month by Year&quot; ), x = paste0(&quot;Month - between &quot;, min_soh_dt, &quot; - &quot;, max_soh_dt), y = &quot;Sales Dollars&quot; ) ## `geom_smooth()` using method = &#39;gam&#39; and formula &#39;y ~ s(x, bs = &quot;cs&quot;)&#39; (#fig:average dollars)SO, SO Dollars, and Average SO Dollars-b The monthly variation is happening on the Sales Rep side. 8.7.3 Compare monthly lagged data by order type First consider month-to-month change. monthly_sales_w_channel_lagged_by_month &lt;- monthly_sales_w_channel %&gt;% group_by(onlineorderflag) %&gt;% mutate( pct_monthly_soh_dollar_change = total_soh_dollars / (lag(total_soh_dollars, 1)) * 100, pct_monthly_soh_count_change = soh_count / (lag(soh_count, 1)) * 100 ) ggplot(monthly_sales_w_channel_lagged_by_month, aes(x = orderdate, y = pct_monthly_soh_dollar_change)) + scale_x_date(date_breaks = &quot;year&quot;, date_labels = &quot;%Y&quot;, date_minor_breaks = &quot;3 months&quot;) + facet_wrap(&quot;onlineorderflag&quot;) + geom_line() + theme(plot.title = element_text(hjust = .5)) + # Center ggplot title labs( title = glue( &quot;Monthly Percent Sales Change \\n&quot;, &quot;Comparing Online to Sales Rep Sales&quot; ), x = paste0(&quot;Month - between &quot;, min_soh_dt, &quot; - &quot;, max_soh_dt), y = &quot;% Dollar Change&quot; ) ## Warning: Removed 1 rows containing missing values (geom_path). For Sales Reps it looks like the variation is in the number of orders, not just dollars, as shown in the following plot. ggplot(monthly_sales_w_channel_lagged_by_month, aes(x = orderdate, y = pct_monthly_soh_count_change)) + scale_x_date(date_breaks = &quot;year&quot;, date_labels = &quot;%Y&quot;, date_minor_breaks = &quot;3 months&quot;) + facet_wrap(&quot;onlineorderflag&quot;) + geom_line() + theme(plot.title = element_text(hjust = .5)) + # Center ggplot title labs( title = glue( &quot;Monthly Order Volume Change \\n&quot;, &quot;Comparing Online to Sales Rep Sales\\n&quot;, min_soh_dt, &quot; - &quot;, max_soh_dt ), x = &quot;Month&quot;, y = &quot;Change number of orders&quot; ) ## Warning: Removed 1 rows containing missing values (geom_path). Let’s examine whether there is a large year-to-year change. monthly_sales_w_channel_lagged_by_year &lt;- monthly_sales_w_channel %&gt;% group_by(onlineorderflag) %&gt;% mutate( pct_monthly_soh_dollar_change = total_soh_dollars / (lag(total_soh_dollars, 12)) * 100, pct_monthly_soh_count_change = soh_count / (lag(soh_count, 12)) * 100 ) ggplot( monthly_sales_w_channel_lagged_by_year, aes(x = orderdate, y = pct_monthly_soh_dollar_change) ) + scale_x_date( date_breaks = &quot;year&quot;, date_labels = &quot;%Y&quot;, date_minor_breaks = &quot;3 months&quot; ) + scale_y_continuous(limits = c(-10, 300)) + facet_wrap(&quot;onlineorderflag&quot;) + geom_line() + theme(plot.title = element_text(hjust = .5)) + # Center ggplot title labs( title = glue( &quot;Year-on-Year Total Monthly Sales Change \\n&quot;, &quot;Comparing Online to Sales Rep Sales&quot; ), x = paste0(&quot;Month - between &quot;, min_soh_dt, &quot; - &quot;, max_soh_dt), y = &quot;% Dollar Change&quot; ) ## Warning: Removed 12 rows containing missing values (geom_path). That’s much smaller than the month-to-month change. ??? Comparing the number of sales orders year over year by month for 2013 and 2012, one can see that the 2013 sales are between 1.2 and 1.8 times larger than the corresponding month of 2012 from January through June. In July the 2013 sales are 5 to 6 times the 2012 sales orders. ??? This trend continues into 2014 before the number of sales plummet to just 1.3 time in June. We suspect that the business has changed a lot with the advent of Online orders. 8.8 Detect and diagnose the day of the month problem Looking at the raw data leads us to suspect that there is a problem with the dates on which Sales Rep orders are entered. 8.8.1 Sales Rep Orderdate Distribution Look at the dates when sales are entered for sales by Sales Reps. The following query merits some discussion. sales_rep_day_of_month_sales &lt;- tbl(con, in_schema(&quot;sales&quot;, &quot;salesorderheader&quot;)) %&gt;% filter(onlineorderflag == FALSE) %&gt;% # Drop online orders select(orderdate, subtotal) %&gt;% mutate( year = year(orderdate), month = month(orderdate), day = day(orderdate) ) %&gt;% count(year, month, day, name = &quot;orders&quot;) %&gt;% group_by(year, month) %&gt;% summarize( days_with_orders = n(), total_orders = sum(orders, na.rm = TRUE), min_day = min(day, na.rm = FALSE) ) %&gt;% show_query() %&gt;% collect() %&gt;% mutate( days_with_orders = as.numeric(days_with_orders), order_month = as.Date(paste0(year, &quot;-&quot;, month, &quot;-01&quot;)), min_day_factor = if_else(min_day &lt; 2, &quot;Month start&quot;, &quot;Month end&quot;) ) %&gt;% complete(order_month = seq(min(order_month), max(order_month), by = &quot;month&quot;)) %&gt;% mutate(days_with_orders = replace_na(days_with_orders, 0)) %&gt;% ungroup() ## &lt;SQL&gt; ## Warning: Missing values are always removed in SQL. ## Use `MIN(x, na.rm = TRUE)` to silence this warning ## This warning is displayed only once per session. ## SELECT &quot;year&quot;, &quot;month&quot;, COUNT(*) AS &quot;days_with_orders&quot;, SUM(&quot;orders&quot;) AS &quot;total_orders&quot;, MIN(&quot;day&quot;) AS &quot;min_day&quot; ## FROM (SELECT &quot;year&quot;, &quot;month&quot;, &quot;day&quot;, COUNT(*) AS &quot;orders&quot; ## FROM (SELECT &quot;orderdate&quot;, &quot;subtotal&quot;, EXTRACT(year FROM &quot;orderdate&quot;) AS &quot;year&quot;, EXTRACT(MONTH FROM &quot;orderdate&quot;) AS &quot;month&quot;, EXTRACT(day FROM &quot;orderdate&quot;) AS &quot;day&quot; ## FROM (SELECT * ## FROM sales.salesorderheader ## WHERE (&quot;onlineorderflag&quot; = FALSE)) &quot;dbplyr_007&quot;) &quot;dbplyr_008&quot; ## GROUP BY &quot;year&quot;, &quot;month&quot;, &quot;day&quot;) &quot;dbplyr_009&quot; ## GROUP BY &quot;year&quot;, &quot;month&quot; sales_rep_day_of_month_sales %&gt;% ggplot(aes(order_month, days_with_orders, fill = min_day_factor)) + geom_col() + coord_flip() + labs( title = &quot;How many days had Sales Rep orders posted&quot;, subtitle = &quot;At the beginning or end of the month?&quot;, fill = &quot;Day Sales were posted&quot;, y = &quot;Number of days with transactions&quot;, x = &quot;Date&quot; ) Suspicious months are those where sales were recorded on more than one day or there were no sales recorded in the month at all. suspicious_months &lt;- sales_rep_day_of_month_sales %&gt;% filter(days_with_orders == 0 | days_with_orders &gt; 1) %&gt;% arrange(order_month) %&gt;% select(order_month) %&gt;% unique() Here are the 8 suspicious months: suspicious_months ## # A tibble: 8 x 1 ## order_month ## &lt;date&gt; ## 1 2011-06-01 ## 2 2011-08-01 ## 3 2011-09-01 ## 4 2011-10-01 ## 5 2011-11-01 ## 6 2012-01-01 ## 7 2014-01-01 ## 8 2014-03-01 months_to_inspect &lt;- tibble(target_month = suspicious_months) %&gt;% mutate( current_month = target_month$order_month, next_month = current_month %m+% months(1), last_month = current_month %m-% months(1) ) %&gt;% select(current_month, next_month, last_month) %&gt;% pivot_longer(cols = tidyselect::peek_vars()) %&gt;% select(value) %&gt;% distinct() We have 15 months when we add the month before and the month after the suspicious months. We don’t know whether the problem postings have been carried forward or backward. We check for and eliminate duplicates as well. monthly_sales_w_channel_to_inspect &lt;- monthly_sales_w_channel %&gt;% filter(onlineorderflag == &quot;Sales Rep&quot;) %&gt;% mutate(order_month = round_date(orderdate, &quot;month&quot;)) %&gt;% right_join(months_to_inspect, by = c(&quot;order_month&quot; = &quot;value&quot;)) %&gt;% select(-onlineorderflag, -min_soh_orderdate, -max_soh_orderdate) %&gt;% arrange(desc(orderdate)) monthly_sales_w_channel_to_inspect ## # A tibble: 18 x 5 ## orderdate total_soh_dollars avg_total_soh_dollars soh_count order_month ## &lt;date&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;date&gt; ## 1 2014-03-31 3314519. 18621. 178 2014-04-01 ## 2 2014-03-30 7291. 3646. 2 2014-04-01 ## 3 2014-03-01 2204542. 24226. 91 2014-03-01 ## 4 2014-02-28 3231. 1077. 3 2014-03-01 ## 5 2014-01-29 2737188. 15822. 173 2014-02-01 ## 6 2014-01-28 1565. 782. 2 2014-02-01 ## 7 2013-12-31 2703811. 15450. 175 2014-01-01 ## 8 2013-11-30 1668952. 17385. 96 2013-12-01 ## 9 2012-01-29 1455280. 22739. 64 2012-02-01 ## 10 2012-01-01 1900789. 24061. 79 2012-01-01 ## 11 2011-12-01 713117. 17828. 40 2011-12-01 ## 12 2011-10-31 1702945. 27031. 63 2011-11-01 ## 13 2011-10-01 2324136. 25824. 90 2011-10-01 ## 14 2011-08-31 844721 21118. 40 2011-09-01 ## 15 2011-08-01 1165897. 19432. 60 2011-08-01 ## 16 2011-07-01 1538408. 20512. 75 2011-07-01 ## 17 2011-05-31 489329. 12877. 38 2011-06-01 ## 18 NA NA NA NA 2011-05-01 That is unexpected. A couple of things immediately jump out from the first page of data: July, September, and November are missing for 2011. Most of the Sales Reps’ orders are entered on a single day of the month, unique days = 1. It is possible that these are monthly recurring orders that get released on a given day of the month. If that is the case, what are the Sales Reps doing the rest of the month? ** ?? The lines with multiple days, unique_days &gt; 1, have a noticeable higher number of orders, so_cnt, and associated so dollars.?? ** The plot clearly shows that two months with multiple sales rep order days for 2011, (11/08 and 11/10), one for 2012, (1201), and two in 2014, (14/01 and 14/03). The 14/03 is the only three day sales rep order month. In the next code block, we flesh out the dates associated with the Sales Reps’ orders. Since 4 out of the 5 months with multiple order days only have two dates, the code block captures them with a min/max orderdate. 8.9 Correcting the order date for Sales Reps monthly_sales_rep_adjusted &lt;- tbl(con, in_schema(&quot;sales&quot;, &quot;salesorderheader&quot;)) %&gt;% select(orderdate, subtotal, onlineorderflag) %&gt;% mutate( orderdate = as.Date(orderdate), day = day(orderdate) ) %&gt;% show_query() %&gt;% collect() # From here on we&#39;re in R ## &lt;SQL&gt; ## SELECT &quot;orderdate&quot;, &quot;subtotal&quot;, &quot;onlineorderflag&quot;, EXTRACT(day FROM &quot;orderdate&quot;) AS &quot;day&quot; ## FROM (SELECT CAST(&quot;orderdate&quot; AS DATE) AS &quot;orderdate&quot;, &quot;subtotal&quot;, &quot;onlineorderflag&quot; ## FROM sales.salesorderheader) &quot;dbplyr_013&quot; monthly_sales_rep_adjusted %&gt;% filter(day == 1 &amp; onlineorderflag == FALSE) %&gt;% count(orderdate) %&gt;% as.data.frame() ## orderdate n ## 1 2011-07-01 75 ## 2 2011-08-01 60 ## 3 2011-10-01 90 ## 4 2011-12-01 40 ## 5 2012-01-01 79 ## 6 2014-03-01 91 ## 7 2014-05-01 179 dbGetQuery( con, &quot; with udays as ( SELECT to_char(orderdate,&#39;YYMM&#39;) yymm ,EXTRACT(YEAR FROM soh.orderdate) yr , EXTRACT(MONTH FROM soh.orderdate) mo , COUNT(DISTINCT soh.orderdate) *1.0 unique_days , COUNT(*) so_cnt , sum(subtotal) so_dollars FROM sales.salesorderheader soh where not onlineorderflag group by to_char(orderdate,&#39;YYMM&#39;) , EXTRACT(MONTH FROM orderdate) , EXTRACT(YEAR FROM orderdate) ORDER BY to_char(orderdate,&#39;YYMM&#39;) ) select soh.orderdate,count(*) from udays join sales.salesorderheader soh on to_char(soh.orderdate,&#39;YYMM&#39;) = udays.yymm where unique_days &gt; 1 and not onlineorderflag group by soh.orderdate having count(*) &gt; 1 order by orderdate &quot; ) ## orderdate count ## 1 2011-08-01 60 ## 2 2011-08-31 40 ## 3 2011-10-01 90 ## 4 2011-10-31 63 ## 5 2012-01-01 79 ## 6 2012-01-29 64 ## 7 2014-01-28 2 ## 8 2014-01-29 173 ## 9 2014-03-01 91 ## 10 2014-03-30 2 ## 11 2014-03-31 178 8.9.1 Define a date correction function in R monthly_sales_rep_adjusted &lt;- tbl(con, in_schema(&quot;sales&quot;, &quot;salesorderheader&quot;)) %&gt;% select(orderdate, subtotal, onlineorderflag) %&gt;% mutate( orderdate = as.Date(orderdate), day = day(orderdate) ) %&gt;% show_query() %&gt;% collect() %&gt;% # From here on we&#39;re in R # Writing the mutate statement in a generic form, so it applies only # to Sales Rep orders mutate( adjusted_orderdate = case_when( onlineorderflag == FALSE &amp; day == 1L ~ orderdate - 1, TRUE ~ orderdate ) ) %&gt;% filter(onlineorderflag == FALSE) %&gt;% group_by(adjusted_orderdate) %&gt;% summarize( total_soh_dollars = round(sum(subtotal, na.rm = TRUE), 2), avg_total_soh_dollars = round(mean(subtotal, na.rm = TRUE), 2), soh_count = n() ) %&gt;% ungroup() ## &lt;SQL&gt; ## SELECT &quot;orderdate&quot;, &quot;subtotal&quot;, &quot;onlineorderflag&quot;, EXTRACT(day FROM &quot;orderdate&quot;) AS &quot;day&quot; ## FROM (SELECT CAST(&quot;orderdate&quot; AS DATE) AS &quot;orderdate&quot;, &quot;subtotal&quot;, &quot;onlineorderflag&quot; ## FROM sales.salesorderheader) &quot;dbplyr_015&quot; 8.9.2 Define and store a PostgreSQL function to correct the date dbExecute( con, &quot;CREATE OR REPLACE FUNCTION so_adj_date(so_date timestamp, ONLINE_ORDER boolean) RETURNS timestamp AS $$ BEGIN IF (ONLINE_ORDER) THEN RETURN (SELECT so_date); ELSE RETURN(SELECT CASE WHEN EXTRACT(DAY FROM so_date) = 1 THEN so_date - &#39;1 day&#39;::interval ELSE so_date END ); END IF; END; $$ LANGUAGE PLPGSQL; &quot; ) ## [1] 0 8.9.3 Use the PostgreSQL function If you can do the heavy lifting on the database side, that’s good. R can do it, but it’s best for finding the issues. monthly_sales_rep_adjusted_with_psql_function &lt;- tbl(con, in_schema(&quot;sales&quot;, &quot;salesorderheader&quot;)) %&gt;% select(orderdate, subtotal, onlineorderflag) %&gt;% mutate( orderdate = as.Date(orderdate), day = day(orderdate) ) %&gt;% mutate(adjusted_orderdate = as.Date(so_adj_date(orderdate, onlineorderflag))) %&gt;% filter(onlineorderflag == FALSE) %&gt;% group_by(adjusted_orderdate) %&gt;% summarize( total_soh_dollars = round(sum(subtotal, na.rm = TRUE), 2), avg_total_soh_dollars = round(mean(subtotal, na.rm = TRUE), 2), soh_count = n() ) %&gt;% show_query() %&gt;% collect() %&gt;% ungroup() ## &lt;SQL&gt; ## SELECT &quot;adjusted_orderdate&quot;, ROUND((SUM(&quot;subtotal&quot;)) :: numeric, 2) AS &quot;total_soh_dollars&quot;, ROUND((AVG(&quot;subtotal&quot;)) :: numeric, 2) AS &quot;avg_total_soh_dollars&quot;, COUNT(*) AS &quot;soh_count&quot; ## FROM (SELECT &quot;orderdate&quot;, &quot;subtotal&quot;, &quot;onlineorderflag&quot;, &quot;day&quot;, CAST(so_adj_date(&quot;orderdate&quot;, &quot;onlineorderflag&quot;) AS DATE) AS &quot;adjusted_orderdate&quot; ## FROM (SELECT &quot;orderdate&quot;, &quot;subtotal&quot;, &quot;onlineorderflag&quot;, EXTRACT(day FROM &quot;orderdate&quot;) AS &quot;day&quot; ## FROM (SELECT CAST(&quot;orderdate&quot; AS DATE) AS &quot;orderdate&quot;, &quot;subtotal&quot;, &quot;onlineorderflag&quot; ## FROM sales.salesorderheader) &quot;dbplyr_017&quot;) &quot;dbplyr_018&quot;) &quot;dbplyr_019&quot; ## WHERE (&quot;onlineorderflag&quot; = FALSE) ## GROUP BY &quot;adjusted_orderdate&quot; There’s one minor difference between the two: all_equal(monthly_sales_rep_adjusted, monthly_sales_rep_adjusted_with_psql_function) ## [1] &quot;Incompatible type for column `soh_count`: x integer, y integer64&quot; monthly_sales_rep_adjusted %&gt;% mutate(day_of_month = day(adjusted_orderdate)) %&gt;% ggplot(aes(x = day_of_month, y = soh_count)) + geom_col() + scale_x_continuous(limits = c(1, 32)) + labs( title = glue( &quot;Transactions Entered by Day of Month \\n&quot;, &quot;Comparing Online to Sales Rep Sales\\n&quot;, min_soh_dt, &quot; - &quot;, max_soh_dt ), x = &quot;Day of the Month&quot;, y = &quot;Recorded Sales&quot; ) + theme(plot.title = element_text(hjust = .5)) # Center ggplot title 8.9.4 Monthly Sales by Order Type with corrected dates – relative to a trend line monthly_sales_rep_as_is &lt;- monthly_sales_w_channel %&gt;% filter(onlineorderflag == &quot;Sales Rep&quot;) ggplot( data = monthly_sales_rep_adjusted, aes(x = adjusted_orderdate, y = soh_count) ) + geom_line(alpha = .5) + geom_smooth(se = FALSE) + geom_smooth( data = monthly_sales_rep_as_is, aes( orderdate, soh_count ), color = &quot;red&quot;, alpha = .5, se = FALSE ) + theme(plot.title = element_text(hjust = .5)) + # Center ggplot title labs( title = glue( &quot;Number of Sales per month using corrected dates\\n&quot;, &quot;Counting Sales Order Header records&quot; ), x = paste0(&quot;Monthly - between &quot;, min_soh_dt, &quot; - &quot;, max_soh_dt), y = &quot;Number of Sales Recorded&quot; ) ## `geom_smooth()` using method = &#39;loess&#39; and formula &#39;y ~ x&#39; ## `geom_smooth()` using method = &#39;loess&#39; and formula &#39;y ~ x&#39; 8.10 Disconnect from the database and stop Docker dbDisconnect(con) sp_docker_stop(&quot;adventureworks&quot;) "],
["chapter-sales-forecasting.html", "Chapter 9 Sales Forecasting 9.1 Setup 9.2 Exploring the sales data 9.3 Cleaning up 9.4 Disconnect from the database and stop Docker", " Chapter 9 Sales Forecasting This chapter demonstrates how to: Use tidyverts packages to explore and forecast sales data. 9.1 Setup The following packages are used in this chapter: library(tidyverse) library(DBI) library(RPostgres) require(knitr) library(bookdown) library(sqlpetr) library(tsibble) library(fable) library(zoo) Analyzing sales time series, in particular determining seasonality and forecasting future sales, is a common activty in business management. A collection of packages called tidyverst is designed to do this in a tidy data framework. First, we make sure the Docker container is ready and connect to the adventureworks database. sqlpetr::sp_docker_start(&quot;adventureworks&quot;) con &lt;- sqlpetr::sp_get_postgres_connection( user = Sys.getenv(&quot;DEFAULT_POSTGRES_USER_NAME&quot;), password = Sys.getenv(&quot;DEFAULT_POSTGRES_PASSWORD&quot;), dbname = &quot;adventureworks&quot;, port = 5432, seconds_to_test = 20, connection_tab = FALSE ) Next, we retrieve the “sales order header” table from the database, close the connection and stop the container. dbExecute(con, &quot;set search_path to sales;&quot;) ## [1] 0 salesorderheader_tibble &lt;- DBI::dbReadTable(con, &quot;salesorderheader&quot;) DBI::dbDisconnect(con) sqlpetr::sp_docker_stop(&quot;adventureworks&quot;) 9.2 Exploring the sales data Some assumptions: 1. The business requirement is to analyze / forecast revenue. 2. The revenue figures we care about are those in the subtotal column. The shipping and tax numbers are costs, not revenue. 3. The values in column subtotal have been converted to the common currency of the Adventureworks headquarters. In a real-world setting, the analyst would need to validate these assumptions. Given that, our first task is to create a tsibble - a time series tibble - of monthly revenue figures. 9.2.1 Creating a tsibble monthly_tsibble &lt;- salesorderheader_tibble %&gt;% dplyr::mutate( origin = ifelse(onlineorderflag, &quot;online&quot;, &quot;sales_rep&quot;), month = lubridate::floor_date(shipdate, unit = &quot;months&quot;) %&gt;% as.yearmon() ) %&gt;% dplyr::group_by(origin, month) %&gt;% dplyr::summarize(orders = n(), total_revenue = sum(subtotal)) %&gt;% dplyr::ungroup() # Note that there are two more months - June and July of 2014 - for the # online data, and the revenue values are suspiciously low. # We remove them for consistency. monthly_tsibble &lt;- monthly_tsibble %&gt;% dplyr::filter(month &lt; &#39;2014-06-01&#39;) %&gt;% tsibble::as_tsibble(key = origin, index = month) 9.2.2 Exploring the data First, let’s look at orders for online and sales representative sales: monthly_tsibble %&gt;% autoplot(orders) ## Don&#39;t know how to automatically pick scale for object of type yearmon. Defaulting to continuous. Disconnect from the database: dbDisconnect(con) ## Warning in connection_release(conn@ptr): Already disconnected 9.3 Cleaning up Always have R disconnect from the database when you’re done. dbDisconnect(con) ## Warning in connection_release(conn@ptr): Already disconnected ``` Wow! Online orders really took off in the late spring - early summer of 2013! How about revenues? monthly_tsibble %&gt;% autoplot(total_revenue) ## Don&#39;t know how to automatically pick scale for object of type yearmon. Defaulting to continuous. There’s an increase, but the sales representatives always brought in more revenue than the online platform. And there’s a pronounced variation in the revenue from sales representatives on a month-to-month basis. Before moving on, let’s look at revenue per order. monthly_tsibble %&gt;% autoplot(total_revenue / orders) ## Don&#39;t know how to automatically pick scale for object of type yearmon. Defaulting to continuous. For the sales representatives, there’s still a month-to-month variation but the revenue per order appears to be bounded both below and above. However, the online revenue per order is decreasing. Note that this decline appears to be in steps between May and June each year; that could mean it’s an artifact of the database creation process and not a “natural” phenomenon. 9.4 Disconnect from the database and stop Docker dbDisconnect(con) ## Warning in connection_release(conn@ptr): Already disconnected sp_docker_stop(&quot;adventureworks&quot;) "],
["chapter-lazy-evaluation-queries.html", "Chapter 10 Lazy Evaluation and Lazy Queries 10.1 Setup 10.2 R is lazy and comes with guardrails 10.3 Lazy evaluation and lazy queries 10.3.3 Source: lazy query [?? x 4] 10.3.3 Database: postgres 10.3.3 [postgres@localhost:5432/adventureworks] 10.4 Other resources", " Chapter 10 Lazy Evaluation and Lazy Queries This chapter: Reviews lazy loading, lazy evaluation and lazy query execution Demonstrates how dplyr code gets executed (and how R determines what is translated to SQL and what is processed locally by R) Offers some further resources on lazy loading, evaluation, execution, etc. 10.1 Setup The following packages are used in this chapter: library(tidyverse) library(DBI) library(RPostgres) library(dbplyr) require(knitr) library(bookdown) library(sqlpetr) Start your adventureworks container: sqlpetr::sp_docker_start(&quot;adventureworks&quot;) Connect to the database: con &lt;- sqlpetr::sp_get_postgres_connection( user = Sys.getenv(&quot;DEFAULT_POSTGRES_USER_NAME&quot;), password = Sys.getenv(&quot;DEFAULT_POSTGRES_PASSWORD&quot;), dbname = &quot;adventureworks&quot;, port = 5432, seconds_to_test = 20, connection_tab = TRUE ) 10.2 R is lazy and comes with guardrails By design, R is both a language and an interactive development environment (IDE). As a language, R tries to be as efficient as possible. As an IDE, R creates some guardrails to make it easy and safe to work with your data. For example getOption(&quot;max.print&quot;) prevents R from printing more rows of data than you want to handle in an interactive session, with a default of 99999 lines, which may or may not suit you. On the other hand SQL is a “Structured Query Language (SQL): a standard computer language for relational database management and data manipulation.”.1 SQL has various database-specific Interactive Development Environments (IDEs), such as pgAdmin for PostgreSQL. Roger Peng explains in R Programming for Data Science that: R has maintained the original S philosophy, which is that it provides a language that is both useful for interactive work, but contains a powerful programming language for developing new tools. This is complicated when R interacts with SQL. In a vignette for dbplyr Hadley Wickham explains: The most important difference between ordinary data frames and remote database queries is that your R code is translated into SQL and executed in the database on the remote server, not in R on your local machine. When working with databases, dplyr tries to be as lazy as possible: It never pulls data into R unless you explicitly ask for it. It delays doing any work until the last possible moment: it collects together everything you want to do and then sends it to the database in one step. Exactly when, which, and how much data is returned from the dbms is the topic of this chapter. Exactly how the data is represented in the dbms and then translated to a data frame is discussed in the DBI specification. Eventually, if you are interacting with a dbms from R you will need to understand the differences between lazy loading, lazy evaluation, and lazy queries. 10.2.1 Lazy loading “Lazy loading is always used for code in packages but is optional (selected by the package maintainer) for datasets in packages.”2 Lazy loading means that the code for a particular function doesn’t actually get loaded into memory until the last minute – when it’s actually being used. 10.2.2 Lazy evaluation Essentially “Lazy evaluation is a programming strategy that allows a symbol to be evaluated only when needed.”3 That means that lazy evaluation is about symbols such as function arguments4 when they are evaluated. Tidy evaluation complicates lazy evaluation.5 10.2.3 Lazy Queries “When you create a &quot;lazy&quot; query, you’re creating a pointer to a set of conditions on the database, but the query isn’t actually run and the data isn’t actually loaded until you call &quot;next&quot; or some similar method to actually fetch the data and load it into an object.”6 10.3 Lazy evaluation and lazy queries When does a lazy query trigger data retrieval? It depends on a lot of factors, as we explore below: 10.3.1 Create a black box query for experimentation Define the three tables discussed in the previous chapter to build a black box query: sales_person_table &lt;- tbl(con, in_schema(&quot;sales&quot;, &quot;salesperson&quot;)) %&gt;% select(-rowguid) %&gt;% rename(sale_info_updated = modifieddate) employee_table &lt;- tbl(con, in_schema(&quot;humanresources&quot;, &quot;employee&quot;)) %&gt;% select(-modifieddate, -rowguid) person_table &lt;- tbl(con, in_schema(&quot;person&quot;, &quot;person&quot;)) %&gt;% select(-modifieddate, -rowguid) Here is a typical string of dplyr verbs strung together with the magrittr %&gt;% pipe command that will be used to tease out the several different behaviors that a lazy query has when passed to different R functions. This query joins three connection objects into a query we’ll call Q: Q &lt;- sales_person_table %&gt;% dplyr::left_join(employee_table, by = c(&quot;businessentityid&quot; = &quot;businessentityid&quot;)) %&gt;% dplyr::left_join(person_table , by = c(&quot;businessentityid&quot; = &quot;businessentityid&quot;)) %&gt;% dplyr::select(firstname, lastname, salesytd, birthdate) The str function gives us a hint at how R is collecting information that can be used to construct and execute a query later on: str(Q, max.level = 2) ## List of 2 ## $ src:List of 2 ## ..$ con :Formal class &#39;PqConnection&#39; [package &quot;RPostgres&quot;] with 3 slots ## ..$ disco: NULL ## ..- attr(*, &quot;class&quot;)= chr [1:4] &quot;src_PqConnection&quot; &quot;src_dbi&quot; &quot;src_sql&quot; &quot;src&quot; ## $ ops:List of 4 ## ..$ name: chr &quot;select&quot; ## ..$ x :List of 4 ## .. ..- attr(*, &quot;class&quot;)= chr [1:3] &quot;op_join&quot; &quot;op_double&quot; &quot;op&quot; ## ..$ dots: list() ## ..$ args:List of 1 ## ..- attr(*, &quot;class&quot;)= chr [1:3] &quot;op_select&quot; &quot;op_single&quot; &quot;op&quot; ## - attr(*, &quot;class&quot;)= chr [1:5] &quot;tbl_PqConnection&quot; &quot;tbl_dbi&quot; &quot;tbl_sql&quot; &quot;tbl_lazy&quot; ... 10.3.2 Experiment overview Think of Q as a black box for the moment. The following examples will show how Q is interpreted differently by different functions. It’s important to remember in the following discussion that the “and then” operator (%&gt;%) actually wraps the subsequent code inside the preceding code so that Q %&gt;% print() is equivalent to print(Q). Notation A single green check indicates that some rows are returned. Two green checks indicate that all the rows are returned. The red X indicates that no rows are returned. R code Result Q %&gt;% print() Prints x rows; same as just entering Q Q %&gt;% dplyr::as_tibble() Forces Q to be a tibble Q %&gt;% head() Prints the first 6 rows Q %&gt;% tail() Error: tail() is not supported by sql sources Q %&gt;% length() Counts the rows in Q Q %&gt;% str() Shows the top 3 levels of the object Q Q %&gt;% nrow() Attempts to determine the number of rows Q %&gt;% dplyr::tally() Counts all the rows – on the dbms side Q %&gt;% dplyr::collect(n = 20) Prints 20 rows Q %&gt;% dplyr::collect(n = 20) %&gt;% head() Prints 6 rows Q %&gt;% ggplot Plots a barchart Q %&gt;% dplyr::show_query() Translates the lazy query object into SQL The next chapter will discuss how to build queries and how to explore intermediate steps. But first, the following subsections provide a more detailed discussion of each row in the preceding table. 10.3.3 Q %&gt;% print() Remember that Q %&gt;% print() is equivalent to print(Q) and the same as just entering Q on the command line. We use the magrittr pipe operator here, because chaining functions highlights how the same object behaves differently in each use. Q %&gt;% print() ## # Source: lazy query [?? x 4] ## # Database: postgres [postgres@localhost:5432/adventureworks] ## firstname lastname salesytd birthdate ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;date&gt; ## 1 Stephen Jiang 559698. 1951-10-17 ## 2 Michael Blythe 3763178. 1968-12-25 ## 3 Linda Mitchell 4251369. 1980-02-27 ## 4 Jillian Carson 3189418. 1962-08-29 ## 5 Garrett Vargas 1453719. 1975-02-04 ## 6 Tsvi Reiter 2315186. 1974-01-18 ## 7 Pamela Ansman-Wolfe 1352577. 1974-12-06 ## 8 Shu Ito 2458536. 1968-03-09 ## 9 José Saraiva 2604541. 1963-12-11 ## 10 David Campbell 1573013. 1974-02-11 ## # … with more rows R retrieves 10 observations and 3 columns. In its role as IDE, R has provided nicely formatted output that is similar to what it prints for a tibble, with descriptive information about the dataset and each column: 10.3.3 Source: lazy query [?? x 4] 10.3.3 Database: postgres 10.3.3 [postgres@localhost:5432/adventureworks] firstname lastname salesytd birthdate R has not determined how many rows are left to retrieve as it shows with [?? x 4] and ... with more rows in the data summary. 10.3.4 Q %&gt;% dplyr::as_tibble() In contrast to print(), the as_tibble() function causes R to download the whole table, using tibble’s default of displaying only the first 10 rows. Q %&gt;% dplyr::as_tibble() ## # A tibble: 17 x 4 ## firstname lastname salesytd birthdate ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;date&gt; ## 1 Stephen Jiang 559698. 1951-10-17 ## 2 Michael Blythe 3763178. 1968-12-25 ## 3 Linda Mitchell 4251369. 1980-02-27 ## 4 Jillian Carson 3189418. 1962-08-29 ## 5 Garrett Vargas 1453719. 1975-02-04 ## 6 Tsvi Reiter 2315186. 1974-01-18 ## 7 Pamela Ansman-Wolfe 1352577. 1974-12-06 ## 8 Shu Ito 2458536. 1968-03-09 ## 9 José Saraiva 2604541. 1963-12-11 ## 10 David Campbell 1573013. 1974-02-11 ## 11 Tete Mensa-Annan 1576562. 1978-01-05 ## 12 Syed Abbas 172524. 1975-01-11 ## 13 Lynn Tsoflias 1421811. 1977-02-14 ## 14 Amy Alberts 519906. 1957-09-20 ## 15 Rachel Valdez 1827067. 1975-07-09 ## 16 Jae Pak 4116871. 1968-03-17 ## 17 Ranjit Varkey Chudukatil 3121616. 1975-09-30 10.3.5 Q %&gt;% head() The head() function is very similar to print but has a different “max.print” value. Q %&gt;% head() ## # Source: lazy query [?? x 4] ## # Database: postgres [postgres@localhost:5432/adventureworks] ## firstname lastname salesytd birthdate ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;date&gt; ## 1 Stephen Jiang 559698. 1951-10-17 ## 2 Michael Blythe 3763178. 1968-12-25 ## 3 Linda Mitchell 4251369. 1980-02-27 ## 4 Jillian Carson 3189418. 1962-08-29 ## 5 Garrett Vargas 1453719. 1975-02-04 ## 6 Tsvi Reiter 2315186. 1974-01-18 10.3.6 Q %&gt;% tail() Produces an error, because Q does not hold all of the data, so it is not possible to list the last few items from the table: try( Q %&gt;% tail(), silent = FALSE, outFile = stdout() ) ## Error : tail() is not supported by sql sources 10.3.7 Q %&gt;% length() Because the Q object is relatively complex, using str() on it prints many lines. You can glimpse what’s going on with length(): Q %&gt;% length() ## [1] 2 10.3.8 Q %&gt;% str() Looking inside shows some of what’s going on (three levels deep): Q %&gt;% str(max.level = 3) ## List of 2 ## $ src:List of 2 ## ..$ con :Formal class &#39;PqConnection&#39; [package &quot;RPostgres&quot;] with 3 slots ## ..$ disco: NULL ## ..- attr(*, &quot;class&quot;)= chr [1:4] &quot;src_PqConnection&quot; &quot;src_dbi&quot; &quot;src_sql&quot; &quot;src&quot; ## $ ops:List of 4 ## ..$ name: chr &quot;select&quot; ## ..$ x :List of 4 ## .. ..$ name: chr &quot;join&quot; ## .. ..$ x :List of 2 ## .. .. ..- attr(*, &quot;class&quot;)= chr [1:5] &quot;tbl_PqConnection&quot; &quot;tbl_dbi&quot; &quot;tbl_sql&quot; &quot;tbl_lazy&quot; ... ## .. ..$ y :List of 2 ## .. .. ..- attr(*, &quot;class&quot;)= chr [1:5] &quot;tbl_PqConnection&quot; &quot;tbl_dbi&quot; &quot;tbl_sql&quot; &quot;tbl_lazy&quot; ... ## .. ..$ args:List of 4 ## .. ..- attr(*, &quot;class&quot;)= chr [1:3] &quot;op_join&quot; &quot;op_double&quot; &quot;op&quot; ## ..$ dots: list() ## ..$ args:List of 1 ## .. ..$ vars:List of 4 ## ..- attr(*, &quot;class&quot;)= chr [1:3] &quot;op_select&quot; &quot;op_single&quot; &quot;op&quot; ## - attr(*, &quot;class&quot;)= chr [1:5] &quot;tbl_PqConnection&quot; &quot;tbl_dbi&quot; &quot;tbl_sql&quot; &quot;tbl_lazy&quot; ... 10.3.9 Q %&gt;% nrow() Notice the difference between nrow() and tally(). The nrow functions returns NA and does not execute a query: Q %&gt;% nrow() ## [1] NA 10.3.10 Q %&gt;% dplyr::tally() The tally function actually counts all the rows. Q %&gt;% dplyr::tally() ## # Source: lazy query [?? x 1] ## # Database: postgres [postgres@localhost:5432/adventureworks] ## n ## &lt;int64&gt; ## 1 17 The nrow() function knows that Q is a list. On the other hand, the tally() function tells SQL to go count all the rows. Notice that Q results in 1,000 rows – the same number of rows as film. 10.3.11 Q %&gt;% dplyr::collect() The dplyr::collect function triggers a call to the DBI:dbFetch() function behind the scenes, which forces R to download a specified number of rows: Q %&gt;% dplyr::collect(n = 20) ## # A tibble: 17 x 4 ## firstname lastname salesytd birthdate ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;date&gt; ## 1 Stephen Jiang 559698. 1951-10-17 ## 2 Michael Blythe 3763178. 1968-12-25 ## 3 Linda Mitchell 4251369. 1980-02-27 ## 4 Jillian Carson 3189418. 1962-08-29 ## 5 Garrett Vargas 1453719. 1975-02-04 ## 6 Tsvi Reiter 2315186. 1974-01-18 ## 7 Pamela Ansman-Wolfe 1352577. 1974-12-06 ## 8 Shu Ito 2458536. 1968-03-09 ## 9 José Saraiva 2604541. 1963-12-11 ## 10 David Campbell 1573013. 1974-02-11 ## 11 Tete Mensa-Annan 1576562. 1978-01-05 ## 12 Syed Abbas 172524. 1975-01-11 ## 13 Lynn Tsoflias 1421811. 1977-02-14 ## 14 Amy Alberts 519906. 1957-09-20 ## 15 Rachel Valdez 1827067. 1975-07-09 ## 16 Jae Pak 4116871. 1968-03-17 ## 17 Ranjit Varkey Chudukatil 3121616. 1975-09-30 Q %&gt;% dplyr::collect(n = 20) %&gt;% head() ## # A tibble: 6 x 4 ## firstname lastname salesytd birthdate ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;date&gt; ## 1 Stephen Jiang 559698. 1951-10-17 ## 2 Michael Blythe 3763178. 1968-12-25 ## 3 Linda Mitchell 4251369. 1980-02-27 ## 4 Jillian Carson 3189418. 1962-08-29 ## 5 Garrett Vargas 1453719. 1975-02-04 ## 6 Tsvi Reiter 2315186. 1974-01-18 The dplyr::collect function triggers the creation of a tibble and controls the number of rows that the DBMS sends to R. Notice that head only prints 6 of the 20 rows that R has retrieved. If you do not provide a value for the n argument, all of the rows will be retrieved into your R workspace. 10.3.12 Q %&gt;% ggplot Passing the Q object to ggplot executes the query and plots the result. Q %&gt;% ggplot2::ggplot(aes(birthdate, salesytd)) + geom_point() * Rewrite previous query and this comment with adventureworks in mind. Comment on the plot… 10.3.13 Q %&gt;% dplyr::show_query() Q %&gt;% dplyr::show_query() ## &lt;SQL&gt; ## SELECT &quot;firstname&quot;, &quot;lastname&quot;, &quot;salesytd&quot;, &quot;birthdate&quot; ## FROM (SELECT &quot;LHS&quot;.&quot;businessentityid&quot; AS &quot;businessentityid&quot;, &quot;LHS&quot;.&quot;territoryid&quot; AS &quot;territoryid&quot;, &quot;LHS&quot;.&quot;salesquota&quot; AS &quot;salesquota&quot;, &quot;LHS&quot;.&quot;bonus&quot; AS &quot;bonus&quot;, &quot;LHS&quot;.&quot;commissionpct&quot; AS &quot;commissionpct&quot;, &quot;LHS&quot;.&quot;salesytd&quot; AS &quot;salesytd&quot;, &quot;LHS&quot;.&quot;saleslastyear&quot; AS &quot;saleslastyear&quot;, &quot;LHS&quot;.&quot;sale_info_updated&quot; AS &quot;sale_info_updated&quot;, &quot;LHS&quot;.&quot;nationalidnumber&quot; AS &quot;nationalidnumber&quot;, &quot;LHS&quot;.&quot;loginid&quot; AS &quot;loginid&quot;, &quot;LHS&quot;.&quot;jobtitle&quot; AS &quot;jobtitle&quot;, &quot;LHS&quot;.&quot;birthdate&quot; AS &quot;birthdate&quot;, &quot;LHS&quot;.&quot;maritalstatus&quot; AS &quot;maritalstatus&quot;, &quot;LHS&quot;.&quot;gender&quot; AS &quot;gender&quot;, &quot;LHS&quot;.&quot;hiredate&quot; AS &quot;hiredate&quot;, &quot;LHS&quot;.&quot;salariedflag&quot; AS &quot;salariedflag&quot;, &quot;LHS&quot;.&quot;vacationhours&quot; AS &quot;vacationhours&quot;, &quot;LHS&quot;.&quot;sickleavehours&quot; AS &quot;sickleavehours&quot;, &quot;LHS&quot;.&quot;currentflag&quot; AS &quot;currentflag&quot;, &quot;LHS&quot;.&quot;organizationnode&quot; AS &quot;organizationnode&quot;, &quot;RHS&quot;.&quot;persontype&quot; AS &quot;persontype&quot;, &quot;RHS&quot;.&quot;namestyle&quot; AS &quot;namestyle&quot;, &quot;RHS&quot;.&quot;title&quot; AS &quot;title&quot;, &quot;RHS&quot;.&quot;firstname&quot; AS &quot;firstname&quot;, &quot;RHS&quot;.&quot;middlename&quot; AS &quot;middlename&quot;, &quot;RHS&quot;.&quot;lastname&quot; AS &quot;lastname&quot;, &quot;RHS&quot;.&quot;suffix&quot; AS &quot;suffix&quot;, &quot;RHS&quot;.&quot;emailpromotion&quot; AS &quot;emailpromotion&quot;, &quot;RHS&quot;.&quot;additionalcontactinfo&quot; AS &quot;additionalcontactinfo&quot;, &quot;RHS&quot;.&quot;demographics&quot; AS &quot;demographics&quot; ## FROM (SELECT &quot;LHS&quot;.&quot;businessentityid&quot; AS &quot;businessentityid&quot;, &quot;LHS&quot;.&quot;territoryid&quot; AS &quot;territoryid&quot;, &quot;LHS&quot;.&quot;salesquota&quot; AS &quot;salesquota&quot;, &quot;LHS&quot;.&quot;bonus&quot; AS &quot;bonus&quot;, &quot;LHS&quot;.&quot;commissionpct&quot; AS &quot;commissionpct&quot;, &quot;LHS&quot;.&quot;salesytd&quot; AS &quot;salesytd&quot;, &quot;LHS&quot;.&quot;saleslastyear&quot; AS &quot;saleslastyear&quot;, &quot;LHS&quot;.&quot;sale_info_updated&quot; AS &quot;sale_info_updated&quot;, &quot;RHS&quot;.&quot;nationalidnumber&quot; AS &quot;nationalidnumber&quot;, &quot;RHS&quot;.&quot;loginid&quot; AS &quot;loginid&quot;, &quot;RHS&quot;.&quot;jobtitle&quot; AS &quot;jobtitle&quot;, &quot;RHS&quot;.&quot;birthdate&quot; AS &quot;birthdate&quot;, &quot;RHS&quot;.&quot;maritalstatus&quot; AS &quot;maritalstatus&quot;, &quot;RHS&quot;.&quot;gender&quot; AS &quot;gender&quot;, &quot;RHS&quot;.&quot;hiredate&quot; AS &quot;hiredate&quot;, &quot;RHS&quot;.&quot;salariedflag&quot; AS &quot;salariedflag&quot;, &quot;RHS&quot;.&quot;vacationhours&quot; AS &quot;vacationhours&quot;, &quot;RHS&quot;.&quot;sickleavehours&quot; AS &quot;sickleavehours&quot;, &quot;RHS&quot;.&quot;currentflag&quot; AS &quot;currentflag&quot;, &quot;RHS&quot;.&quot;organizationnode&quot; AS &quot;organizationnode&quot; ## FROM (SELECT &quot;businessentityid&quot;, &quot;territoryid&quot;, &quot;salesquota&quot;, &quot;bonus&quot;, &quot;commissionpct&quot;, &quot;salesytd&quot;, &quot;saleslastyear&quot;, &quot;modifieddate&quot; AS &quot;sale_info_updated&quot; ## FROM sales.salesperson) &quot;LHS&quot; ## LEFT JOIN (SELECT &quot;businessentityid&quot;, &quot;nationalidnumber&quot;, &quot;loginid&quot;, &quot;jobtitle&quot;, &quot;birthdate&quot;, &quot;maritalstatus&quot;, &quot;gender&quot;, &quot;hiredate&quot;, &quot;salariedflag&quot;, &quot;vacationhours&quot;, &quot;sickleavehours&quot;, &quot;currentflag&quot;, &quot;organizationnode&quot; ## FROM humanresources.employee) &quot;RHS&quot; ## ON (&quot;LHS&quot;.&quot;businessentityid&quot; = &quot;RHS&quot;.&quot;businessentityid&quot;) ## ) &quot;LHS&quot; ## LEFT JOIN (SELECT &quot;businessentityid&quot;, &quot;persontype&quot;, &quot;namestyle&quot;, &quot;title&quot;, &quot;firstname&quot;, &quot;middlename&quot;, &quot;lastname&quot;, &quot;suffix&quot;, &quot;emailpromotion&quot;, &quot;additionalcontactinfo&quot;, &quot;demographics&quot; ## FROM person.person) &quot;RHS&quot; ## ON (&quot;LHS&quot;.&quot;businessentityid&quot; = &quot;RHS&quot;.&quot;businessentityid&quot;) ## ) &quot;dbplyr_009&quot; Hand-written SQL code to do the same job will probably look a lot nicer and could be more efficient, but functionally dplyr does the job. ## Disconnect from the database and stop Docker dbDisconnect(con) sp_docker_stop(&quot;adventureworks&quot;) 10.4 Other resources Benjamin S. Baumer. 2017. A Grammar for Reproducible and Painless Extract-Transform-Load Operations on Medium Data. https://arxiv.org/abs/1708.07073 dplyr Reference documentation: Remote tables. https://dplyr.tidyverse.org/reference/index.html#section-remote-tables Data Carpentry. SQL Databases and R. https://datacarpentry.org/R-ecology-lesson/05-r-and-databases.html https://www.techopedia.com/definition/1245/structured-query-language-sql↩ https://cran.r-project.org/doc/manuals/r-release/R-ints.html#Lazy-loading↩ https://colinfay.me/lazyeval/↩ http://adv-r.had.co.nz/Functions.html#function-arguments↩ https://colinfay.me/tidyeval-1/↩ https://www.quora.com/What-is-a-lazy-query↩ "],
["chapter-lazy-evaluation-and-timing.html", "Chapter 11 Lazy Evaluation and Execution Environment 11.1 Setup 11.2 Disconnect from the database and stop Docker 11.3 Other resources", " Chapter 11 Lazy Evaluation and Execution Environment This chapter: Builds on the lazy loading discussion in the previous chapter Demonstrates how the use of the dplyr::collect() creates a boundary between code that is sent to a dbms and code that is executed locally 11.1 Setup The following packages are used in this chapter: library(tidyverse) library(DBI) library(RPostgres) library(dbplyr) require(knitr) library(bookdown) library(sqlpetr) If you have not yet set up the Docker container with PostgreSQL and the dvdrental database, go back to [those instructions][Build the pet-sql Docker Image] to configure your environment. Otherwise, start your adventureworks container: sqlpetr::sp_docker_start(&quot;adventureworks&quot;) Connect to the database: con &lt;- sqlpetr::sp_get_postgres_connection( user = Sys.getenv(&quot;DEFAULT_POSTGRES_USER_NAME&quot;), password = Sys.getenv(&quot;DEFAULT_POSTGRES_PASSWORD&quot;), dbname = &quot;adventureworks&quot;, port = 5432, seconds_to_test = 20, connection_tab = TRUE ) Here is a simple string of dplyr verbs similar to the query used to illustrate issues in the last chapter: Note that in the previous example we follow this book’s convention of creating a connection object to each table and fully qualifying function names (e.g., specifying the package). In practice, it’s possible and convenient to use more abbreviated notation. Q &lt;- tbl(con, in_schema(&quot;sales&quot;, &quot;salesperson&quot;)) %&gt;% left_join(tbl(con, in_schema(&quot;humanresources&quot;, &quot;employee&quot;)), by = c(&quot;businessentityid&quot; = &quot;businessentityid&quot;)) %&gt;% select(birthdate, saleslastyear) Q ## # Source: lazy query [?? x 2] ## # Database: postgres [postgres@localhost:5432/adventureworks] ## birthdate saleslastyear ## &lt;date&gt; &lt;dbl&gt; ## 1 1951-10-17 0 ## 2 1968-12-25 1750406. ## 3 1980-02-27 1439156. ## 4 1962-08-29 1997186. ## 5 1975-02-04 1620277. ## 6 1974-01-18 1849641. ## 7 1974-12-06 1927059. ## 8 1968-03-09 2073506. ## 9 1963-12-11 2038235. ## 10 1974-02-11 1371635. ## # … with more rows 11.1.1 Experiment overview Think of Q as a black box for the moment. The following examples will show how Q is interpreted differently by different functions. It’s important to remember in the following discussion that the “and then” operator (%&gt;%) actually wraps the subsequent code inside the preceding code so that Q %&gt;% print() is equivalent to print(Q). Notation Symbol Explanation A single green check indicates that some rows are returned. Two green checks indicate that all the rows are returned. The red X indicates that no rows are returned. R code Result Time-based, execution environment issues Qc &lt;- Q %&gt;% count(saleslastyear, sort = TRUE) Extends the lazy query object The next chapter will discuss how to build queries and how to explore intermediate steps. But first, the following subsections provide a more detailed discussion of each row in the preceding table. 11.1.2 Time-based, execution environment issues Remember that if the expression is assigned to an object, it is not executed. If an expression is entered on the command line or appears in your script by itself, a print() function is implied. These two are different: Q %&gt;% sum(saleslastyear) Q_query &lt;- Q %&gt;% sum(saleslastyear) This behavior is the basis of a useful debugging and development process where queries are built up incrementally. 11.1.3 Q %&gt;% more dplyr Because the following statement implies a print() function at the end, we can run it repeatedly, adding dplyr expressions, and only get 10 rows back. Every time we add a dplyr expression to a chain, R will rewrite the SQL code. For example: As we understand more about the data, we simply add dplyr expressions to pinpoint what we are looking for: Q %&gt;% filter(saleslastyear &gt; 40) %&gt;% arrange(desc(saleslastyear)) ## # Source: lazy query [?? x 2] ## # Database: postgres [postgres@localhost:5432/adventureworks] ## # Ordered by: desc(saleslastyear) ## birthdate saleslastyear ## &lt;date&gt; &lt;dbl&gt; ## 1 1975-09-30 2396540. ## 2 1977-02-14 2278549. ## 3 1968-03-09 2073506. ## 4 1963-12-11 2038235. ## 5 1962-08-29 1997186. ## 6 1974-12-06 1927059. ## 7 1974-01-18 1849641. ## 8 1968-12-25 1750406. ## 9 1968-03-17 1635823. ## 10 1975-02-04 1620277. ## # … with more rows Q %&gt;% summarize(total_sales = sum(saleslastyear, na.rm = TRUE), sales_persons_count = n()) ## # Source: lazy query [?? x 2] ## # Database: postgres [postgres@localhost:5432/adventureworks] ## total_sales sales_persons_count ## &lt;dbl&gt; &lt;int64&gt; ## 1 23685964. 17 When all the accumulated dplyr verbs are executed, they are submitted to the dbms and the number of rows that are returned follow the same rules as discussed above. ### Interspersing SQL and dplyr Q %&gt;% # mutate(birthdate = date(birthdate)) %&gt;% show_query() ## &lt;SQL&gt; ## SELECT &quot;birthdate&quot;, &quot;saleslastyear&quot; ## FROM (SELECT &quot;LHS&quot;.&quot;businessentityid&quot; AS &quot;businessentityid&quot;, &quot;LHS&quot;.&quot;territoryid&quot; AS &quot;territoryid&quot;, &quot;LHS&quot;.&quot;salesquota&quot; AS &quot;salesquota&quot;, &quot;LHS&quot;.&quot;bonus&quot; AS &quot;bonus&quot;, &quot;LHS&quot;.&quot;commissionpct&quot; AS &quot;commissionpct&quot;, &quot;LHS&quot;.&quot;salesytd&quot; AS &quot;salesytd&quot;, &quot;LHS&quot;.&quot;saleslastyear&quot; AS &quot;saleslastyear&quot;, &quot;LHS&quot;.&quot;rowguid&quot; AS &quot;rowguid.x&quot;, &quot;LHS&quot;.&quot;modifieddate&quot; AS &quot;modifieddate.x&quot;, &quot;RHS&quot;.&quot;nationalidnumber&quot; AS &quot;nationalidnumber&quot;, &quot;RHS&quot;.&quot;loginid&quot; AS &quot;loginid&quot;, &quot;RHS&quot;.&quot;jobtitle&quot; AS &quot;jobtitle&quot;, &quot;RHS&quot;.&quot;birthdate&quot; AS &quot;birthdate&quot;, &quot;RHS&quot;.&quot;maritalstatus&quot; AS &quot;maritalstatus&quot;, &quot;RHS&quot;.&quot;gender&quot; AS &quot;gender&quot;, &quot;RHS&quot;.&quot;hiredate&quot; AS &quot;hiredate&quot;, &quot;RHS&quot;.&quot;salariedflag&quot; AS &quot;salariedflag&quot;, &quot;RHS&quot;.&quot;vacationhours&quot; AS &quot;vacationhours&quot;, &quot;RHS&quot;.&quot;sickleavehours&quot; AS &quot;sickleavehours&quot;, &quot;RHS&quot;.&quot;currentflag&quot; AS &quot;currentflag&quot;, &quot;RHS&quot;.&quot;rowguid&quot; AS &quot;rowguid.y&quot;, &quot;RHS&quot;.&quot;modifieddate&quot; AS &quot;modifieddate.y&quot;, &quot;RHS&quot;.&quot;organizationnode&quot; AS &quot;organizationnode&quot; ## FROM sales.salesperson AS &quot;LHS&quot; ## LEFT JOIN humanresources.employee AS &quot;RHS&quot; ## ON (&quot;LHS&quot;.&quot;businessentityid&quot; = &quot;RHS&quot;.&quot;businessentityid&quot;) ## ) &quot;dbplyr_006&quot; # Need to come up with a different example illustrating where # the `collect` statement goes. # sales_person_table %&gt;% # mutate(birthdate = date(birthdate)) # # try(sales_person_table %&gt;% # mutate(birthdate = lubridate::date(birthdate)) # ) # # sales_person_table %&gt;% collect() %&gt;% # mutate(birthdate = lubridate::date(birthdate)) This may not be relevant in the context where it turns out that dates in adventureworks come through as date! The idea is to show how functions are interpreted BEFORE sending to the SQL translator. to_char &lt;- function(date, fmt) {return(fmt)} # sales_person_table %&gt;% # mutate(birthdate = to_char(birthdate, &quot;YYYY-MM&quot;)) %&gt;% # show_query() # # sales_person_table %&gt;% # mutate(birthdate = to_char(birthdate, &quot;YYYY-MM&quot;)) 11.1.4 Many handy R functions can’t be translated to SQL It just so happens that PostgreSQL has a date function that does the same thing as the date function in the lubridate package. In the following code the date function is executed by PostreSQL. # sales_person_table %&gt;% mutate(birthdate = date(birthdate)) If we specify that we want to use the lubridate version (or any number of other R functions) they are passed to the dbms unless we explicitly tell dplyr to stop translating and bring the results back to the R environment for local processing. try(sales_person_table %&gt;% collect() %&gt;% mutate(birthdate = lubridate::date(birthdate))) ## Error in eval(lhs, parent, parent) : ## object &#39;sales_person_table&#39; not found 11.1.5 Further lazy execution examples See more examples of lazy execution here. 11.2 Disconnect from the database and stop Docker dbDisconnect(con) sp_docker_stop(&quot;adventureworks&quot;) 11.3 Other resources Benjamin S. Baumer. 2017. A Grammar for Reproducible and Painless Extract-Transform-Load Operations on Medium Data. https://arxiv.org/abs/1708.07073 dplyr Reference documentation: Remote tables. https://dplyr.tidyverse.org/reference/index.html#section-remote-tables Data Carpentry. SQL Databases and R. https://datacarpentry.org/R-ecology-lesson/05-r-and-databases.html "],
["chapter-leveraging-database-views.html", "Chapter 12 Leveraging Database Views 12.1 Setup our standard working environment 12.2 The role of database views 12.3 Reproduce the view with dplyr 12.4 Save a view in the database", " Chapter 12 Leveraging Database Views This chapter demonstrates how to: Assess database views, understand their importance Unpack a database view and check its assumptions Create a database view either for personal use or for submittal to your enterprise DBA 12.1 Setup our standard working environment Use these libraries: library(tidyverse) library(DBI) library(RPostgres) library(glue) require(knitr) library(dbplyr) library(sqlpetr) library(bookdown) library(here) library(lubridate) library(skimr) library(DiagrammeR) library(scales) # ggplot xy scales theme_set(theme_light()) Connect to adventureworks: sp_docker_start(&quot;adventureworks&quot;) Sys.sleep(sleep_default) con &lt;- sp_get_postgres_connection( host = &quot;localhost&quot;, port = 5432, user = &quot;postgres&quot;, password = &quot;postgres&quot;, dbname = &quot;adventureworks&quot;, seconds_to_test = sleep_default, connection_tab = TRUE ) 12.2 The role of database views A database view is an SQL query that is stored in the database. Most views are used for data retrieval, since they usually denormalize the tables involved. Because they are standardized and well-understood, they can save you a lot of work. 12.2.1 Why database views are useful Database views are useful for many reasons. Authoritative: database views are typically written by the business application vendor or DBA, so they contain authoritative knowledge about the structure and intended use of the database. Performance: views are designed to gather data in an efficient way, using all the indexes in an efficient sequence and doing as much work on the database server as possible. Abstraction: views are abstractions or simplifications of complex queries that provide customary (useful) aggregations. Common examples would be monthly totals or aggregation of activity tied to one individual. Reuse: a view puts commonly used code in one place where it can be used for many purposes by many people. If there is a change or a problem found in a view, it only needs to be fixed in one place, rather than having to change many places downstream. Security: a view can give selective access to someone who does not have access to underlying tables or columns. Provenance: views standardize data provenance. For example, the AdventureWorks database all of them are named in a consistent way that suggests the underlying tables that they query. And they all start with a v. 12.2.2 Rely on and be critical of views Because they represent a conventional view of the database, a view may seem quite boring; remember why they are very important. Just because they are conventional and authorized, they may still need verification or auditing when used for a purpose other than the original intent. They can guide you toward what you need from the database but they could also mislead because they are easy to use and available. People may forget why a specific view exists and who is using it. Therefore any given view might be a forgotten vestige or part of an production data pipeline or might a priceless nugget of insight. 12.2.3 How to unpack and inspect a view From a retrieval perspective a database view is just like any other table. Using a view to retrieve data from the database will be completely standard across all flavors of SQL. (To find out what a view does behind the scenes requires that you use functions that are not standard.) v_salesperson_sales_by_fiscal_years_data &lt;- tbl(con, in_schema(&quot;sales&quot;,&quot;vsalespersonsalesbyfiscalyearsdata&quot;)) %&gt;% collect() str(v_salesperson_sales_by_fiscal_years_data) ## Classes &#39;tbl_df&#39;, &#39;tbl&#39; and &#39;data.frame&#39;: 48 obs. of 6 variables: ## $ salespersonid : int 275 275 275 275 276 276 276 276 277 277 ... ## $ fullname : chr &quot;Michael G Blythe&quot; &quot;Michael G Blythe&quot; &quot;Michael G Blythe&quot; &quot;Michael G Blythe&quot; ... ## $ jobtitle : chr &quot;Sales Representative&quot; &quot;Sales Representative&quot; &quot;Sales Representative&quot; &quot;Sales Representative&quot; ... ## $ salesterritory: chr &quot;Northeast&quot; &quot;Northeast&quot; &quot;Northeast&quot; &quot;Northeast&quot; ... ## $ salestotal : num 63763 2399593 3765459 3065088 5476 ... ## $ fiscalyear : num 2011 2012 2013 2014 2011 ... skim(tbl(con, in_schema(&quot;sales&quot;,&quot;vsalespersonsalesbyfiscalyearsdata&quot;))) Table 12.1: Data summary Name tbl(con, in_schema(&quot;sales… Number of rows 48 Number of columns 6 _______________________ Column type frequency: character 3 numeric 3 ________________________ Group variables None Variable type: character skim_variable n_missing complete_rate min max empty n_unique whitespace fullname 0 1 9 26 0 14 0 jobtitle 0 1 20 20 0 1 0 salesterritory 0 1 6 14 0 10 0 Variable type: numeric skim_variable n_missing complete_rate mean sd p0 p25 p50 p75 p100 hist salespersonid 0 1 281.19 4.57 275.00 277.75 280.50 283.25 290 ▇▆▅▁▃ salestotal 0 1 1635214.51 1243833.87 5475.95 533827.70 1371169.72 2409498.88 4188307 ▇▇▆▃▃ fiscalyear 0 1 2012.69 1.09 2011.00 2012.00 2013.00 2014.00 2014 ▅▆▁▇▇ tbl(con, in_schema(&quot;sales&quot;,&quot;vsalespersonsalesbyfiscalyearsdata&quot;)) %&gt;% filter(salespersonid == 275) ## # Source: lazy query [?? x 6] ## # Database: postgres [postgres@localhost:5432/adventureworks] ## salespersonid fullname jobtitle salesterritory salestotal fiscalyear ## &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 275 Michael G B… Sales Represe… Northeast 63763. 2011 ## 2 275 Michael G B… Sales Represe… Northeast 2399593. 2012 ## 3 275 Michael G B… Sales Represe… Northeast 3765459. 2013 ## 4 275 Michael G B… Sales Represe… Northeast 3065088. 2014 Local idioms for looking at a view itself will vary. Here is the code to retrieve a PostgreSQL view (using the pg_get_viewdef function): view_definition &lt;- dbGetQuery(con, &quot;select pg_get_viewdef(&#39;sales.vsalespersonsalesbyfiscalyearsdata&#39;, true)&quot;) str(view_definition) ## &#39;data.frame&#39;: 1 obs. of 1 variable: ## $ pg_get_viewdef: chr &quot; SELECT granular.salespersonid,\\n granular.fullname,\\n granular.jobtitle,\\n granular.salesterritory,\\n&quot;| __truncated__ cat(str_replace_all(view_definition$pg_get_viewdef, &quot;\\\\\\\\\\\\\\\\n&quot;, &quot;\\\\\\\\n&quot;)) ## SELECT granular.salespersonid, ## granular.fullname, ## granular.jobtitle, ## granular.salesterritory, ## sum(granular.subtotal) AS salestotal, ## granular.fiscalyear ## FROM ( SELECT soh.salespersonid, ## ((p.firstname::text || &#39; &#39;::text) || COALESCE(p.middlename::text || &#39; &#39;::text, &#39;&#39;::text)) || p.lastname::text AS fullname, ## e.jobtitle, ## st.name AS salesterritory, ## soh.subtotal, ## date_part(&#39;year&#39;::text, soh.orderdate + &#39;6 mons&#39;::interval) AS fiscalyear ## FROM sales.salesperson sp ## JOIN sales.salesorderheader soh ON sp.businessentityid = soh.salespersonid ## JOIN sales.salesterritory st ON sp.territoryid = st.territoryid ## JOIN humanresources.employee e ON soh.salespersonid = e.businessentityid ## JOIN person.person p ON p.businessentityid = sp.businessentityid) granular ## GROUP BY granular.salespersonid, granular.fullname, granular.jobtitle, granular.salesterritory, granular.fiscalyear; Even if you don’t intend to become fluent in SQL, it’s useful to read as much of it as possible. To understand this query, you really need to have the Entity Relationship Diagram (ERD) handy. The ERD for AdventureWorks is here 12.3 Reproduce the view with dplyr Save and study the SQL. It can be helpful to actually mark up the ERD to identify the specific tables that are involved in the view you are going to reproduce. Define each table that is involved and identify the columns that will be needed from that table. The tables that are involved are: employee person sales_person sales_order_header sales_territory Select the columns and do any necessary changes or renaming. In this case we follow the convention that any column that we change or create on the fly uses a snake case naming con vention. sales_order_header &lt;- tbl(con, in_schema(&quot;sales&quot;, &quot;salesorderheader&quot;)) %&gt;% # Because we&#39;re lazy, we will keep both a crude `year` column and `orderdate` for later use mutate(sales_order_year = year(orderdate)) %&gt;% select(sales_order_year, salespersonid, subtotal, orderdate) sales_territory &lt;- tbl(con, in_schema(&quot;sales&quot;, &quot;salesterritory&quot;)) %&gt;% select(territoryid, territory_name = name) sales_person &lt;- tbl(con, in_schema(&quot;sales&quot;, &quot;salesperson&quot;)) %&gt;% select(businessentityid, territoryid) employee &lt;- tbl(con, in_schema(&quot;humanresources&quot;, &quot;employee&quot;)) %&gt;% select(businessentityid, jobtitle) person &lt;- tbl(con, in_schema(&quot;person&quot;, &quot;person&quot;)) %&gt;% mutate(full_name = paste(firstname, middlename, lastname)) %&gt;% select(businessentityid, full_name) Double check on the names that are defined in each tbl object. First define a function to show the names of columns you will retrieve. getnames &lt;- function(table) { {table} %&gt;% collect(n = 5) %&gt;% names() } Verify the names selected: getnames(employee) ## [1] &quot;businessentityid&quot; &quot;jobtitle&quot; getnames(person) ## [1] &quot;businessentityid&quot; &quot;full_name&quot; getnames(sales_person) ## [1] &quot;businessentityid&quot; &quot;territoryid&quot; getnames(sales_order_header) ## [1] &quot;sales_order_year&quot; &quot;salespersonid&quot; &quot;subtotal&quot; &quot;orderdate&quot; getnames(sales_territory) ## [1] &quot;territoryid&quot; &quot;territory_name&quot; Join all of the data pertaining to a person. salesperson_info &lt;- sales_person %&gt;% left_join(employee) %&gt;% left_join(person) %&gt;% left_join(sales_territory) %&gt;% collect() ## Joining, by = &quot;businessentityid&quot;Joining, by = &quot;businessentityid&quot;Joining, by = ## &quot;territoryid&quot; str(salesperson_info) ## Classes &#39;tbl_df&#39;, &#39;tbl&#39; and &#39;data.frame&#39;: 17 obs. of 5 variables: ## $ businessentityid: int 274 275 276 277 278 279 280 281 282 283 ... ## $ territoryid : int NA 2 4 3 6 5 1 4 6 1 ... ## $ jobtitle : chr &quot;North American Sales Manager&quot; &quot;Sales Representative&quot; &quot;Sales Representative&quot; &quot;Sales Representative&quot; ... ## $ full_name : chr &quot;Stephen Y Jiang&quot; &quot;Michael G Blythe&quot; &quot;Linda C Mitchell&quot; &quot;Jillian Carson&quot; ... ## $ territory_name : chr NA &quot;Northeast&quot; &quot;Southwest&quot; &quot;Central&quot; ... Do a crude version with sales_order_year. All of the work can be done on the database server. sales_data_year &lt;- sales_person %&gt;% left_join(sales_order_header, by = c(&quot;businessentityid&quot; = &quot;salespersonid&quot;)) %&gt;% group_by(businessentityid, sales_order_year) %&gt;% summarize(sales_total = sum(subtotal, na.rm = TRUE)) %&gt;% collect() Lubridate makes it very easy to convert orderdate to fiscal_year. Doing that conversion interleaving dplyr and ANSI-STANDARD SQL is harder. Too lazy! Therefore we just pull the data from the server after the left_join and do the rest of the job on the R side. sales_data_fiscal_year &lt;- sales_person %&gt;% left_join(sales_order_header, by = c(&quot;businessentityid&quot; = &quot;salespersonid&quot;)) %&gt;% collect() %&gt;% mutate(fiscal_year = year(orderdate %m+% months(6))) %&gt;% group_by(businessentityid, fiscal_year) %&gt;% summarize(sales_total = sum(subtotal, na.rm = TRUE)) %&gt;% ungroup() Put the two parts together: sales_data_fiscal_year and person_info to yeild the final query. salesperson_sales_by_fiscal_years_dplyr &lt;- sales_data_fiscal_year %&gt;% left_join(salesperson_info) %&gt;% filter(!is.na(territoryid)) ## Joining, by = &quot;businessentityid&quot; Notice that we’re droping the Sales Managers – who don’t have a territoryid. 12.3.1 Compare the two versions Use pivot_wider to make it easier to compare the native view to our dplyr version. salesperson_sales_by_fiscal_years_dplyr %&gt;% select(-jobtitle, - territoryid) %&gt;% pivot_wider(names_from = fiscal_year, values_from = sales_total) ## # A tibble: 14 x 7 ## businessentityid full_name territory_name `2011` `2012` `2013` `2014` ## &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 275 Michael G Bly… Northeast 63763. 2399593. 3.77e6 3.07e6 ## 2 276 Linda C Mitch… Southwest 5476. 3013884. 4.06e6 3.28e6 ## 3 277 Jillian Carson Central 46696. 3496244. 3.94e6 2.58e6 ## 4 278 Garrett R Var… Canada 9109. 1254087. 1.18e6 1.17e6 ## 5 279 Tsvi Michael … Southeast 104419. 3037175. 2.16e6 1.87e6 ## 6 280 Pamela O Ansm… Northwest 24433. 1533076. 5.88e5 1.18e6 ## 7 281 Shu K Ito Southwest 59708. 1953001. 2.44e6 1.98e6 ## 8 282 José Edvaldo … Canada 106252. 2171995. 1.39e6 2.26e6 ## 9 283 David R Campb… Northwest 69473. 1291905. 1.15e6 1.22e6 ## 10 284 Tete A Mensa-… Northwest NA NA 9.59e5 1.35e6 ## 11 286 Lynn N Tsofli… Australia NA NA 1.84e5 1.24e6 ## 12 288 Rachel B Vald… Germany NA NA 3.72e5 1.46e6 ## 13 289 Jae B Pak United Kingdom NA 963345. 4.19e6 3.35e6 ## 14 290 Ranjit R Vark… France NA 360246. 1.77e6 2.38e6 v_salesperson_sales_by_fiscal_years_data %&gt;% select(-jobtitle) %&gt;% pivot_wider(names_from = fiscalyear, values_from = salestotal) ## # A tibble: 14 x 7 ## salespersonid fullname salesterritory `2011` `2012` `2013` `2014` ## &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 275 Michael G Blythe Northeast 63763. 2399593. 3.77e6 3.07e6 ## 2 276 Linda C Mitchell Southwest 5476. 3013884. 4.06e6 3.28e6 ## 3 277 Jillian Carson Central 46696. 3496244. 3.94e6 2.58e6 ## 4 278 Garrett R Vargas Canada 9109. 1254087. 1.18e6 1.17e6 ## 5 279 Tsvi Michael Re… Southeast 104419. 3037175. 2.16e6 1.87e6 ## 6 280 Pamela O Ansman… Northwest 24433. 1533076. 5.88e5 1.18e6 ## 7 281 Shu K Ito Southwest 59708. 1953001. 2.44e6 1.98e6 ## 8 282 José Edvaldo Sa… Canada 106252. 2171995. 1.39e6 2.26e6 ## 9 283 David R Campbell Northwest 69473. 1291905. 1.15e6 1.22e6 ## 10 284 Tete A Mensa-An… Northwest NA NA 9.59e5 1.35e6 ## 11 286 Lynn N Tsoflias Australia NA NA 1.84e5 1.24e6 ## 12 288 Rachel B Valdez Germany NA NA 3.72e5 1.46e6 ## 13 289 Jae B Pak United Kingdom NA 963345. 4.19e6 3.35e6 ## 14 290 Ranjit R Varkey… France NA 360246. 1.77e6 2.38e6 The column names don’t match up, partly because we are using snake case convention for derived elements. names(salesperson_sales_by_fiscal_years_dplyr) %&gt;% sort() ## [1] &quot;businessentityid&quot; &quot;fiscal_year&quot; &quot;full_name&quot; &quot;jobtitle&quot; ## [5] &quot;sales_total&quot; &quot;territory_name&quot; &quot;territoryid&quot; names(v_salesperson_sales_by_fiscal_years_data) %&gt;% sort() ## [1] &quot;fiscalyear&quot; &quot;fullname&quot; &quot;jobtitle&quot; &quot;salespersonid&quot; ## [5] &quot;salesterritory&quot; &quot;salestotal&quot; Why 3 sales folks in vsalesperson don’t show up in 2014 vsalespersonsalesbyfiscalyearsdata Different environments / SQL dialects 12.3.2 Revise the view What about by month? This could be motivation for creating a new view that does aggregation in the database, rather than in R. See SQL code for ‘vsalespersonsalesbyfiscalyearsdata’. Consider: Modifying that to include quantity of sales. Modifying that to include monthly totals, in addition to the yearly totals that it already has. Why are 3 of the sales people from ‘vsalesperson’ missing in ‘vsalespersonsalesbyfiscalyearsdata’? Amy Alberts Stephen Jiang Syed Abbas Making the change may not be your prerogative, but it’s your responsibility to propose any reasonable changes to those who have the authority to make the make the change. 12.4 Save a view in the database "],
["chapter-postgresql-metadata.html", "Chapter 13 Getting metadata about and from PostgreSQL 13.1 Views trick parked here for the time being 13.2 Database contents and structure 13.3 What columns do those tables contain? 13.4 Characterizing how things are named 13.5 Database keys 13.6 Creating your own data dictionary 13.7 Save your work!", " Chapter 13 Getting metadata about and from PostgreSQL This chapter demonstrates: What kind of data about the database is contained in a dbms Several methods for obtaining metadata from the dbms The following packages are used in this chapter: library(tidyverse) library(DBI) library(RPostgres) library(glue) library(here) require(knitr) library(dbplyr) library(sqlpetr) Assume that the Docker container with PostgreSQL and the dvdrental database are ready to go. sp_docker_start(&quot;adventureworks&quot;) Connect to the database: con &lt;- sqlpetr::sp_get_postgres_connection( user = Sys.getenv(&quot;DEFAULT_POSTGRES_USER_NAME&quot;), password = Sys.getenv(&quot;DEFAULT_POSTGRES_PASSWORD&quot;), dbname = &quot;adventureworks&quot;, port = 5432, seconds_to_test = 20, connection_tab = TRUE ) 13.1 Views trick parked here for the time being 13.1.1 Explore the vsalelsperson and vsalespersonsalesbyfiscalyearsdata views The following trick goes later in the book, where it’s used to prove the finding that to make sense of othe data you need to cat(unlist(dbGetQuery(con, &quot;select pg_get_viewdef(&#39;sales.vsalesperson&#39;, true)&quot;))) ## SELECT s.businessentityid, ## p.title, ## p.firstname, ## p.middlename, ## p.lastname, ## p.suffix, ## e.jobtitle, ## pp.phonenumber, ## pnt.name AS phonenumbertype, ## ea.emailaddress, ## p.emailpromotion, ## a.addressline1, ## a.addressline2, ## a.city, ## sp.name AS stateprovincename, ## a.postalcode, ## cr.name AS countryregionname, ## st.name AS territoryname, ## st.&quot;group&quot; AS territorygroup, ## s.salesquota, ## s.salesytd, ## s.saleslastyear ## FROM sales.salesperson s ## JOIN humanresources.employee e ON e.businessentityid = s.businessentityid ## JOIN person.person p ON p.businessentityid = s.businessentityid ## JOIN person.businessentityaddress bea ON bea.businessentityid = s.businessentityid ## JOIN person.address a ON a.addressid = bea.addressid ## JOIN person.stateprovince sp ON sp.stateprovinceid = a.stateprovinceid ## JOIN person.countryregion cr ON cr.countryregioncode::text = sp.countryregioncode::text ## LEFT JOIN sales.salesterritory st ON st.territoryid = s.territoryid ## LEFT JOIN person.emailaddress ea ON ea.businessentityid = p.businessentityid ## LEFT JOIN person.personphone pp ON pp.businessentityid = p.businessentityid ## LEFT JOIN person.phonenumbertype pnt ON pnt.phonenumbertypeid = pp.phonenumbertypeid; ## pg_get_viewdef ## 1 SELECT granular.salespersonid,\\n granular.fullname,\\n granular.jobtitle,\\n granular.salesterritory,\\n sum(granular.subtotal) AS salestotal,\\n granular.fiscalyear\\n FROM ( SELECT soh.salespersonid,\\n ((p.firstname::text || &#39; &#39;::text) || COALESCE(p.middlename::text || &#39; &#39;::text, &#39;&#39;::text)) || p.lastname::text AS fullname,\\n e.jobtitle,\\n st.name AS salesterritory,\\n soh.subtotal,\\n date_part(&#39;year&#39;::text, soh.orderdate + &#39;6 mons&#39;::interval) AS fiscalyear\\n FROM sales.salesperson sp\\n JOIN sales.salesorderheader soh ON sp.businessentityid = soh.salespersonid\\n JOIN sales.salesterritory st ON sp.territoryid = st.territoryid\\n JOIN humanresources.employee e ON soh.salespersonid = e.businessentityid\\n JOIN person.person p ON p.businessentityid = sp.businessentityid) granular\\n GROUP BY granular.salespersonid, granular.fullname, granular.jobtitle, granular.salesterritory, granular.fiscalyear; 13.2 Database contents and structure After just looking at the data you seek, it might be worthwhile stepping back and looking at the big picture. 13.2.1 Database structure For large or complex databases you need to use both the available documentation for your database (e.g., the dvdrental database) and the other empirical tools that are available. For example it’s worth learning to interpret the symbols in an Entity Relationship Diagram: The information_schema is a trove of information about the database. Its format is more or less consistent across the different SQL implementations that are available. Here we explore some of what’s available using several different methods. PostgreSQL stores a lot of metadata. 13.2.2 Contents of the information_schema For this chapter R needs the dbplyr package to access alternate schemas. A schema is an object that contains one or more tables. Most often there will be a default schema, but to access the metadata, you need to explicitly specify which schema contains the data you want. 13.2.3 What tables are in the database? The simplest way to get a list of tables is with … NO LONGER WORKS: schema_list &lt;- tbl(con, in_schema(&quot;information_schema&quot;, &quot;schemata&quot;)) %&gt;% select(catalog_name, schema_name, schema_owner) %&gt;% collect() sp_print_df(head(schema_list)) ### Digging into the information_schema We usually need more detail than just a list of tables. Most SQL databases have an information_schema that has a standard structure to describe and control the database. The information_schema is in a different schema from the default, so to connect to the tables table in the information_schema we connect to the database in a different way: table_info_schema_table &lt;- tbl(con, dbplyr::in_schema(&quot;information_schema&quot;, &quot;tables&quot;)) The information_schema is large and complex and contains 343 tables. So it’s easy to get lost in it. This query retrieves a list of the tables in the database that includes additional detail, not just the name of the table. table_info &lt;- table_info_schema_table %&gt;% # filter(table_schema == &quot;public&quot;) %&gt;% select(table_catalog, table_schema, table_name, table_type) %&gt;% arrange(table_type, table_name) %&gt;% collect() sp_print_df(head(table_info)) In this context table_catalog is synonymous with database. Notice that VIEWS are composites made up of one or more BASE TABLES. The SQL world has its own terminology. For example rs is shorthand for result set. That’s equivalent to using df for a data frame. The following SQL query returns the same information as the previous dplyr code. rs &lt;- dbGetQuery( con, &quot;select table_catalog, table_schema, table_name, table_type from information_schema.tables where table_schema not in (&#39;pg_catalog&#39;,&#39;information_schema&#39;) order by table_type, table_name ;&quot; ) sp_print_df(head(rs)) 13.3 What columns do those tables contain? Of course, the DBI package has a dbListFields function that provides the simplest way to get the minimum, a list of column names: # DBI::dbListFields(con, &quot;rental&quot;) But the information_schema has a lot more useful information that we can use. columns_info_schema_table &lt;- tbl(con, dbplyr::in_schema(&quot;information_schema&quot;, &quot;columns&quot;)) Since the information_schema contains 2961 columns, we are narrowing our focus to just one table. This query retrieves more information about the rental table: columns_info_schema_info &lt;- columns_info_schema_table %&gt;% # filter(table_schema == &quot;public&quot;) %&gt;% select( table_catalog, table_schema, table_name, column_name, data_type, ordinal_position, character_maximum_length, column_default, numeric_precision, numeric_precision_radix ) %&gt;% collect(n = Inf) %&gt;% mutate(data_type = case_when( data_type == &quot;character varying&quot; ~ paste0(data_type, &quot; (&quot;, character_maximum_length, &quot;)&quot;), data_type == &quot;real&quot; ~ paste0(data_type, &quot; (&quot;, numeric_precision, &quot;,&quot;, numeric_precision_radix, &quot;)&quot;), TRUE ~ data_type )) %&gt;% # filter(table_name == &quot;rental&quot;) %&gt;% select(-table_schema, -numeric_precision, -numeric_precision_radix) glimpse(columns_info_schema_info) ## Observations: 2,961 ## Variables: 7 ## $ table_catalog &lt;chr&gt; &quot;adventureworks&quot;, &quot;adventureworks&quot;, &quot;adventu… ## $ table_name &lt;chr&gt; &quot;pg_proc&quot;, &quot;pg_proc&quot;, &quot;pg_proc&quot;, &quot;pg_proc&quot;, … ## $ column_name &lt;chr&gt; &quot;proname&quot;, &quot;pronamespace&quot;, &quot;proowner&quot;, &quot;prol… ## $ data_type &lt;chr&gt; &quot;name&quot;, &quot;oid&quot;, &quot;oid&quot;, &quot;oid&quot;, &quot;real (24,2)&quot;, … ## $ ordinal_position &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 1… ## $ character_maximum_length &lt;int&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, … ## $ column_default &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, … sp_print_df(head(columns_info_schema_info)) 13.3.1 What is the difference between a VIEW and a BASE TABLE? The BASE TABLE has the underlying data in the database table_info_schema_table %&gt;% filter( table_type == &quot;BASE TABLE&quot;) %&gt;% # filter(table_schema == &quot;public&quot; &amp; table_type == &quot;BASE TABLE&quot;) %&gt;% select(table_name, table_type) %&gt;% left_join(columns_info_schema_table, by = c(&quot;table_name&quot; = &quot;table_name&quot;)) %&gt;% select( table_type, table_name, column_name, data_type, ordinal_position, column_default ) %&gt;% collect(n = Inf) %&gt;% filter(str_detect(table_name, &quot;cust&quot;)) %&gt;% head() %&gt;% sp_print_df() Probably should explore how the VIEW is made up of data from BASE TABLEs. table_info_schema_table %&gt;% filter( table_type == &quot;VIEW&quot;) %&gt;% # filter(table_schema == &quot;public&quot; &amp; table_type == &quot;VIEW&quot;) %&gt;% select(table_name, table_type) %&gt;% left_join(columns_info_schema_table, by = c(&quot;table_name&quot; = &quot;table_name&quot;)) %&gt;% select( table_type, table_name, column_name, data_type, ordinal_position, column_default ) %&gt;% collect(n = Inf) %&gt;% filter(str_detect(table_name, &quot;cust&quot;)) %&gt;% head() %&gt;% sp_print_df() 13.3.2 What data types are found in the database? columns_info_schema_info %&gt;% count(data_type) %&gt;% head() %&gt;% sp_print_df() 13.4 Characterizing how things are named Names are the handle for accessing the data. Tables and columns may or may not be named consistently or in a way that makes sense to you. You should look at these names as data. 13.4.1 Counting columns and name reuse Pull out some rough-and-ready but useful statistics about your database. Since we are in SQL-land we talk about variables as columns. this is wrong! public_tables &lt;- columns_info_schema_table %&gt;% # filter(str_detect(table_name, &quot;pg_&quot;) == FALSE) %&gt;% # filter(table_schema == &quot;public&quot;) %&gt;% collect() public_tables %&gt;% count(table_name, sort = TRUE) %&gt;% head(n = 15) %&gt;% sp_print_df() How many column names are shared across tables (or duplicated)? public_tables %&gt;% count(column_name, sort = TRUE) %&gt;% filter(n &gt; 1) %&gt;% head() ## # A tibble: 6 x 2 ## column_name n ## &lt;chr&gt; &lt;int&gt; ## 1 modifieddate 140 ## 2 rowguid 61 ## 3 id 60 ## 4 name 59 ## 5 businessentityid 49 ## 6 productid 32 How many column names are unique? public_tables %&gt;% count(column_name) %&gt;% filter(n == 1) %&gt;% count() %&gt;% head() ## # A tibble: 1 x 1 ## n ## &lt;int&gt; ## 1 882 13.5 Database keys 13.5.1 Direct SQL How do we use this output? Could it be generated by dplyr? rs &lt;- dbGetQuery( con, &quot; --SELECT conrelid::regclass as table_from select table_catalog||&#39;.&#39;||table_schema||&#39;.&#39;||table_name table_name , conname, pg_catalog.pg_get_constraintdef(r.oid, true) as condef FROM information_schema.columns c,pg_catalog.pg_constraint r WHERE 1 = 1 --r.conrelid = &#39;16485&#39; AND r.contype in (&#39;f&#39;,&#39;p&#39;) ORDER BY 1 ;&quot; ) glimpse(rs) ## Observations: 467,838 ## Variables: 3 ## $ table_name &lt;chr&gt; &quot;adventureworks.hr.d&quot;, &quot;adventureworks.hr.d&quot;, &quot;adventurewo… ## $ conname &lt;chr&gt; &quot;FK_SalesOrderDetail_SpecialOfferProduct_SpecialOfferIDPro… ## $ condef &lt;chr&gt; &quot;FOREIGN KEY (specialofferid, productid) REFERENCES sales.… sp_print_df(head(rs)) The following is more compact and looks more useful. What is the difference between the two? rs &lt;- dbGetQuery( con, &quot;select conrelid::regclass as table_from ,c.conname ,pg_get_constraintdef(c.oid) from pg_constraint c join pg_namespace n on n.oid = c.connamespace where c.contype in (&#39;f&#39;,&#39;p&#39;) and n.nspname = &#39;public&#39; order by conrelid::regclass::text, contype DESC; &quot; ) glimpse(rs) ## Observations: 0 ## Variables: 3 ## $ table_from &lt;chr&gt; ## $ conname &lt;chr&gt; ## $ pg_get_constraintdef &lt;chr&gt; sp_print_df(head(rs)) dim(rs)[1] ## [1] 0 13.5.2 Database keys with dplyr This query shows the primary and foreign keys in the database. tables &lt;- tbl(con, dbplyr::in_schema(&quot;information_schema&quot;, &quot;tables&quot;)) table_constraints &lt;- tbl(con, dbplyr::in_schema(&quot;information_schema&quot;, &quot;table_constraints&quot;)) key_column_usage &lt;- tbl(con, dbplyr::in_schema(&quot;information_schema&quot;, &quot;key_column_usage&quot;)) referential_constraints &lt;- tbl(con, dbplyr::in_schema(&quot;information_schema&quot;, &quot;referential_constraints&quot;)) constraint_column_usage &lt;- tbl(con, dbplyr::in_schema(&quot;information_schema&quot;, &quot;constraint_column_usage&quot;)) keys &lt;- tables %&gt;% left_join(table_constraints, by = c( &quot;table_catalog&quot; = &quot;table_catalog&quot;, &quot;table_schema&quot; = &quot;table_schema&quot;, &quot;table_name&quot; = &quot;table_name&quot; )) %&gt;% # table_constraints %&gt;% filter(constraint_type %in% c(&quot;FOREIGN KEY&quot;, &quot;PRIMARY KEY&quot;)) %&gt;% left_join(key_column_usage, by = c( &quot;table_catalog&quot; = &quot;table_catalog&quot;, &quot;constraint_catalog&quot; = &quot;constraint_catalog&quot;, &quot;constraint_schema&quot; = &quot;constraint_schema&quot;, &quot;table_name&quot; = &quot;table_name&quot;, &quot;table_schema&quot; = &quot;table_schema&quot;, &quot;constraint_name&quot; = &quot;constraint_name&quot; ) ) %&gt;% # left_join(constraint_column_usage) %&gt;% # does this table add anything useful? select(table_name, table_type, constraint_name, constraint_type, column_name, ordinal_position) %&gt;% arrange(table_name) %&gt;% collect() glimpse(keys) ## Observations: 190 ## Variables: 6 ## $ table_name &lt;chr&gt; &quot;address&quot;, &quot;address&quot;, &quot;addresstype&quot;, &quot;billofmaterial… ## $ table_type &lt;chr&gt; &quot;BASE TABLE&quot;, &quot;BASE TABLE&quot;, &quot;BASE TABLE&quot;, &quot;BASE TABL… ## $ constraint_name &lt;chr&gt; &quot;FK_Address_StateProvince_StateProvinceID&quot;, &quot;PK_Addr… ## $ constraint_type &lt;chr&gt; &quot;FOREIGN KEY&quot;, &quot;PRIMARY KEY&quot;, &quot;PRIMARY KEY&quot;, &quot;FOREIG… ## $ column_name &lt;chr&gt; &quot;stateprovinceid&quot;, &quot;addressid&quot;, &quot;addresstypeid&quot;, &quot;co… ## $ ordinal_position &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 2, 1, 1, 1, 2… sp_print_df(head(keys)) What do we learn from the following query? How is it useful? rs &lt;- dbGetQuery( con, &quot;SELECT r.*, pg_catalog.pg_get_constraintdef(r.oid, true) as condef FROM pg_catalog.pg_constraint r WHERE 1=1 --r.conrelid = &#39;16485&#39; AND r.contype = &#39;f&#39; ORDER BY 1; &quot; ) head(rs) ## conname connamespace contype condeferrable condeferred ## 1 cardinal_number_domain_check 12771 c FALSE FALSE ## 2 yes_or_no_check 12771 c FALSE FALSE ## 3 CK_Employee_BirthDate 16386 c FALSE FALSE ## 4 CK_Employee_Gender 16386 c FALSE FALSE ## 5 CK_Employee_HireDate 16386 c FALSE FALSE ## 6 CK_Employee_MaritalStatus 16386 c FALSE FALSE ## convalidated conrelid contypid conindid conparentid confrelid confupdtype ## 1 TRUE 0 12785 0 0 0 ## 2 TRUE 0 12797 0 0 0 ## 3 TRUE 16450 0 0 0 0 ## 4 TRUE 16450 0 0 0 0 ## 5 TRUE 16450 0 0 0 0 ## 6 TRUE 16450 0 0 0 0 ## confdeltype confmatchtype conislocal coninhcount connoinherit conkey confkey ## 1 TRUE 0 FALSE &lt;NA&gt; &lt;NA&gt; ## 2 TRUE 0 FALSE &lt;NA&gt; &lt;NA&gt; ## 3 TRUE 0 FALSE {5} &lt;NA&gt; ## 4 TRUE 0 FALSE {7} &lt;NA&gt; ## 5 TRUE 0 FALSE {8} &lt;NA&gt; ## 6 TRUE 0 FALSE {6} &lt;NA&gt; ## conpfeqop conppeqop conffeqop conexclop ## 1 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 2 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 3 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 4 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 5 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 6 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## conbin ## 1 {OPEXPR :opno 525 :opfuncid 150 :opresulttype 16 :opretset false :opcollid 0 :inputcollid 0 :args ({COERCETODOMAINVALUE :typeId 23 :typeMod -1 :collation 0 :location 195} {CONST :consttype 23 :consttypmod -1 :constcollid 0 :constlen 4 :constbyval true :constisnull false :location 204 :constvalue 4 [ 0 0 0 0 0 0 0 0 ]}) :location 201} ## 2 {SCALARARRAYOPEXPR :opno 98 :opfuncid 67 :useOr true :inputcollid 100 :args ({RELABELTYPE :arg {COERCETODOMAINVALUE :typeId 1043 :typeMod 7 :collation 100 :location 121} :resulttype 25 :resulttypmod -1 :resultcollid 100 :relabelformat 2 :location -1} {ARRAYCOERCEEXPR :arg {ARRAY :array_typeid 1015 :array_collid 100 :element_typeid 1043 :elements ({CONST :consttype 1043 :consttypmod -1 :constcollid 100 :constlen -1 :constbyval false :constisnull false :location 131 :constvalue 7 [ 28 0 0 0 89 69 83 ]} {CONST :consttype 1043 :consttypmod -1 :constcollid 100 :constlen -1 :constbyval false :constisnull false :location 138 :constvalue 6 [ 24 0 0 0 78 79 ]}) :multidims false :location -1} :elemexpr {RELABELTYPE :arg {CASETESTEXPR :typeId 1043 :typeMod -1 :collation 0} :resulttype 25 :resulttypmod -1 :resultcollid 100 :relabelformat 2 :location -1} :resulttype 1009 :resulttypmod -1 :resultcollid 100 :coerceformat 2 :location -1}) :location 127} ## 3 {BOOLEXPR :boolop and :args ({OPEXPR :opno 1098 :opfuncid 1090 :opresulttype 16 :opretset false :opcollid 0 :inputcollid 0 :args ({VAR :varno 1 :varattno 5 :vartype 1082 :vartypmod -1 :varcollid 0 :varlevelsup 0 :varnoold 1 :varoattno 5 :location 804} {CONST :consttype 1082 :consttypmod -1 :constcollid 0 :constlen 4 :constbyval true :constisnull false :location 817 :constvalue 4 [ 33 -100 -1 -1 -1 -1 -1 -1 ]}) :location 814} {OPEXPR :opno 2359 :opfuncid 2352 :opresulttype 16 :opretset false :opcollid 0 :inputcollid 0 :args ({VAR :varno 1 :varattno 5 :vartype 1082 :vartypmod -1 :varcollid 0 :varlevelsup 0 :varnoold 1 :varoattno 5 :location 842} {OPEXPR :opno 1329 :opfuncid 1190 :opresulttype 1184 :opretset false :opcollid 0 :inputcollid 0 :args ({FUNCEXPR :funcid 1299 :funcresulttype 1184 :funcretset false :funcvariadic false :funcformat 0 :funccollid 0 :inputcollid 0 :args &lt;&gt; :location 856} {CONST :consttype 1186 :consttypmod -1 :constcollid 0 :constlen 16 :constbyval false :constisnull false :location 864 :constvalue 16 [ 0 0 0 0 0 0 0 0 0 0 0 0 -40 0 0 0 ]}) :location 862}) :location 852}) :location 837} ## 4 {SCALARARRAYOPEXPR :opno 98 :opfuncid 67 :useOr true :inputcollid 100 :args ({FUNCEXPR :funcid 871 :funcresulttype 25 :funcretset false :funcvariadic false :funcformat 0 :funccollid 100 :inputcollid 100 :args ({FUNCEXPR :funcid 401 :funcresulttype 25 :funcretset false :funcvariadic false :funcformat 1 :funccollid 100 :inputcollid 100 :args ({VAR :varno 1 :varattno 7 :vartype 1042 :vartypmod 5 :varcollid 100 :varlevelsup 0 :varnoold 1 :varoattno 7 :location 941}) :location 948}) :location 934} {ARRAY :array_typeid 1009 :array_collid 100 :element_typeid 25 :elements ({CONST :consttype 25 :consttypmod -1 :constcollid 100 :constlen -1 :constbyval false :constisnull false :location 969 :constvalue 5 [ 20 0 0 0 77 ]} {CONST :consttype 25 :consttypmod -1 :constcollid 100 :constlen -1 :constbyval false :constisnull false :location 980 :constvalue 5 [ 20 0 0 0 70 ]}) :multidims false :location 963}) :location 956} ## 5 {BOOLEXPR :boolop and :args ({OPEXPR :opno 1098 :opfuncid 1090 :opresulttype 16 :opretset false :opcollid 0 :inputcollid 0 :args ({VAR :varno 1 :varattno 8 :vartype 1082 :vartypmod -1 :varcollid 0 :varlevelsup 0 :varnoold 1 :varoattno 8 :location 1042} {CONST :consttype 1082 :consttypmod -1 :constcollid 0 :constlen 4 :constbyval true :constisnull false :location 1054 :constvalue 4 [ 1 -5 -1 -1 -1 -1 -1 -1 ]}) :location 1051} {OPEXPR :opno 2359 :opfuncid 2352 :opresulttype 16 :opretset false :opcollid 0 :inputcollid 0 :args ({VAR :varno 1 :varattno 8 :vartype 1082 :vartypmod -1 :varcollid 0 :varlevelsup 0 :varnoold 1 :varoattno 8 :location 1079} {OPEXPR :opno 1327 :opfuncid 1189 :opresulttype 1184 :opretset false :opcollid 0 :inputcollid 0 :args ({FUNCEXPR :funcid 1299 :funcresulttype 1184 :funcretset false :funcvariadic false :funcformat 0 :funccollid 0 :inputcollid 0 :args &lt;&gt; :location 1092} {CONST :consttype 1186 :consttypmod -1 :constcollid 0 :constlen 16 :constbyval false :constisnull false :location 1100 :constvalue 16 [ 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 ]}) :location 1098}) :location 1088}) :location 1074} ## 6 {SCALARARRAYOPEXPR :opno 98 :opfuncid 67 :useOr true :inputcollid 100 :args ({FUNCEXPR :funcid 871 :funcresulttype 25 :funcretset false :funcvariadic false :funcformat 0 :funccollid 100 :inputcollid 100 :args ({FUNCEXPR :funcid 401 :funcresulttype 25 :funcretset false :funcvariadic false :funcformat 1 :funccollid 100 :inputcollid 100 :args ({VAR :varno 1 :varattno 6 :vartype 1042 :vartypmod 5 :varcollid 100 :varlevelsup 0 :varnoold 1 :varoattno 6 :location 1181}) :location 1195}) :location 1174} {ARRAY :array_typeid 1009 :array_collid 100 :element_typeid 25 :elements ({CONST :consttype 25 :consttypmod -1 :constcollid 100 :constlen -1 :constbyval false :constisnull false :location 1216 :constvalue 5 [ 20 0 0 0 77 ]} {CONST :consttype 25 :consttypmod -1 :constcollid 100 :constlen -1 :constbyval false :constisnull false :location 1227 :constvalue 5 [ 20 0 0 0 83 ]}) :multidims false :location 1210}) :location 1203} ## consrc ## 1 (VALUE &gt;= 0) ## 2 ((VALUE)::text = ANY ((ARRAY[&#39;YES&#39;::character varying, &#39;NO&#39;::character varying])::text[])) ## 3 ((birthdate &gt;= &#39;1930-01-01&#39;::date) AND (birthdate &lt;= (now() - &#39;18 years&#39;::interval))) ## 4 (upper((gender)::text) = ANY (ARRAY[&#39;M&#39;::text, &#39;F&#39;::text])) ## 5 ((hiredate &gt;= &#39;1996-07-01&#39;::date) AND (hiredate &lt;= (now() + &#39;1 day&#39;::interval))) ## 6 (upper((maritalstatus)::text) = ANY (ARRAY[&#39;M&#39;::text, &#39;S&#39;::text])) ## condef ## 1 CHECK (VALUE &gt;= 0) ## 2 CHECK (VALUE::text = ANY (ARRAY[&#39;YES&#39;::character varying, &#39;NO&#39;::character varying]::text[])) ## 3 CHECK (birthdate &gt;= &#39;1930-01-01&#39;::date AND birthdate &lt;= (now() - &#39;18 years&#39;::interval)) ## 4 CHECK (upper(gender::text) = ANY (ARRAY[&#39;M&#39;::text, &#39;F&#39;::text])) ## 5 CHECK (hiredate &gt;= &#39;1996-07-01&#39;::date AND hiredate &lt;= (now() + &#39;1 day&#39;::interval)) ## 6 CHECK (upper(maritalstatus::text) = ANY (ARRAY[&#39;M&#39;::text, &#39;S&#39;::text])) 13.6 Creating your own data dictionary If you are going to work with a database for an extended period it can be useful to create your own data dictionary. This can take the form of keeping detaild notes as well as extracting metadata from the dbms. Here is an illustration of the idea. This probably doens’t work anymore # some_tables &lt;- c(&quot;rental&quot;, &quot;city&quot;, &quot;store&quot;) # # all_meta &lt;- map_df(some_tables, sp_get_dbms_data_dictionary, con = con) # # all_meta # # glimpse(all_meta) # # sp_print_df(head(all_meta)) 13.7 Save your work! The work you do to understand the structure and contents of a database can be useful for others (including future-you). So at the end of a session, you might look at all the data frames you want to save. Consider saving them in a form where you can add notes at the appropriate level (as in a Google Doc representing table or columns that you annotate over time). ls() ## [1] &quot;columns_info_schema_info&quot; &quot;columns_info_schema_table&quot; ## [3] &quot;con&quot; &quot;constraint_column_usage&quot; ## [5] &quot;cranex&quot; &quot;key_column_usage&quot; ## [7] &quot;keys&quot; &quot;public_tables&quot; ## [9] &quot;referential_constraints&quot; &quot;rs&quot; ## [11] &quot;schema_list&quot; &quot;table_constraints&quot; ## [13] &quot;table_info&quot; &quot;table_info_schema_table&quot; ## [15] &quot;tables&quot; ``` ## Cleaning up Always have R disconnect from the database when you’re done and stop the Adventureworks Container dbDisconnect(con) sp_docker_stop(&quot;adventureworks&quot;) "],
["appendix-background-basic-concepts.html", "A Background and Basic Concepts A.1 The big picture: R and the Docker / PostgreSQL playground on your machine A.2 Your computer and its operating system A.3 R A.4 Our sqlpetr package A.5 Docker A.6 ‘Normal’ and ‘normalized’ data A.7 SQL Language A.8 Enterprise DBMS", " A Background and Basic Concepts This Appendix describes: The overall structure of our Docker-based PostgreSQL sandbox Basic concepts around each of the elements that make up our sandbox: tidy data, pipes, Docker, PostgreSQL, data representation, and our petsqlr package. A.1 The big picture: R and the Docker / PostgreSQL playground on your machine Here is an overview of how R and Docker fit on your operating system in this book’s sandbox: R and Docker You run R from RStudio to set up Docker, launch PostgreSQL inside it and then send queries directly to PostgreSQL from R. (We provide more details about our sandbox environment in the chapter on mapping your environment. A.2 Your computer and its operating system The playground that we construct in this book is designed so that some of the mysteries of accessing a corporate database are more visible – it’s all happening on your computer. The challenge, however, is that we know very little about your computer and its operating system. In the workshops we’ve given about this book, the details of individual computers have turned out to be diverse and difficult to pin down in advance. So there can be many issues, but not many basic concepts that we can highlight in advance. A.3 R We assume a general familiarity with R and RStudio. RStudio’s Big Data workshop at the 2019 RStudio has an abundance of introductory material (Ruiz 2019). This book is Tidyverse-oriented, so we assume familiarity with the pipe operator, tidy data (Wickham 2014), dplyr, and techniques for tidying data (Wickham 2018). R connects to a database by means of a series of packages that work together. The following diagram from a big data workshop at the 2019 RStudio conference shows the big picture. The biggest difference in terms of retrieval strategies is between writing dplyr and native SQL code. Dplyr generates SQL-92 standard code; whereas you can write SQL code that leverages the specific language features of your DBMS when you write SQL code yourself. Rstudio’s DBMS architecture - slide # 33 A.4 Our sqlpetr package The sqlpetr package is the companion R package for this database tutorial. It has two classes of functions: Functions to install the dependencies needed to build the book and perform the operations covered in the tutorial, and Utilities for dealing with Docker and the PostgreSQL Docker image we use. sqlpetr has a pkgdown site at https://smithjd.github.io/sqlpetr/. A.5 Docker Docker and the DevOps tools surrounding it have fostered a revolution in the way services are delivered over the internet. In this book, we’re piggybacking on a small piece of that revolution, Docker on the desktop. A.5.1 Virtual machines and hypervisors A virtual machine is a machine that is running purely as software hosted by another real machine. To the user, a virtual machine looks just like a real one. But it has no processors, memory or I/O devices of its own - all of those are supplied and managed by the host. A virtual machine can run any operating system that will run on the host’s hardware. A Linux host can run a Windows virtual machine and vice versa. A hypervisor is the component of the host system software that manages virtual machines, usually called guests. Linux systems have a native hypervisor called Kernel Virtual Machine (kvm). And laptop, desktop and server processors from Intel and Advanced Micro Devices (AMD) have hardware that makes this hypervisor more efficient. Windows servers and Windows 10 Pro have a hypervisor called Hyper-V. Like kvm, Hyper-V can take advantage of the hardware in Intel and AMD processors. On Macintosh, there is a Hypervisor Framework (https://developer.apple.com/documentation/hypervisor) and other tools build on that. If this book is about Docker, why do we care about virtual machines and hypervisors? Docker is a Linux subsystem - it only runs on Linux laptops, desktops and servers. As we’ll see shortly, if we want to run Docker on Windows or MacOS, we’ll need a hypervisor, a Linux virtual machine and some “glue logic” to provide a Docker user experience equivalent to the one on a Linux system. A.5.2 Containers A container is a set of processes running in an operating system. The host operating system is usually Linux, but other operating systems also can host containers. Unlike a virtual machine, the container has no operating system kernel of its own. If the host is running the Linux kernel, so is the container. And since the container OS is the same as the host OS, there’s no need for a hypervisor or hardware to support the hypervisor. So a container is more efficient than a virtual machine. A container does have its own file system. From inside the container, this file system looks like a Linux file system, but it can use any Linux distro. For example, you can have an Ubuntu 18.04 LTS host running Ubuntu 14.04 LTS or Fedora 28 or CentOS 7 containers. The kernel will always be the host kernel, but the utilities and applications will be those from the container. A.5.3 Docker itself While there are both older (lxc) and newer container tools, the one that has caught on in terms of widespread use is Docker (Docker 2019a). Docker is widely used on cloud providers to deploy services of all kinds. Using Docker on the desktop to deliver standardized packages, as we are doing in this book, is a secondary use case, but a common one. If you’re using a Linux laptop / desktop, all you need to do is install Docker CE (Docker 2018a). However, most laptops and desktops don’t run Linux - they run Windows or MacOS. As noted above, to use Docker on Windows or MacOS, you need a hypervisor and a Linux virtual machine. A.5.4 Docker objects The Docker subsystem manages several kinds of objects - containers, images, volumes and networks. In this book, we are only using the basic command line tools to manage containers, images and volumes. Docker images are files that define a container’s initial file system. You can find pre-built images on Docker Hub and the Docker Store - the base PostgreSQL image we use comes from Docker Hub (https://hub.docker.com/_/postgres/). If there isn’t a Docker image that does exactly what you want, you can build your own by creating a Dockerfile and running docker build. We do this in [Build the pet-sql Docker Image]. Docker volumes – explain mount. A.5.5 Hosting Docker on Windows machines There are two ways to get Docker on Windows. For Windows 10 Home and older versions of Windows, you need Docker Toolbox (Docker 2019e). Note that for Docker Toolbox, you need a 64-bit AMD or Intel processor with the virtualization hardware installed and enabled in the BIOS. For Windows 10 Pro, you have the Hyper-V virtualizer as standard equipment, and can use Docker for Windows (Docker 2019c). A.5.6 Hosting Docker on macOS machines As with Windows, there are two ways to get Docker. For older Intel systems, you’ll need Docker Toolbox (Docker 2019d). Newer systems (2010 or later running at least macOS El Capitan 10.11) can run Docker for Mac (Docker 2019b). A.5.7 Hosting Docker on UNIX machines Unix was the original host for both R and Docker. Unix-like commands show up. A.6 ‘Normal’ and ‘normalized’ data A.6.1 Tidy data Tidy data (Wickham 2014) is well-behaved from the point of view of analysis and tools in the Tidyverse (RStudio 2019). Tidy data is easier to think about and it is usually worthwhile to make the data tidy (Wickham 2018). Tidy data is roughly equivalent to third normal form as discussed below. A.6.2 Design of “normal data” Data in a database is most often optimized to minimize storage space and increase performance while preserving integrity when adding, changing, or deleting data. The Wikipedia article on Database Normalization has a good introduction to the characteristics of “normal” data and the process of re-organizing it to meet those desirable criteria (Wikipedia 2019). The bottom line is that “data normalization is practical” although there are mathematical arguments for normalization based on the preservation of data integrity. A.7 SQL Language SQL stands for Structured Query Language. It is a database language where we can perform certain operations on the existing database and we can use it create a new database. There are four main categories where the SQL commands fall into: DML, DDL, DCL, and TCL. A.7.1 Data Manipulation Langauge (DML) These four SQL commands deal with the manipulation of data in the database. For everyday analytical work, these are the commands that you will use the most. 1. SELECT 2. INSERT 3. UPDATE 4. DELETE A.7.2 Data Definition Langauge (DDL) It consists of the SQL commands that can be used to define a database schema. The DDL commands include: 1. CREATE 2. ALTER 3. TRUNCATE 4. COMMENT 5. RENAME 6. DROP A.7.3 Data Control Language (DCL) The DCL commands deals with user rights, permissions and other controls in database management system. 1. GRANT 2. REVOKE A.7.4 Transaction Control Language (TCL) These commands deal with the control over transaction within the database. Transaction combines a set of tasks into single execution. 1. SET TRANSACTION 2. SAVEPOINT 3. ROLLBACK 4. COMMIT A.8 Enterprise DBMS The organizational context of a database matters just as much as its design characteristics. The design of a database (or data model) may have been purchased from an external vendor or developed in-house. In either case time has a tendency to erode the original design concept so that the data you find in a DBMS may not quite match the original design specification. And the original design may or may not be well reflected in the current naming of tables, columns and other objects. It’s a naive misconception to think that the data you are analyzing just “comes from the database”, although that’s literally true and may be the step that happens before you get your hands on it. In fact it comes from the people who design, enter, manage, protect, and use your organization’s data. In practice, a database administrator (DBA) is often a key point of contact in terms of access and may have stringent criteria for query performance. Make friends with your DBA. A.8.1 SQL databases Although there are ANSI standards for SQL syntax, different implementations vary in enough details that R’s ability to customize queries for those implementations is very helpful. The tables in a DBMS correspond to a data frame in R, so interaction with a DBMS is fairly natural for useRs. SQL code is characterized by the fact that it describes what to retrieve, leaving the DBMS back end to determine how to do it. Therefore it has a batch feel. The pipe operator (%&gt;%, which is read as and then) is inherently procedural when it’s used with dplyr: it can be used to construct queries step-by-step. Once a test dplyr query has been executed, it is easy to inspect the results and add steps with the pipe operator to refine or expand the query. A.8.2 Data mapping between R vs SQL data types The following code shows how different elements of the R bestiary are translated to and from ANSI standard data types. Note that R factors are translated as TEXT so that missing levels are ignored on the SQL side. library(DBI) dbDataType(ANSI(), 1:5) ## [1] &quot;INT&quot; dbDataType(ANSI(), 1) ## [1] &quot;DOUBLE&quot; dbDataType(ANSI(), TRUE) ## [1] &quot;SMALLINT&quot; dbDataType(ANSI(), Sys.Date()) ## [1] &quot;DATE&quot; dbDataType(ANSI(), Sys.time()) ## [1] &quot;TIMESTAMP&quot; dbDataType(ANSI(), Sys.time() - as.POSIXct(Sys.Date())) ## [1] &quot;TIME&quot; dbDataType(ANSI(), c(&quot;x&quot;, &quot;abc&quot;)) ## [1] &quot;TEXT&quot; dbDataType(ANSI(), list(raw(10), raw(20))) ## [1] &quot;BLOB&quot; dbDataType(ANSI(), I(3)) ## [1] &quot;DOUBLE&quot; dbDataType(ANSI(), iris) ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## &quot;DOUBLE&quot; &quot;DOUBLE&quot; &quot;DOUBLE&quot; &quot;DOUBLE&quot; &quot;TEXT&quot; The DBI specification provides extensive documentation that is worth digesting if you intend to work with a DBMS from R. As you work through the examples in this book, you will also want to refer to the following resources: RStudio’s Databases using R site describes many of the technical details involved. The RStudio community is an excellent place to ask questions or study what has been discussed previously. A.8.3 PostgreSQL and connection parameters An important detail: We use a PostgreSQL database server running in a Docker container for the database functions. It is installed inside Docker, so you do not have to download or install it yourself. To connect to it, you have to define some parameters. These parameters are used in two places: When the Docker container is created, they’re used to initialize the database, and Whenever we connect to the database, we need to specify them to authenticate. We define the parameters in an environment file that R reads when starting up. The file is called .Renviron, and is located in your home directory. See the discussion of securing and using dbms credentials. A.8.4 Connecting the R and DBMS environments Although everything happens on one machine in our Docker / PostgreSQL playground, in real life R and PostgreSQL (or other DBMS) will be in different environments on separate machines. How R connects them gives you control over where the work happens. You need to be aware of the differences beween the R and DBMS environments as well as how you can leverage the strengths of each one. Characteristics of local vs. server processing Dimension Local Remote Design purpose The R environment on your local machine is designed to be flexible and easy to use; ideal for data investigation. The DBMS environment is designed for large and complex databases where data integrity is more important than flexibility or ease of use. Processor power Your local machine has less memory, speed, and storage than the typical database server. Database servers are specialized, more expensive, and have more power. Memory constraint In R, query results must fit into memory. Servers have a lot of memory and write intermediate results to disk if needed without you knowing about it. Data crunching Data lives in the DBMS, so crunching it down locally requires you to pull it over the network. A DBMS has powerful data crunching capabilities once you know what you want and moves data over the server backbone to crunch it. Security Local control. Whether it is good or not depends on you. Responsibility of database administrators who set the rules. You play by their rules. Storage of intermediate results Very easy to save a data frame with intermediate results locally. May require extra privileges to save results in the database. Analytical resources Ecosystem of available R packages Extending SQL instruction set involves dbms-specific functions or R pseudo functions Collaboration One person working on a few data.frames. Many people collaborating on many tables. A.8.5 Using SQLite to simulate an enterprise DBMS SQLite engine is embedded in one file, so that many tables are stored together in one object. SQL commands can run against an SQLite database as demonstrated in how many uses of SQLite are in the RStudio dbplyr documentation. References "],
["chapter-appendix-setup-instructions.html", "B - Setup instructions B.1 Sandbox prerequisites B.2 R, RStudio and Git B.3 Install Docker", " B - Setup instructions This appendix explains: Hardware and software prerequisites for setting up the sandbox used in this book Documentation for all of the elements used in this sandbox B.1 Sandbox prerequisites The sandbox environment requires: A computer running Windows (Windows 7 64-bit or later - Windows 10-Pro is recommended), MacOS, or Linux (any Linux distro that will run Docker Community Edition, R and RStudio will work) Current versions of R and RStudio [Vargas (2018)) required. Docker (instructions below) Our companion package sqlpetr (Borasky et al. 2018) The database we use is PostgreSQL 11, but you do not need to install it - it’s installed via a Docker image. In addition to the current version of R and RStudio, you will need current versions of the following packages: DBI (R Special Interest Group on Databases (R-SIG-DB), Wickham, and Müller 2018) DiagrammeR (Iannone 2019) RPostgres (Wickham, Ooms, and Müller 2018) dbplyr (Wickham and Ruiz 2019) devtools (Wickham, Hester, and Chang 2019) downloader (Chang 2015) glue (Hester 2019) here (Müller 2017) knitr (Xie 2019b) skimr (Waring et al. 2019) tidyverse (Wickham 2019) bookdown (Xie 2019a) (for compiling the book, if you want to) B.2 R, RStudio and Git Most readers will probably have these already, but if not: If you do not have R: Go to https://cran.rstudio.com/ (R Core Team 2018). Select the download link for your system. For Linux, choose your distro. We recommend Ubuntu 18.04 LTS “Bionic Beaver”. It’s much easier to find support answers on the web for Ubuntu than other distros. Follow the instructions. Note: if you already have R, make sure it’s upgraded to R 3.5.1. We don’t test on older versions! If you do not have RStudio: go to https://www.rstudio.com/products/rstudio/download/#download. Make sure you have version 1.1.463 or later. If you do not have Git: On Windows, go to https://git-scm.com/download/win and follow instructions. There are a lot of options. Just pick the defaults!!! On MacOS, go to https://sourceforge.net/projects/git-osx-installer/files/ and follow instructions. On Linux, install Git from your distribution. B.3 Install Docker Installation depends on your operating system and we have found that it can be somewhat intricate. You will need Docker Community Edition (Docker CE): For Windows, consider these issues and follow these instructions: Go to https://store.docker.com/editions/community/docker-ce-desktop-windows. If you don’t have a Docker Store log in, you’ll need to create one. Then: If you have Windows 10 Pro, download and install Docker for Windows. If you have an older version of Windows, download and install Docker Toolbox (https://docs.docker.com/toolbox/overview/). Note that both versions require 64-bit hardware and the virtualization needs to be enabled in the firmware. On a Mac (Docker 2018c): Go to https://store.docker.com/editions/community/docker-ce-desktop-mac. If you don’t have a Docker Store login, you’ll need to create one. Then download and install Docker for Mac. Your MacOS must be at least release Yosemite (10.10.3). On UNIX flavors (Docker 2018a): note that, as with Windows and MacOS, you’ll need a Docker Store loin. Although most Linux distros ship with some version of Docker, chances are it’s not the same as the official Docker CE version. Ubuntu: https://store.docker.com/editions/community/docker-ce-server-ubuntu, Fedora: https://store.docker.com/editions/community/docker-ce-server-fedora, Cent OS: https://store.docker.com/editions/community/docker-ce-server-centos, Debian: https://store.docker.com/editions/community/docker-ce-server-debian. Note that on Linux, you will need to be a member of the docker group to use Docker. To do that, execute sudo usermod -aG docker ${USER}. Then, log out and back in again. References "],
["chapter-appendix-postgres-local-db-installation.html", "C Appendix E - Install adventureworks on your own machine C.1 Overview C.2 Resources", " C Appendix E - Install adventureworks on your own machine This appendix demonstrates how to: Setup the adventureworks database locally on your machine Connect to the adventureworks database These instructions should be tested by a Windows user The PostgreSQL tutorial links do not work, despite being pasted from the site C.1 Overview This appendix details the process to download and restore the adventureworks database so that you can work with the database locally on your own machine. This tutorial assumes that (1) you have PostgreSQL installed on your computer, and (2) that you have configured your system to run psql at the command line. Installation of PostgreSQL and configuration of psql are outside the scope of this book. C.1.1 Download the adventureworks database Download the adventureworks database from here. C.1.2 Restore the dvdrental database at the command line Launch the psql tool Enter account information to log into the PostgreSQL database server, if prompted Enter the following command to create a new database CREATE DATABASE adventureworks; Open a new terminal window (not in psql) and navigate to the folder where the adventureworks.sql file is located. Use the cd command in the terminal, followed by the file path to change directories to the location of adventureworks.sql. For example: cd /Users/username/Documents/adventureworks. Enter the following command prompt: pg_restore -d adventureworks -f -U postgres adventureworks.sql C.1.3 Restore the adventureworks database using pgAdmin Another option to restore the adventureworks database locally on your machine is with the pgAdmin graphical user interface. However, we highly recommend using the command line methods detailed above. Installation and configuration of pgAdmin is outside the scope of this book. C.2 Resources Instructions by PostgreSQL Tutorial to load the dvdrental database. (PostgreSQL Tutorial Website 2019). Windows installation of PostgreSQL by PostgreSQL Tutorial. (PostgreSQL Tutorial Website 2019). Installation of PostgreSQL on a Mac using Postgres.app. (Postgres.app 2019). Command line configuration of PosgreSQL on a Mac with Postgres.app. (Postgres.app 2019). Installing PostgreSQL for Linux, Arch Linux, Windows, Mac and other operating systems, by Postgres Guide. (Postgres Guide Website 2019). "],
["chapter-windows-tech-details.html", "D Appendix B - Additional technical details for Windows users D.1 Hardware requirements D.2 Software requirements D.3 Docker for Windows settings D.4 Git, GitHub and line endings", " D Appendix B - Additional technical details for Windows users This chapter explains: How to setup your environment for Windows How to use Git and GitHub effectively on Windows Skip these instructions if your computer has either OSX or a Unix variant. D.1 Hardware requirements You will need an Intel or AMD processor with 64-bit hardware and the hardware virtualization feature. Most machines you buy today will have that, but older ones may not. You will need to go into the BIOS / firmware and enable the virtualization feature. You will need at least 4 gigabytes of RAM! D.2 Software requirements You will need Windows 7 64-bit or later. If you can afford it, I highly recommend upgrading to Windows 10 Pro. D.2.1 Windows 7, 8, 8.1 and Windows 10 Home (64 bit) Install Docker Toolbox. The instructions are here: https://docs.docker.com/toolbox/toolbox_install_windows/. Make sure you try the test cases and they work! D.2.2 Windows 10 Pro Install Docker for Windows stable. The instructions are here: https://docs.docker.com/docker-for-windows/install/#start-docker-for-windows. Again, make sure you try the test cases and they work. D.3 Docker for Windows settings D.3.1 Shared drives If you’re going to mount host files into container file systems (as we do in the following chapters), you need to set up shared drives. Open the Docker settings dialog and select Shared Drives. Check the drives you want to share. In this screenshot, the D: drive is my 1 terabyte hard drive. D.3.2 Kubernetes Kubernetes is a container orchestration / cloud management package that’s a major DevOps tool. It’s heavily supported by Red Hat and Google, and as a result is becoming a required skill for DevOps. However, it’s overkill for this project at the moment. So you should make sure it’s not enabled. Go to the Kubernetes dialog and make sure the Enable Kubernetes checkbox is cleared. D.4 Git, GitHub and line endings Git was originally developed for Linux - in fact, it was created by Linus Torvalds to manage hundreds of different versions of the Linux kernel on different machines all around the world. As usage has grown, Git has achieved a huge following and is the version control system used by most large open source projects, including this one. If you’re on Windows, there are some things about Git and GitHub you need to watch. First of all, there are quite a few tools for running Git on Windows, but the RStudio default and recommended one is Git for Windows (https://git-scm.com/download/win). By default, text files on Linux end with a single linefeed (\\n) character. But on Windows, text files end with a carriage return and a line feed (\\r\\n). See https://en.wikipedia.org/wiki/Newline for the gory details. Git defaults to checking files out in the native mode. So if you’re on Linux, a text file will show up with the Linux convention, and if you’re on Windows, it will show up with the Windows convention. Most of the time this doesn’t cause any problems. But Docker containers usually run Linux, and if you have files from a repository on Windows that you’ve sent to the container, the container may malfunction or give weird results. This kind of situation has caused a lot of grief for contributors to this project, so beware. In particular, executable sh or bash scripts will fail in a Docker container if they have Windows line endings. You may see an error message with \\r in it, which means the shell saw the carriage return (\\r) and gave up. But often you’ll see no hint at all what the problem was. So you need a way to tell Git that some files need to be checked out with Linux line endings. See https://help.github.com/articles/dealing-with-line-endings/ for the details. Summary: You’ll need a .gitattributes file in the root of the repository. In that file, all text files (scripts, program source, data, etc.) that are destined for a Docker container will need to have the designator &lt;spec&gt; text eol=lf, where &lt;spec&gt; is the file name specifier, for example, *.sh. This repo includes a sample: .gitattributes "],
["chapter-appendix-postresql-authentication.html", "E Appendix C - PostgreSQL Authentication E.1 Introduction E.2 Password authentication on the PostgreSQL Docker image E.3 Adding roles", " E Appendix C - PostgreSQL Authentication E.1 Introduction PostgreSQL has a very robust and flexible set of authentication methods (PostgreSQL Global Development Group 2018a). In most production environments, these will be managed by the database administrator (DBA) on a need-to-access basis. People and programs will be granted access only to a minimum set of capabilities required to function, and nothing more. In this book, we are using a PostgreSQL Docker image (Docker 2018d). When we create a container from that image, we use its native mechanism to create the postgres database superuser with a password specified in an R environment file ~/.Renviron. See Securing and using your dbms log-in credentials for how we do this. What that means is that you are the DBA - the database superuser - for the PostgreSQL database cluster running in the container! You can create and destroy databases, schemas, tables, views, etc. You can also create and destroy users - called roles in PostgreSQL, and GRANT or REVOKE their privileges with great precision. You don’t have to do that to use this book. But if you want to experiment with it, feel free! E.2 Password authentication on the PostgreSQL Docker image Of the many PostgreSQL authentication mechanisms, the simplest that’s universallly available is password authentication (PostgreSQL Global Development Group 2018c). That’s what we use for the postgres database superuser, and what we recommend for any roles you may create. Once a role has been created, you need five items to open a connection to the PostgreSQL database cluster: The host. This is a name or IP address that your network can access. In this book, with the database running in a Docker container, that’s usually localhost. The port. This is the port the server is listening on. It’s usually the default, 5439, and that’s what we use. But in a secure environment, it will often be some random number to lower the chances that an attacker can find the database server. And if you have more than one server on the network, you’ll need to use different ports for each of them. The dbname to connect to. This database must exist or the connection attempt will fail. The user. This user must exist in the database cluster and be allowed to access the database. We are using the database superuser postgres in this book. The password. This is set by the DBA for the user. In this book we use the password defined in Securing and using your dbms log-in credentials. E.3 Adding roles As noted above, PostgreSQL has a very flexible fine-grained access permissions system. We can’t cover all of it; see PostgreSQL Global Development Group (2018b) for the full details. But we can give an example. E.3.1 Setting up Docker First, we need to make sure we don’t have any other databases listening on the default port 5439. sqlpetr::sp_check_that_docker_is_up() ## [1] &quot;Docker is up but running no containers&quot; sqlpetr::sp_docker_remove_container(&quot;cattle&quot;) ## [1] 0 # in case you&#39;ve been doing things out of order, stop a container named &#39;adventureworks&#39; if it exists: # sqlpetr::sp_docker_stop(&quot;adventureworks&quot;) E.3.2 Creating a new container We’ll create a “cattle” container with a default PostgreSQL 10 database cluster. sqlpetr::sp_make_simple_pg(&quot;cattle&quot;) con &lt;- sqlpetr::sp_get_postgres_connection( host = &quot;localhost&quot;, port = 5439, dbname = &quot;postgres&quot;, user = &quot;postgres&quot;, password = &quot;postgres&quot;, seconds_to_test = 30 ) E.3.3 Adding a role Now, let’s add a role. We’ll add a role that can log in and create databases, but isn’t a superuser. Since this is a demo and not a real production database cluster, we’ll specify a password in plaintext. And we’ll create a database for our new user. Create the role: DBI::dbExecute( con, &quot;CREATE ROLE charlie LOGIN CREATEDB PASSWORD &#39;chaplin&#39;;&quot; ) ## [1] 0 Create the database: DBI::dbExecute( con, &quot;CREATE DATABASE charlie OWNER = charlie&quot;) ## [1] 0 E.3.4 Did it work? DBI::dbDisconnect(con) con &lt;- sqlpetr::sp_get_postgres_connection( host = &quot;localhost&quot;, port = 5439, dbname = &quot;postgres&quot;, user = &quot;charlie&quot;, password = &quot;chaplin&quot;, seconds_to_test = 30 ) OK, we can connect. Let’s do some stuff! data(&quot;iris&quot;) dbCreateTable creates the table with columns matching the data frame. But it does not send data to the table. DBI::dbCreateTable(con, &quot;iris&quot;, iris) To send data, we use dbAppendTable. DBI::dbAppendTable(con, &quot;iris&quot;, iris) ## Warning: Factors converted to character ## [1] 150 DBI::dbListTables(con) ## [1] &quot;iris&quot; head(DBI::dbReadTable(con, &quot;iris&quot;)) ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## 1 5.1 3.5 1.4 0.2 setosa ## 2 4.9 3.0 1.4 0.2 setosa ## 3 4.7 3.2 1.3 0.2 setosa ## 4 4.6 3.1 1.5 0.2 setosa ## 5 5.0 3.6 1.4 0.2 setosa ## 6 5.4 3.9 1.7 0.4 setosa E.3.5 Disconnect and remove the container DBI::dbDisconnect(con) sqlpetr::sp_docker_remove_container(&quot;cattle&quot;) ## [1] 0 References "],
["chapter-appendix-dbi-index.html", "F DBI package functions - INDEX", " F DBI package functions - INDEX Where are these covered and should the by included? DBI 1st time Call Example/Notes DBIConnct 6.3.2 (04) in sp_get_postgres_connection dbAppendTable dbCreateTable dbDisconnect 6.4n (04) dbDisconnect(con) dbExecute 10.4.2 (13) Executes a statement and returns the number of rows affected. dbExecute() comes with a default implementation (which should work with most backends) that calls dbSendStatement(), then dbGetRowsAffected(), ensuring that the result is always free-d by dbClearResult(). dbExistsTable dbExistsTable(con,‘actor’) dbFetch 17.1 (72) dbFetch(rs) dbGetException dbGetInfo dbGetInfo(con) dbGetQuery 10.4.1 (13) dbGetQuery(con,‘select * from store;’) dbIsReadOnly dbIsReadOnly(con) dbIsValid dbIsValid(con) dbListFields 6.3.3 (04) DBI::dbListFields(con, “mtcars”) dbListObjects dbListObjects(con) dbListTables 6.3.2 (04) DBI::dbListTables(con, con) dbReadTable 8.1.2 DBI::dbReadTable(con, “rental”) dbRemoveTable dbSendQuery 17.1 (72) rs &lt;- dbSendQuery(con, “SELECT * FROM mtcars WHERE cyl = 4”) dbSendStatement The dbSendStatement() method only submits and synchronously executes the SQL data manipulation statement (e.g., UPDATE, DELETE, INSERT INTO, DROP TABLE, …) to the database engine. dbWriteTable 6.3.3 (04) dbWriteTable(con, “mtcars”, mtcars, overwrite = TRUE) "],
["chapter-appendix-dplyr-to-postres-translation.html", "G Appendix _ Dplyr to SQL translations G.1 Overview", " G Appendix _ Dplyr to SQL translations You may be interested in exactly how the DBI package translates R functions into their SQL quivalents – and in which functions are translated and which are not. This Appendix answers those questions. It is based on the work of Dewey Dunnington (@paleolimbot) which he published here: https://apps.fishandwhistle.net/archives/1503 https://rud.is/b/2019/04/10/lost-in-sql-translation-charting-dbplyr-mapped-sql-function-support-across-all-backends/ G.1 Overview These packages are called below: library(tidyverse) library(dbplyr) library(gt) library(here) library(sqlpetr) list the DBI functions that are available: names(sql_translate_env(simulate_dbi())) ## [1] &quot;-&quot; &quot;:&quot; &quot;!&quot; &quot;!=&quot; ## [5] &quot;(&quot; &quot;[&quot; &quot;[[&quot; &quot;{&quot; ## [9] &quot;*&quot; &quot;/&quot; &quot;&amp;&quot; &quot;&amp;&amp;&quot; ## [13] &quot;%%&quot; &quot;%&gt;%&quot; &quot;%in%&quot; &quot;^&quot; ## [17] &quot;+&quot; &quot;&lt;&quot; &quot;&lt;=&quot; &quot;==&quot; ## [21] &quot;&gt;&quot; &quot;&gt;=&quot; &quot;|&quot; &quot;||&quot; ## [25] &quot;$&quot; &quot;abs&quot; &quot;acos&quot; &quot;as_date&quot; ## [29] &quot;as_datetime&quot; &quot;as.character&quot; &quot;as.Date&quot; &quot;as.double&quot; ## [33] &quot;as.integer&quot; &quot;as.integer64&quot; &quot;as.logical&quot; &quot;as.numeric&quot; ## [37] &quot;as.POSIXct&quot; &quot;asin&quot; &quot;atan&quot; &quot;atan2&quot; ## [41] &quot;between&quot; &quot;bitwAnd&quot; &quot;bitwNot&quot; &quot;bitwOr&quot; ## [45] &quot;bitwShiftL&quot; &quot;bitwShiftR&quot; &quot;bitwXor&quot; &quot;c&quot; ## [49] &quot;case_when&quot; &quot;ceil&quot; &quot;ceiling&quot; &quot;coalesce&quot; ## [53] &quot;cos&quot; &quot;cosh&quot; &quot;cot&quot; &quot;coth&quot; ## [57] &quot;day&quot; &quot;desc&quot; &quot;exp&quot; &quot;floor&quot; ## [61] &quot;hour&quot; &quot;if&quot; &quot;if_else&quot; &quot;ifelse&quot; ## [65] &quot;is.na&quot; &quot;is.null&quot; &quot;log&quot; &quot;log10&quot; ## [69] &quot;mday&quot; &quot;minute&quot; &quot;month&quot; &quot;na_if&quot; ## [73] &quot;nchar&quot; &quot;now&quot; &quot;paste&quot; &quot;paste0&quot; ## [77] &quot;pmax&quot; &quot;pmin&quot; &quot;qday&quot; &quot;round&quot; ## [81] &quot;second&quot; &quot;sign&quot; &quot;sin&quot; &quot;sinh&quot; ## [85] &quot;sql&quot; &quot;sqrt&quot; &quot;str_c&quot; &quot;str_conv&quot; ## [89] &quot;str_count&quot; &quot;str_detect&quot; &quot;str_dup&quot; &quot;str_extract&quot; ## [93] &quot;str_extract_all&quot; &quot;str_flatten&quot; &quot;str_glue&quot; &quot;str_glue_data&quot; ## [97] &quot;str_interp&quot; &quot;str_length&quot; &quot;str_locate&quot; &quot;str_locate_all&quot; ## [101] &quot;str_match&quot; &quot;str_match_all&quot; &quot;str_order&quot; &quot;str_pad&quot; ## [105] &quot;str_remove&quot; &quot;str_remove_all&quot; &quot;str_replace&quot; &quot;str_replace_all&quot; ## [109] &quot;str_replace_na&quot; &quot;str_sort&quot; &quot;str_split&quot; &quot;str_split_fixed&quot; ## [113] &quot;str_squish&quot; &quot;str_sub&quot; &quot;str_subset&quot; &quot;str_to_lower&quot; ## [117] &quot;str_to_title&quot; &quot;str_to_upper&quot; &quot;str_trim&quot; &quot;str_trunc&quot; ## [121] &quot;str_view&quot; &quot;str_view_all&quot; &quot;str_which&quot; &quot;str_wrap&quot; ## [125] &quot;substr&quot; &quot;switch&quot; &quot;tan&quot; &quot;tanh&quot; ## [129] &quot;today&quot; &quot;tolower&quot; &quot;toupper&quot; &quot;trimws&quot; ## [133] &quot;wday&quot; &quot;xor&quot; &quot;yday&quot; &quot;year&quot; ## [137] &quot;cume_dist&quot; &quot;cummax&quot; &quot;cummean&quot; &quot;cummin&quot; ## [141] &quot;cumsum&quot; &quot;dense_rank&quot; &quot;first&quot; &quot;lag&quot; ## [145] &quot;last&quot; &quot;lead&quot; &quot;max&quot; &quot;mean&quot; ## [149] &quot;median&quot; &quot;min&quot; &quot;min_rank&quot; &quot;n&quot; ## [153] &quot;n_distinct&quot; &quot;nth&quot; &quot;ntile&quot; &quot;order_by&quot; ## [157] &quot;percent_rank&quot; &quot;quantile&quot; &quot;rank&quot; &quot;row_number&quot; ## [161] &quot;sum&quot; &quot;var&quot; &quot;cume_dist&quot; &quot;cummax&quot; ## [165] &quot;cummean&quot; &quot;cummin&quot; &quot;cumsum&quot; &quot;dense_rank&quot; ## [169] &quot;first&quot; &quot;lag&quot; &quot;last&quot; &quot;lead&quot; ## [173] &quot;max&quot; &quot;mean&quot; &quot;median&quot; &quot;min&quot; ## [177] &quot;min_rank&quot; &quot;n&quot; &quot;n_distinct&quot; &quot;nth&quot; ## [181] &quot;ntile&quot; &quot;order_by&quot; &quot;percent_rank&quot; &quot;quantile&quot; ## [185] &quot;rank&quot; &quot;row_number&quot; &quot;sum&quot; &quot;var&quot; sql_translate_env(simulate_dbi()) ## &lt;sql_variant&gt; ## scalar: -, :, !, !=, (, [, [[, {, *, /, &amp;, &amp;&amp;, %%, %&gt;%, %in%, ^, +, ## scalar: &lt;, &lt;=, ==, &gt;, &gt;=, |, ||, $, abs, acos, as_date, as_datetime, ## scalar: as.character, as.Date, as.double, as.integer, as.integer64, ## scalar: as.logical, as.numeric, as.POSIXct, asin, atan, atan2, ## scalar: between, bitwAnd, bitwNot, bitwOr, bitwShiftL, bitwShiftR, ## scalar: bitwXor, c, case_when, ceil, ceiling, coalesce, cos, cosh, ## scalar: cot, coth, day, desc, exp, floor, hour, if, if_else, ifelse, ## scalar: is.na, is.null, log, log10, mday, minute, month, na_if, ## scalar: nchar, now, paste, paste0, pmax, pmin, qday, round, second, ## scalar: sign, sin, sinh, sql, sqrt, str_c, str_conv, str_count, ## scalar: str_detect, str_dup, str_extract, str_extract_all, ## scalar: str_flatten, str_glue, str_glue_data, str_interp, ## scalar: str_length, str_locate, str_locate_all, str_match, ## scalar: str_match_all, str_order, str_pad, str_remove, ## scalar: str_remove_all, str_replace, str_replace_all, ## scalar: str_replace_na, str_sort, str_split, str_split_fixed, ## scalar: str_squish, str_sub, str_subset, str_to_lower, str_to_title, ## scalar: str_to_upper, str_trim, str_trunc, str_view, str_view_all, ## scalar: str_which, str_wrap, substr, switch, tan, tanh, today, ## scalar: tolower, toupper, trimws, wday, xor, yday, year ## aggregate: cume_dist, cummax, cummean, cummin, cumsum, dense_rank, ## aggregate: first, lag, last, lead, max, mean, median, min, min_rank, n, ## aggregate: n_distinct, nth, ntile, order_by, percent_rank, quantile, ## aggregate: rank, row_number, sum, var ## window: cume_dist, cummax, cummean, cummin, cumsum, dense_rank, ## window: first, lag, last, lead, max, mean, median, min, min_rank, n, ## window: n_distinct, nth, ntile, order_by, percent_rank, quantile, ## window: rank, row_number, sum, var source(here(&quot;book-src&quot;, &quot;dbplyr-sql-function-translation.R&quot;)) ## Warning: The `.drop` argument of `unnest()` is deprecated as of tidyr 1.0.0. ## All list-columns are now preserved. ## This warning is displayed once per session. ## Call `lifecycle::last_warnings()` to see where this warning was generated. Each of the following dbplyr back ends may have a slightly different translation: translations %&gt;% filter(!is.na(sql)) %&gt;% count(variant) ## # A tibble: 11 x 2 ## variant n ## &lt;chr&gt; &lt;int&gt; ## 1 access 193 ## 2 dbi 183 ## 3 hive 187 ## 4 impala 190 ## 5 mssql 196 ## 6 mysql 194 ## 7 odbc 186 ## 8 oracle 184 ## 9 postgres 204 ## 10 sqlite 183 ## 11 teradata 196 Only one postgres translation produces an output: psql &lt;- translations %&gt;% filter(!is.na(sql), variant == &quot;postgres&quot;) %&gt;% select(r, n_args, sql) %&gt;% arrange(r) # sp_print_df(head(psql, n = 40)) sp_print_df(psql) "],
["chapter-appendix-additional-resources.html", "H Appendix Additional resources H.1 Editing this book H.2 Docker alternatives H.3 Docker and R H.4 Documentation for Docker and PostgreSQL H.5 SQL and dplyr H.6 More Resources", " H Appendix Additional resources H.1 Editing this book Here are instructions for editing this book H.2 Docker alternatives Choosing between Docker and Vagrant (Zait 2017) H.3 Docker and R Noam Ross’ talk on Docker for the UseR (Ross 2018b) and his Slides (Ross 2018a) give a lot of context and tips. Good Docker tutorials An introductory Docker tutorial (Srivastav 2018) A Docker curriculum (Hall 2018) Scott Came’s materials about Docker and R on his website (Came 2018) and at the 2018 UseR Conference focus on R inside Docker. It’s worth studying the ROpensci Docker tutorial (ROpenSciLabs 2018) H.4 Documentation for Docker and PostgreSQL The Postgres image documentation (Docker 2018d) PostgreSQL &amp; Docker documentation (Docker 2018d) Dockerize PostgreSQL (Docker 2018b) Usage examples of PostgreSQL with Docker WARNING-EXPIRED CERTIFICATE 2018-12-20 H.5 SQL and dplyr Why SQL is not for analysis but dplyr is (Nishida 2016) Data Manipulation with dplyr (With 50 Examples) (ListenData.com 2016) H.6 More Resources David Severski describes some key elements of connecting to databases with R for MacOS users (Severski 2018) This tutorial picks up ideas and tips from Ed Borasky’s Data Science pet containers (Borasky 2018), which creates a framework based on that Hack Oregon example and explains why this repo is named pet-sql. References "],
["references.html", "References", " References "]
]
