[
["index.html", "Exploring Enterprise Databases with R: A Tidyverse Approach Chapter 1 Introduction 1.1 Using R to query a DBMS in your organization 1.2 Docker as a tool for UseRs 1.3 Alternatives to Docker 1.4 Packages used in this book 1.5 Who are we? 1.6 How did this project come about? 1.7 Navigation", " Exploring Enterprise Databases with R: A Tidyverse Approach John David Smith, Sophie Yang, M. Edward (Ed) Borasky, Jim Tyhurst, Scott Came, Mary Anne Thygesen, and Ian Frantz 2019-10-04 Chapter 1 Introduction This chapter introduces: The motivation for this book and the strategies we have adopted Our approach to exploring issues “beyind the enterprise firewall” using Docker to demonstrate access to a service like PostgreSQL from R Our team and how this project came about 1.1 Using R to query a DBMS in your organization Many R users (or useRs) live a dual life: in the vibrant open-source R community where R is created, improved, discussed, and taught. And then they go to work in a secured, complex, closed organizational environment where they may be on their own. Here is a request on the Rstudio community site for help that has been lightly edited to emphasize the generality that we see: I’m trying to migrate some inherited scripts that […] to connect to a […] database to […] instead. I’ve reviewed the https://db.rstudio.com docs and tried a number of configurations but haven’t been able to connect. I’m in uncharted territory within my org, so haven’t been able to get much help internally. This book will help you create a hybrid environment on your machine that can mimic some of the uncharted territory in your organization. It goes far beyond the basic connection issues and covers issues that you face when you are finding your way around or writing queries to your organization’s databases, not just when maintaining inherited scripts. Technology hurdles. The interfaces (passwords, packages, etc.) and gaps between R and a back end database are hidden from public view as a matter of security, so pinpointing exactly where a problem is can be difficult. A simulated environment such as we offer here can be an important learning resource. Scale issues. We see at least two types of scale issues. Handling large volumes of data so that performance issues must be a consideration requires a basic understanding of what’s happening in “the back end” (which is necessarily hidden from view). Therefore mastering techniques for drawing samples or small batches of data are essential. In addition to their size, your organization’s databases will often have structural characteristics that are complex and obscure. Data documentation is often incomplete and emphasizes operational characteristics, rather than analytic opportunities. A careful useR often needs to confirm the documentation on the fly and de-normalize data carefully. Use cases. R users frequently need to make sense of an organization’s complex data structures and coding schemes to address incompletely formed questions so that informal exploratory data analysis has to be intuitive and fast. The technology details should not get in the way. Sharing and discussing exploratory and diagnostic retrieval techniquesis best in public, but is constrained by organizational requirements. We have found that PostgreSQL in a Docker container solves many of the foregoing problems. 1.2 Docker as a tool for UseRs Noam Ross’s “Docker for the UseR” (Ross 2018a) suggests that there are four distinct Docker use-cases for useRs. Make a fixed working environment for reproducible analysis Access a service outside of R (e.g., PostgreSQL) Create an R based service (e.g., with plumber) Send our compute jobs to the cloud with minimal reconfiguration or revision This book explores #2 because it allows us to work on the database access issues described above and to practice on an industrial-scale DBMS. Docker is a comparatively easy way to simulate the relationship between an R/RStudio session and a database – all on on your machine (provided you have Docker installed and running). Running PostgreSQL on a Docker container avoids OS or system dependencies or conflicts that cause confusion and limit reproducibility. A Docker environment consumes relatively few resources. Our sandbox does much less but only includes PostgreSQL and sample data, so it takes up about 5% of the space taken up by the Vagrant environment that inspired this project. (Makubuya 2018) A simple Docker container such as the one used in our sandbox is easy to use and could be extended for other uses. Docker is a widely used technology for deploying applications in the cloud, so for many useRs it’s worth mastering. 1.3 Alternatives to Docker We have found Docker to be a great tool for simulating the complexities of an enterprise environment. However, installing Docker can be challenging, especially for Windows users. Therefore the code in this book depends on PostgreSQL(Group 2019) in a Docker container, but it can all be readily adapted to either SQLite(Consortium 2019), PostgreSQL running natively on your computer, or even PostgreSQL running in the cloud. The technical details of these alternatives are all in separate chapters. 1.4 Packages used in this book The following packages are used in this book: bookdown DBI dbplyr devtools DiagrammeR downloader glue here knitr RPostgres skimr sqlpetr (installs with: remotes::install_github(\"smithjd/sqlpetr\", force = TRUE, quiet = TRUE, build = TRUE, build_opts = \"\")) tidyverse 1.5 Who are we? We have been collaborating on this book since the Summer of 2018, each of us chipping into the project as time permits: Ian Franz - @ianfrantz Jim Tyhurst - @jimtyhurst John David Smith - @smithjd M. Edward (Ed) Borasky - @znmeb Maryanne Thygesen @maryannet Scott Came - @scottcame Sophie Yang - @SophieMYang 1.6 How did this project come about? We trace this book back to the June 2, 2018 Cascadia R Conf where Aaron Makubuya gave a presentation using Vagrant hosting (Makubuya 2018). After that John Smith, Ian Franz, and Sophie Yang had discussions after the monthly Data Discussion Meetups about the difficulties around setting up Vagrant (a virtual environment), connecting to an enterprise database, and having realistic public environment to demo or practice the issues that come up behind corporate firewalls. Scott Came’s tutorial on R and Docker (Came 2018) (an alternative to Vagrant) at the 2018 UseR Conference in Melbourne was provocative and it turned out he lived nearby. We re-connected with M. Edward (Ed) Borasky who had done extensive development for a Hack Oregon data science containerization project (Borasky 2018). 1.7 Navigation If this is the first bookdown (Xie 2016) book you’ve read, here’s how to navigate the website. The controls on the upper left: there are four controls on the upper left. A “hamburger” menu: this toggles the table of contents on the left side of the page on or off. A magnifying glass: this toggles a search box on or off. A letter “A”: this lets you pick how you want the site to display. You have your choice of small or large text, a serif or sans-serif font, and a white, sepia or night theme. A pencil: this is the “Edit” button. This will take you to a GitHub edit dialog for the chapter you’re reading. If you’re a committer to the repository, you’ll be able to edit the source directly. If not, GitHub will fork a copy of the repository to your own account and you’ll be able to edit that version. Then you can make a pull request. The share buttons in the upper right hand corner. There’s one for Twitter, one for Facebook, and one that gives a menu of options, including LinkedIn. References "],
["chapter-how-to-use-this-book.html", "Chapter 2 How to use this book 2.1 Retrieve the code from GitHub 2.2 Read along, experiment as you go 2.3 Participating", " Chapter 2 How to use this book This chapter explains: Getting the code used in this book How you can contribute to the book project This book is full of examples that you can replicate on your computer. 2.1 Retrieve the code from GitHub The code to generate the book and the exercises it contains can be downloaded from this repo. 2.2 Read along, experiment as you go We have never been sure whether we’re writing an expository book or a massive tutorial. You may use it either way. The best way to learn the material we cover is to experiment. After the introductory chapters and the chapter that creates the persistent database, you can jump around and each chapter stands on its own. 2.3 Participating 2.3.1 Browsing the book If you just want to read the book and copy / paste code into your working environment, simply browse to https://smithjd.github.io/sql-pet. If you get stuck, or find things aren’t working, open an issue at https://github.com/smithjd/sql-pet/issues/new/. 2.3.2 Diving in If you want to experiment with the code in the book, run it in RStudio and interact with it, you’ll need to do two more things: Install the sqlpetr R package (Borasky et al. 2018). See https://smithjd.github.io/sqlpetr for the package documentation. Installation may take some time if it has to install or update packages not available on your computer. Clone the Git repository https://github.com/smithjd/sql-pet.git and open the project file sql-pet.Rproj in RStudio. Enjoy! References "],
["chapter-learning-goals.html", "Chapter 3 Learning Goals and Use Cases 3.1 Challenge: goals, context and expectations 3.2 The Challenge: Investigating a question using an organization’s database 3.3 Making your way through the book 3.4 Adventure Works", " Chapter 3 Learning Goals and Use Cases This chapter sets the context for the book by: Describing our assumptions about the reader of this book: the challenges you face, your R skills, your learning goals, and context. Describing what the book offers in terms of: Problems that are addressed Learning objectives Sequence of topics, ranging from connecting to the database to exploring an issue in response to questions from an executive R packages used Describing the sample database used in the book 3.1 Challenge: goals, context and expectations Working behind the corporate firewall – the need to simulate it. Important because that’s where the data (and possibly your paycheck) are coming from. Differences between production and data warehouse environments. We are simulating a production environment. There are many similarities. Data models are different. Performance is a bigger deal in the OLTP. Data in a organizational environment around the database. Learning to keep your DBAs happy: You are your own DBA in this simulation, so you can wreak havoc and learn from it, but you can learn to be DBA-friendly here. In the end it’s the subject-matter experts (people using the data every day) that really understand your data, but you have to work with your DBAs first. You can’t believe all the data you pull out of the database. 3.2 The Challenge: Investigating a question using an organization’s database Using an enterprise database to create meaningful management insights requires a combination of very different skills: Need both familiarity with the data and a focus question An iterative process where the data resource can shape your understanding of the question the question you need to answer will frame how you see the data resource You need to go back and forth between the two, asking do I understand the question? do I understand the data? A “good enough” understanding of the data resource (in the DBMS) Nobody knows everything about an entire organization’s data resources. We do, however, need to know what more we need to know and estimate what we don’t know yet. Use all available documentation and understand its limits Use your own tools and skills to examine the data resource What is missing from the database: (columns, records, cells) Why is the data missing? A “good enough” understanding of the question you seek to answer How general or specific is your question? How aligned is it with the purpose for which the database was designed and is being operated? How different are your assumptions and concerns from those of the people who enter and use the data on a day to day basis? Some cycles in this iteration between question refinement and reformulation on the one hand and data retrieval and investigation on the other feel like a waste time. That’s inevitable. Bringing R tools and skills to bear on these R is a powerful tool for data access, manipulation, modeling and presentation Different R packages and techniques are available for each of the elements involved in exploring, analyzing and reporting on enterprise behavior using the enterprise database. 3.2.1 Problems that we address in the book This book emphasizes database exploration and the R techniques that are needed. We are emphasizing a tidyverse approach. &amp; graphics to really makes sense of what we find. We can’t call on real people in the adventureworks company, obviously, but we invent some characters to illustrate the investigation process as we have experienced it in various organizational settings. 3.2.2 Book structure The book explores R techniques and and investigation strategies using progressively more complex queries, that lead to this scenario: There is a new Executive VP of Sales at Adventure Works. She wants an overview of sales and the sales organization’s performance at Adventure Works. Once her questions are satisfied, a monthly report is developed that can run automatically and appear in her mailbox. Early chapters demonstrate now to connect to a database and find your way around it, with a pause to discuss how to secure your credentials. Both Rstudio and R script methods are shown for the same database overview. The salesordedrheader table in the sales schema is used to demonstrate packages and functions that show what a single table contains. Then the same table is used but the investigation adopts a business perspective, demonstrating R techniques that are motivated by questions like “How sales for the Adventure Works company?” Starting with base tables, then use views (that contain knowledge about the application) More involved queries join three tables in three different schemas: salesperson, employee, and person. The relevant question might be “Who is my top salesperson? Are the 3 top salespersons older or younger?” Finally, we build a series of queries that explore the sales workflow: sales territories, sales people, top customers by product, product mixture that gives top 80% of sales. What are they producing in detail? Seasonal? Type of product, region, etc.? The book ends by demonstrating how R code can be used for standard reports from the database that are emailed to a list of recipients. 3.3 Making your way through the book After working through the code in this book, you can expect to be able to: R, SQL and PostgreSQL Run queries against PostgreSQL in an environment that simulates what is found in a corporate setting. Understand techniques and some of the trade-offs between: queries aimed at exploration or informal investigation using dplyr (Wickham 2018); and queries that should be written in SQL, because performance is important due to the size of the database or the frequency with which a query is to be run. Understand the equivalence between dplyr and SQL queries, and how R translates one into the other. Gain familiarity with techniques that help you explore a database and verify its documentation. Gain familiarity with the standard metadata that a SQL database contains to describe its own contents. Understand some advanced SQL techniques. Gain some understanding of techniques for assessing query structure and performance. Docker related Set up a PostgreSQL database in a Docker environment. Gain familiarity with the various ways of interacting with the Docker and PostgreSQL environments Understand enough about Docker to swap databases, e.g. Sports DB for the DVD rental database used in this tutorial. Or swap the database management system (DBMS), e.g. MySQL for PostgreSQL. 3.3.1 R Packages These R packages are discussed or used in exercises: DBI dbplyr devtools downloader glue gt here knitr RPostgres skimr sqlpetr (installs with: remotes::install_github(\"smithjd/sqlpetr\", force = TRUE, quiet = TRUE, build = TRUE, build_opts = \"\")) tidyverse In addition, these are used to render the book: * bookdown * DiagrammeR 3.4 Adventure Works In this book we have adopted the Microsoft Adventure Works online transaction processing database for our examples. It is https://docs.microsoft.com/en-us/previous-versions/sql/sql-server-2008/ms124438(v=sql.100) See Sections 3 and 4 Journal of Information Systems Education, Vol. 26(3) Summer 2015. “Teaching Tip Active Learning via a Sample Database: The Case of Microsoft’s Adventure Works” by Michel Mitri http://jise.org/Volume26/n3/JISEv26n3p177.pdf See the AdventureWorks Data Dictionary and a sample table (employee). Here is a (link to an ERD diagram)[https://i.stack.imgur.com/LMu4W.gif] References "],
["chapter-setup-adventureworks-db.html", "Chapter 4 Create and connect to the adventureworks database in PostgreSQL 4.1 Overview 4.2 Verify that Docker is up, running, and clean up if necessary 4.3 Clean up if appropriate 4.4 Build the adventureworks Docker image 4.5 Run the adventureworks Docker Image 4.6 Connect to PostgreSQL 4.7 Adventureworks Schemas 4.8 Investigate the database using Rstudio 4.9 Cleaning up 4.10 Using the adventureworks container in the rest of the book", " Chapter 4 Create and connect to the adventureworks database in PostgreSQL This chapter demonstrates how to: Create and connect to the PostgreSQL adventureworks database in Docker Keep necessary credentials secret while being available to R when it executes. Leverage Rstudio features to get an overview of the database Set up the environment for subsequent chapters 4.1 Overview Docker commands can be run from a terminal (e.g., the Rstudio Terminal pane) or with a system2() command. The necessary functions to start, stop Docker containers and do other busy work are provided in the sqlpetr package. Note: The functions in the package are designed to help you focus on interacting with a dbms from R. You can ignore how they work until you are ready to delve into the details. They are all named to begin with sp_. The first time a function is called in the book, we provide a note explaining its use. Please install the sqlpetr package if not already installed: library(devtools) if (!require(sqlpetr)) { remotes::install_github( &quot;smithjd/sqlpetr&quot;, force = TRUE, build = FALSE, quiet = TRUE) } Note that when you install this package the first time, it will ask you to update the packages it uses and that may take some time. These packages are called in this Chapter: library(tidyverse) library(DBI) library(RPostgres) library(glue) require(knitr) library(dbplyr) library(sqlpetr) library(bookdown) library(here) 4.2 Verify that Docker is up, running, and clean up if necessary The sp_check_that_docker_is_up function from the sqlpetr package checks whether Docker is up and running. If it’s not, then you need to install, launch or re-install Docker. sp_check_that_docker_is_up() ## [1] &quot;Docker is up but running no containers&quot; 4.3 Clean up if appropriate Force-remove the adventureworks container if it was left over (e.g., from a prior runs): sp_docker_remove_container(&quot;adventureworks&quot;) ## [1] 0 4.4 Build the adventureworks Docker image Now we set up a “realistic” database named adventureworks in Docker. NOTE: This chapter doesn’t go into the details of creating or restoring the adventureworks database. For more detail on what’s going on behind the scenes, you can examine the step-by-step code in: source('book-src/restore-adventureworks-postgres-on-docker.R') To save space here in the book, we’ve created a function in sqlpetr to build this image, called OUT OF DATE!! . Vignette Building the adventureworks Docker Image describes the build process. *Ignore the errors in the following step: source(here(&quot;book-src&quot;, &quot;restore-adventureworks-postgres-on-docker.R&quot;)) ## docker run --detach --name adventureworks --publish 5432:5432 --mount type=bind,source=&quot;/home/znmeb/Projects/sql-pet&quot;,target=/petdir postgres:11 4.5 Run the adventureworks Docker Image Now we can run the image in a container and connect to the database. To run the image we use an sqlpetr function called OUT OF DATE sp_pg_docker_run For the rest of the book we will assume that you have a Docker container called adventureworks that can be stopped and started. In that sense each chapter in the book is independent. sp_docker_start(&quot;adventureworks&quot;) 4.6 Connect to PostgreSQL *CHECK for sqlpetr update!Thesp_make_simple_pgfunction we called above created a container from thepostgres:11library image downloaded from Docker Hub. As part of the process, it set the password for the PostgreSQL database superuserpostgres` to the value “postgres”. For simplicity, we are using a weak password at this point and it’s shown here and in the code in plain text. That is bad practice because user credentials should not be shared in open code like that. A subsequent chapter demonstrates how to store and use credentials to access the DBMS so that they are kept private. The sp_get_postgres_connection function from the sqlpetr package gets a DBI connection string to a PostgreSQL database, waiting if it is not ready. This function connects to an instance of PostgreSQL and we assign it to a symbol, con, for subsequent use. The connctions_tab = TRUE parameter opens a connections tab that’s useful for navigating a database. Note that we are using port 5439 for PostgreSQL inside the container and published to localhost. Why? If you have PostgreSQL already running on the host or another container, it probably claimed port 5432, since that’s the default. So we need to use a different port for our PostgreSQL container. Use the DBI package to connect to the adventureworks database in PostgreSQL. Remember the settings discussion about [keeping passwords hidden][Pause for some security considerations] con &lt;- sp_get_postgres_connection( host = &quot;localhost&quot;, port = 5432, # this version still using 5432!!! user = &quot;postgres&quot;, password = &quot;postgres&quot;, dbname = &quot;adventureworks&quot;, seconds_to_test = 20, connection_tab = TRUE ) 4.7 Adventureworks Schemas Think of the Adventureworks database as a model of the Adventureworks business. The business is organized around different departments (humanresources, sales, and purchasing), business processes (production), and resources (person). Each schema is a container for the all the database objects needed to model the departments, business processes, and resources. As a data analyst, the connections tab has three of the five database objects of interest. These are schemas, tables and views. The other two database objects of interest not shown in the connetions tab are the table primary and foreign keys, PK and FK. Those database objects enforce the referential integrity of the data and the performance of the application. Let the DBA’s worry about them. The Connections tab has three icons. The node icon represents a schema. The schema helps organize the structure and design of the database. The schema contains the views, the grid with the glasses, and tables, the grids without the glasses, that are of interest to the data analyst. A table is a database object usually represents something useful to a business process. For example, a sales person may enter a new order. The first screen is typically called the sales order header screen which contains information about the customer placing the order. This information is captured in salesorderheader table. The customers ordered items are typically entered via multiple screens. These are captured in the salesorderdetail table. A view is a database object that maybe a subset of either the columns or rows of a single table. For example, the customer table has information on all the customers, but the customer view, c, shows only a single customer. Or a view may have data from a primary/driving table and joined to other tables to provide a better understanding/view of the information in the primary table. For example, the primary table typically has a primary key column, PK, and zero or more foreign key columns, FK. The PK and FK are usually an integer which is great for a computer, but not so nice us mere mortals. An extended view pulls information associated with the FK. For example a sales order view a customer foreign key, can show the actual customer name. 4.8 Investigate the database using Rstudio The Rstudio Connections tab shows that you are connected to Postgres and that the adventureworks database has a many schemas each of which has multiple tables and views in it. The drop-down icon to the left of a table lists the table’s columns. Connections tab - adventureworks Clicking on the icon to the left of a schema expands the list of tables and views in that schema. Clicking on the View or Table icon opens up Rstudio’s View pane to get a peek at the data: View of employee table The number of rows and columns shown in the View pane depends on the size of the window. Disconnect from the database: dbDisconnect(con) 4.9 Cleaning up Always have R disconnect from the database when you’re done. dbDisconnect(con) ## Warning in connection_release(conn@ptr): Already disconnected Stop the adventureworks container: sp_docker_stop(&quot;adventureworks&quot;) Show that the container still exists even though it’s not running sp_show_all_docker_containers() ## CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES ## ee0f6ee5a1cc postgres:11 &quot;docker-entrypoint.s…&quot; 30 seconds ago Exited (0) Less than a second ago adventureworks ## 8435d4b7ee78 znmeb/postgis &quot;docker-entrypoint.s…&quot; 9 days ago Exited (0) 8 days ago containers_postgis_1 ## da60e11b3727 6b2784b116df &quot;/bin/sh -c &#39;mkdir /…&quot; 9 days ago Exited (2) 9 days ago condescending_carson ## 4bbb083cba07 443540ec5848 &quot;/bin/sh -c &#39;R -e &#39;i…&quot; 9 days ago Exited (1) 9 days ago trusting_euler ## 186b3ced7ad2 4ca242b1c323 &quot;/bin/sh -c &#39;R -e &#39;i…&quot; 9 days ago Exited (1) 9 days ago angry_varahamihira ## 010df0104f51 znmeb/pgadmin4 &quot;/entrypoint.sh&quot; 9 days ago Exited (0) 8 days ago containers_pgadmin4_1 Next time, you can just use this command to start the container: sp_docker_start(\"adventureworks\") And once stopped, the container can be removed with: sp_check_that_docker_is_up(\"adventureworks\") 4.10 Using the adventureworks container in the rest of the book After this point in the book, we assume that Docker is up and that we can always start up our adventureworks database with: sp_docker_start(\"adventureworks\") "],
["chapter-dbms-login-credentials.html", "Chapter 5 Securing and using your dbms log-in credentials 5.1 Set up the adventureworks Docker container 5.2 Storing your dbms credentials 5.3 Clean up", " Chapter 5 Securing and using your dbms log-in credentials This chapter demonstrates how to: Keep necessary credentials secret or at least invisible Interact with PostgreSQL using your stored dbms credentials Connecting to a dbms can be very frustrating at first. In many organizations, simply getting access credentials takes time and may involve jumping through multiple hoops. In addition, a dbms is terse or deliberately inscrutable when your credetials are incorrect. That’s a security strategy, not a limitation of your understanding or of your software. When R can’t log you on to a dbms, you usually will have no information as to what went wrong. There are many different strategies for managing credentials. See Securing Credentials in RStudio’s Databases using R documentation for some alternatives to the method we adopt in this book. We provide more details about PostgreSQL Authentication in our sandbox environment in an appendix. The following packages are used in this chapter: library(tidyverse) library(DBI) library(RPostgres) require(knitr) library(sqlpetr) 5.1 Set up the adventureworks Docker container 5.1.1 Verify that Docker is running Check that Docker is up and running: sp_check_that_docker_is_up() ## [1] &quot;Docker is up but running no containers&quot; 5.1.2 Start the Docker container: Start the adventureworks Docker container: sp_docker_start(&quot;adventureworks&quot;) 5.2 Storing your dbms credentials In previous chapters the connection string for connecting to the dbms has used default credentials specified in plain text as follows: user= 'postgres', password = 'postgres' When we call sp_get_postgres_connection below we’ll use environment variables that R obtains from reading the .Renviron file when R starts up. This approach has two benefits: that file is not uploaded to GitHub and R looks for it in your default directory every time it loads. To see whether you have already created that file, use the R Studio Files tab to look at your home directory: That file should contain lines that look like the example below. Although in this example it contains the PostgreSQL default values for the username and password, they are obviously not secret. But this approach demonstrates where you should put secrets that R needs while not risking accidental uploaded to GitHub or some other public location.. Open your .Renviron file with this command: file.edit(\"~/.Renviron\") Or you can execute define_postgresql_params.R to create the file or you could copy / paste the following into your .Renviron file: DEFAULT_POSTGRES_PASSWORD=postgres DEFAULT_POSTGRES_USER_NAME=postgres Once that file is created, restart R, and after that R reads it every time it comes up. 5.2.1 Connect with Postgres using the Sys.getenv function Connect to the postgrSQL using the sp_get_postgres_connection function: con &lt;- sp_get_postgres_connection(user = Sys.getenv(&quot;DEFAULT_POSTGRES_USER_NAME&quot;), password = Sys.getenv(&quot;DEFAULT_POSTGRES_PASSWORD&quot;), port = 5432, dbname = &quot;adventureworks&quot;, seconds_to_test = 30, connection_tab = TRUE) Once the connection object has been created, you can list all of the tables in one of the schemas: dbExecute(con, &quot;set search_path to humanresources, public;&quot;) # watch for duplicates! ## [1] 0 dbListTables(con) ## [1] &quot;employee&quot; &quot;shift&quot; ## [3] &quot;employeepayhistory&quot; &quot;jobcandidate&quot; ## [5] &quot;department&quot; &quot;vemployee&quot; ## [7] &quot;vemployeedepartment&quot; &quot;vemployeedepartmenthistory&quot; ## [9] &quot;vjobcandidate&quot; &quot;vjobcandidateeducation&quot; ## [11] &quot;vjobcandidateemployment&quot; &quot;employeedepartmenthistory&quot; 5.3 Clean up Afterwards, always disconnect from the dbms: dbDisconnect(con) Tell Docker to stop the adventureworks container: sp_docker_stop(&quot;adventureworks&quot;) "],
["chapter-connect-to-db-with-r-code.html", "Chapter 6 Connecting to the database with R code 6.1 Verify that Docker is up and running, and start the database 6.2 Connect to PostgreSQL 6.3 Set schema search path and list its contents 6.4 Anatomy of a dplyr connection object 6.5 Cleaning up", " Chapter 6 Connecting to the database with R code This chapter demonstrates how to: Connect to and disconnect R from the adventureworks database Use dplyr to get an overview of the database, replicating the facilities provided by RStudio These packages are called in this Chapter: library(tidyverse) library(DBI) library(RPostgres) library(glue) require(knitr) library(dbplyr) library(sqlpetr) library(bookdown) library(here) 6.1 Verify that Docker is up and running, and start the database The sp_check_that_docker_is_up function from the sqlpetr package checks whether Docker is up and running. If it’s not, then you need to install, launch or re-install Docker. sp_check_that_docker_is_up() ## [1] &quot;Docker is up but running no containers&quot; sp_docker_start(&quot;adventureworks&quot;) 6.2 Connect to PostgreSQL *CHECK for sqlpetr update!Thesp_make_simple_pgfunction we called above created a container from thepostgres:11library image downloaded from Docker Hub. As part of the process, it set the password for the PostgreSQL database superuserpostgres` to the value “postgres”. For simplicity, we are using a weak password at this point and it’s shown here and in the code in plain text. That is bad practice because user credentials should not be shared in open code like that. A subsequent chapter demonstrates how to store and use credentials to access the DBMS so that they are kept private. The sp_get_postgres_connection function from the sqlpetr package gets a DBI connection string to a PostgreSQL database, waiting if it is not ready. This function connects to an instance of PostgreSQL and we assign it to a symbol, con, for subsequent use. The connctions_tab = TRUE parameter opens a connections tab that’s useful for navigating a database. Note that we are using port 5439 for PostgreSQL inside the container and published to localhost. Why? If you have PostgreSQL already running on the host or another container, it probably claimed port 5432, since that’s the default. So we need to use a different port for our PostgreSQL container. Use the DBI package to connect to the adventureworks database in PostgreSQL. Remember the settings discussion about [keeping passwords hidden][Pause for some security considerations] con &lt;- sp_get_postgres_connection( host = &quot;localhost&quot;, port = 5432, # this version still using 5432!!! user = &quot;postgres&quot;, password = &quot;postgres&quot;, dbname = &quot;adventureworks&quot;, seconds_to_test = 20, connection_tab = TRUE ) 6.3 Set schema search path and list its contents Schemas will be discussed later on because multiple schemas are the norm in an enterprise database environment, but they are a side issue at this point. So we switch the order in which PostgreSQL searches for objects with the following SQL code: dbExecute(con, &quot;set search_path to sales;&quot;) ## [1] 0 With the custom search_path, the following command shows the tables in the sales schema. In the adventureworks database, there are no tables in the public schema. dbListTables(con) ## [1] &quot;countryregioncurrency&quot; ## [2] &quot;customer&quot; ## [3] &quot;currencyrate&quot; ## [4] &quot;creditcard&quot; ## [5] &quot;personcreditcard&quot; ## [6] &quot;specialoffer&quot; ## [7] &quot;specialofferproduct&quot; ## [8] &quot;salesorderheadersalesreason&quot; ## [9] &quot;shoppingcartitem&quot; ## [10] &quot;salespersonquotahistory&quot; ## [11] &quot;salesperson&quot; ## [12] &quot;currency&quot; ## [13] &quot;store&quot; ## [14] &quot;salesorderheader&quot; ## [15] &quot;salesorderdetail&quot; ## [16] &quot;salesreason&quot; ## [17] &quot;salesterritoryhistory&quot; ## [18] &quot;vindividualcustomer&quot; ## [19] &quot;vpersondemographics&quot; ## [20] &quot;vsalesperson&quot; ## [21] &quot;vsalespersonsalesbyfiscalyears&quot; ## [22] &quot;vsalespersonsalesbyfiscalyearsdata&quot; ## [23] &quot;vstorewithaddresses&quot; ## [24] &quot;vstorewithcontacts&quot; ## [25] &quot;vstorewithdemographics&quot; ## [26] &quot;salestaxrate&quot; ## [27] &quot;salesterritory&quot; Notice there are several tables that start with the letter v: they are actually views which will turn out to be important. They are clearly distinguished in the connections tab, but the naming is a matter of convention. Same for dbListFields: dbListFields(con, &quot;salesorderheader&quot;) ## [1] &quot;salesorderid&quot; &quot;revisionnumber&quot; ## [3] &quot;orderdate&quot; &quot;duedate&quot; ## [5] &quot;shipdate&quot; &quot;status&quot; ## [7] &quot;onlineorderflag&quot; &quot;purchaseordernumber&quot; ## [9] &quot;accountnumber&quot; &quot;customerid&quot; ## [11] &quot;salespersonid&quot; &quot;territoryid&quot; ## [13] &quot;billtoaddressid&quot; &quot;shiptoaddressid&quot; ## [15] &quot;shipmethodid&quot; &quot;creditcardid&quot; ## [17] &quot;creditcardapprovalcode&quot; &quot;currencyrateid&quot; ## [19] &quot;subtotal&quot; &quot;taxamt&quot; ## [21] &quot;freight&quot; &quot;totaldue&quot; ## [23] &quot;comment&quot; &quot;rowguid&quot; ## [25] &quot;modifieddate&quot; Thus with this search order, the following two produce identical results: tbl(con, in_schema(&quot;sales&quot;, &quot;salesorderheader&quot;)) %&gt;% head() ## # Source: lazy query [?? x 25] ## # Database: postgres [postgres@localhost:5432/adventureworks] ## salesorderid revisionnumber orderdate duedate ## &lt;int&gt; &lt;int&gt; &lt;dttm&gt; &lt;dttm&gt; ## 1 43659 8 2011-05-31 00:00:00 2011-06-12 00:00:00 ## 2 43660 8 2011-05-31 00:00:00 2011-06-12 00:00:00 ## 3 43661 8 2011-05-31 00:00:00 2011-06-12 00:00:00 ## 4 43662 8 2011-05-31 00:00:00 2011-06-12 00:00:00 ## 5 43663 8 2011-05-31 00:00:00 2011-06-12 00:00:00 ## 6 43664 8 2011-05-31 00:00:00 2011-06-12 00:00:00 ## # … with 21 more variables: shipdate &lt;dttm&gt;, status &lt;int&gt;, ## # onlineorderflag &lt;lgl&gt;, purchaseordernumber &lt;chr&gt;, accountnumber &lt;chr&gt;, ## # customerid &lt;int&gt;, salespersonid &lt;int&gt;, territoryid &lt;int&gt;, ## # billtoaddressid &lt;int&gt;, shiptoaddressid &lt;int&gt;, shipmethodid &lt;int&gt;, ## # creditcardid &lt;int&gt;, creditcardapprovalcode &lt;chr&gt;, ## # currencyrateid &lt;int&gt;, subtotal &lt;dbl&gt;, taxamt &lt;dbl&gt;, freight &lt;dbl&gt;, ## # totaldue &lt;dbl&gt;, comment &lt;chr&gt;, rowguid &lt;chr&gt;, modifieddate &lt;dttm&gt; tbl(con, &quot;salesorderheader&quot;) %&gt;% head() ## # Source: lazy query [?? x 25] ## # Database: postgres [postgres@localhost:5432/adventureworks] ## salesorderid revisionnumber orderdate duedate ## &lt;int&gt; &lt;int&gt; &lt;dttm&gt; &lt;dttm&gt; ## 1 43659 8 2011-05-31 00:00:00 2011-06-12 00:00:00 ## 2 43660 8 2011-05-31 00:00:00 2011-06-12 00:00:00 ## 3 43661 8 2011-05-31 00:00:00 2011-06-12 00:00:00 ## 4 43662 8 2011-05-31 00:00:00 2011-06-12 00:00:00 ## 5 43663 8 2011-05-31 00:00:00 2011-06-12 00:00:00 ## 6 43664 8 2011-05-31 00:00:00 2011-06-12 00:00:00 ## # … with 21 more variables: shipdate &lt;dttm&gt;, status &lt;int&gt;, ## # onlineorderflag &lt;lgl&gt;, purchaseordernumber &lt;chr&gt;, accountnumber &lt;chr&gt;, ## # customerid &lt;int&gt;, salespersonid &lt;int&gt;, territoryid &lt;int&gt;, ## # billtoaddressid &lt;int&gt;, shiptoaddressid &lt;int&gt;, shipmethodid &lt;int&gt;, ## # creditcardid &lt;int&gt;, creditcardapprovalcode &lt;chr&gt;, ## # currencyrateid &lt;int&gt;, subtotal &lt;dbl&gt;, taxamt &lt;dbl&gt;, freight &lt;dbl&gt;, ## # totaldue &lt;dbl&gt;, comment &lt;chr&gt;, rowguid &lt;chr&gt;, modifieddate &lt;dttm&gt; 6.4 Anatomy of a dplyr connection object As introduced in the previous chapter, the dplyr::tbl function creates an object that might look like a data frame in that when you enter it on the command line, it prints a bunch of rows from the dbms table. But it is actually a list object that dplyr uses for constructing queries and retrieving data from the DBMS. The following code illustrates these issues. The dplyr::tbl function creates the connection object that we store in an object named salesorderheader_table: salesorderheader_table &lt;- dplyr::tbl(con, in_schema(&quot;sales&quot;, &quot;salesorderheader&quot;)) %&gt;% select(-rowguid) %&gt;% rename(salesorderheader_details_updated = modifieddate) At first glance, it acts like a data frame when you print it, although it only prints 10 of the table’s 31,465 rows: salesorderheader_table ## # Source: lazy query [?? x 24] ## # Database: postgres [postgres@localhost:5432/adventureworks] ## salesorderid revisionnumber orderdate duedate ## &lt;int&gt; &lt;int&gt; &lt;dttm&gt; &lt;dttm&gt; ## 1 43659 8 2011-05-31 00:00:00 2011-06-12 00:00:00 ## 2 43660 8 2011-05-31 00:00:00 2011-06-12 00:00:00 ## 3 43661 8 2011-05-31 00:00:00 2011-06-12 00:00:00 ## 4 43662 8 2011-05-31 00:00:00 2011-06-12 00:00:00 ## 5 43663 8 2011-05-31 00:00:00 2011-06-12 00:00:00 ## 6 43664 8 2011-05-31 00:00:00 2011-06-12 00:00:00 ## 7 43665 8 2011-05-31 00:00:00 2011-06-12 00:00:00 ## 8 43666 8 2011-05-31 00:00:00 2011-06-12 00:00:00 ## 9 43667 8 2011-05-31 00:00:00 2011-06-12 00:00:00 ## 10 43668 8 2011-05-31 00:00:00 2011-06-12 00:00:00 ## # … with more rows, and 20 more variables: shipdate &lt;dttm&gt;, status &lt;int&gt;, ## # onlineorderflag &lt;lgl&gt;, purchaseordernumber &lt;chr&gt;, accountnumber &lt;chr&gt;, ## # customerid &lt;int&gt;, salespersonid &lt;int&gt;, territoryid &lt;int&gt;, ## # billtoaddressid &lt;int&gt;, shiptoaddressid &lt;int&gt;, shipmethodid &lt;int&gt;, ## # creditcardid &lt;int&gt;, creditcardapprovalcode &lt;chr&gt;, ## # currencyrateid &lt;int&gt;, subtotal &lt;dbl&gt;, taxamt &lt;dbl&gt;, freight &lt;dbl&gt;, ## # totaldue &lt;dbl&gt;, comment &lt;chr&gt;, salesorderheader_details_updated &lt;dttm&gt; However, notice that the first output line shows ??, rather than providing the number of rows in the table. Similarly, the next to last line shows: … with more rows, and 20 more variables: whereas the output for a normal tbl of this salesorderheader data would say: … with 31,455 more rows, and 20 more variables: So even though salesorderheader_table is a tbl, it’s also a tbl_PqConnection: class(salesorderheader_table) ## [1] &quot;tbl_PqConnection&quot; &quot;tbl_dbi&quot; &quot;tbl_sql&quot; ## [4] &quot;tbl_lazy&quot; &quot;tbl&quot; It is not just a normal tbl of data. We can see that from the structure of salesorderheader_table: str(salesorderheader_table, max.level = 3) ## List of 2 ## $ src:List of 2 ## ..$ con :Formal class &#39;PqConnection&#39; [package &quot;RPostgres&quot;] with 3 slots ## ..$ disco: NULL ## ..- attr(*, &quot;class&quot;)= chr [1:4] &quot;src_PqConnection&quot; &quot;src_dbi&quot; &quot;src_sql&quot; &quot;src&quot; ## $ ops:List of 4 ## ..$ name: chr &quot;select&quot; ## ..$ x :List of 2 ## .. ..$ x : &#39;ident_q&#39; chr &quot;sales.salesorderheader&quot; ## .. ..$ vars: chr [1:25] &quot;salesorderid&quot; &quot;revisionnumber&quot; &quot;orderdate&quot; &quot;duedate&quot; ... ## .. ..- attr(*, &quot;class&quot;)= chr [1:3] &quot;op_base_remote&quot; &quot;op_base&quot; &quot;op&quot; ## ..$ dots: list() ## ..$ args:List of 1 ## .. ..$ vars:List of 24 ## ..- attr(*, &quot;class&quot;)= chr [1:3] &quot;op_select&quot; &quot;op_single&quot; &quot;op&quot; ## - attr(*, &quot;class&quot;)= chr [1:5] &quot;tbl_PqConnection&quot; &quot;tbl_dbi&quot; &quot;tbl_sql&quot; &quot;tbl_lazy&quot; ... It has only two rows! The first row contains all the information in the con object, which contains information about all the tables and objects in the database. Here is a sample: salesorderheader_table$src$con@typnames$typname[387:418] ## [1] &quot;AccountNumber&quot; &quot;_AccountNumber&quot; ## [3] &quot;Flag&quot; &quot;_Flag&quot; ## [5] &quot;Name&quot; &quot;_Name&quot; ## [7] &quot;NameStyle&quot; &quot;_NameStyle&quot; ## [9] &quot;OrderNumber&quot; &quot;_OrderNumber&quot; ## [11] &quot;Phone&quot; &quot;_Phone&quot; ## [13] &quot;department&quot; &quot;_department&quot; ## [15] &quot;pg_toast_16439&quot; &quot;d&quot; ## [17] &quot;_d&quot; &quot;employee&quot; ## [19] &quot;_employee&quot; &quot;pg_toast_16450&quot; ## [21] &quot;e&quot; &quot;_e&quot; ## [23] &quot;employeedepartmenthistory&quot; &quot;_employeedepartmenthistory&quot; ## [25] &quot;edh&quot; &quot;_edh&quot; ## [27] &quot;employeepayhistory&quot; &quot;_employeepayhistory&quot; ## [29] &quot;pg_toast_16482&quot; &quot;eph&quot; ## [31] &quot;_eph&quot; &quot;jobcandidate&quot; The second row contains a list of the columns in the salesorderheader table, among other things: salesorderheader_table$ops$x$vars ## [1] &quot;salesorderid&quot; &quot;revisionnumber&quot; ## [3] &quot;orderdate&quot; &quot;duedate&quot; ## [5] &quot;shipdate&quot; &quot;status&quot; ## [7] &quot;onlineorderflag&quot; &quot;purchaseordernumber&quot; ## [9] &quot;accountnumber&quot; &quot;customerid&quot; ## [11] &quot;salespersonid&quot; &quot;territoryid&quot; ## [13] &quot;billtoaddressid&quot; &quot;shiptoaddressid&quot; ## [15] &quot;shipmethodid&quot; &quot;creditcardid&quot; ## [17] &quot;creditcardapprovalcode&quot; &quot;currencyrateid&quot; ## [19] &quot;subtotal&quot; &quot;taxamt&quot; ## [21] &quot;freight&quot; &quot;totaldue&quot; ## [23] &quot;comment&quot; &quot;rowguid&quot; ## [25] &quot;modifieddate&quot; salesorderheader_table holds information needed to get the data from the ‘salesorderheader’ table, but salesorderheader_table does not hold the data itself. In the following sections, we will examine more closely this relationship between the salesorderheader_table object and the data in the database’s ‘salesorderheader’ table. Disconnect from the database: dbDisconnect(con) 6.5 Cleaning up Always have R disconnect from the database when you’re done. dbDisconnect(con) ## Warning in connection_release(conn@ptr): Already disconnected Stop the adventureworks container: sp_docker_stop(&quot;adventureworks&quot;) "],
["chapter-dbms-queries-intro.html", "Chapter 7 Introduction to DBMS queries 7.1 Setup 7.2 Methods for downloading a single table 7.3 Translating dplyr code to SQL queries 7.4 Mixing dplyr and SQL 7.5 Examining a single table with R 7.6 Additional reading", " Chapter 7 Introduction to DBMS queries This chapter demonstrates how to: Download all or part of a table from the DBMS, including different kinds of subsets See how dplyr code is translated into SQL commands and how they can be mixed Get acquainted with some useful functions and packages for investigating a single table Begin thinking about how to divide the work between your local R session and the DBMS 7.1 Setup The following packages are used in this chapter: library(tidyverse) library(DBI) library(RPostgres) library(dbplyr) require(knitr) library(bookdown) library(sqlpetr) library(skimr) Assume that the Docker container with PostgreSQL and the adventureworks database are ready to go. If not go back to [Chapter 6][#chapter_setup-adventureworks-db] sqlpetr::sp_docker_start(&quot;adventureworks&quot;) Connect to the database: con &lt;- sqlpetr::sp_get_postgres_connection( user = Sys.getenv(&quot;DEFAULT_POSTGRES_USER_NAME&quot;), password = Sys.getenv(&quot;DEFAULT_POSTGRES_PASSWORD&quot;), dbname = &quot;adventureworks&quot;, port = 5432, seconds_to_test = 20, connection_tab = TRUE ) 7.2 Methods for downloading a single table For the moment, assume you know something about the database and specifically what table you need to retrieve. We return to the topic of investigating the whole database later on. dbExecute(con, &quot;set search_path to sales, humanresources;&quot;) ## [1] 0 7.2.1 Read the entire table There are a few different methods of getting data from a DBMS, and we’ll explore the different ways of controlling each one of them. DBI::dbReadTable will download an entire table into an R tibble. salesorderheader_tibble &lt;- DBI::dbReadTable(con, &quot;salesorderheader&quot;) str(salesorderheader_tibble) ## &#39;data.frame&#39;: 31465 obs. of 25 variables: ## $ salesorderid : int 43659 43660 43661 43662 43663 43664 43665 43666 43667 43668 ... ## $ revisionnumber : int 8 8 8 8 8 8 8 8 8 8 ... ## $ orderdate : POSIXct, format: &quot;2011-05-31&quot; &quot;2011-05-31&quot; ... ## $ duedate : POSIXct, format: &quot;2011-06-12&quot; &quot;2011-06-12&quot; ... ## $ shipdate : POSIXct, format: &quot;2011-06-07&quot; &quot;2011-06-07&quot; ... ## $ status : int 5 5 5 5 5 5 5 5 5 5 ... ## $ onlineorderflag : logi FALSE FALSE FALSE FALSE FALSE FALSE ... ## $ purchaseordernumber : chr &quot;PO522145787&quot; &quot;PO18850127500&quot; &quot;PO18473189620&quot; &quot;PO18444174044&quot; ... ## $ accountnumber : chr &quot;10-4020-000676&quot; &quot;10-4020-000117&quot; &quot;10-4020-000442&quot; &quot;10-4020-000227&quot; ... ## $ customerid : int 29825 29672 29734 29994 29565 29898 29580 30052 29974 29614 ... ## $ salespersonid : int 279 279 282 282 276 280 283 276 277 282 ... ## $ territoryid : int 5 5 6 6 4 1 1 4 3 6 ... ## $ billtoaddressid : int 985 921 517 482 1073 876 849 1074 629 529 ... ## $ shiptoaddressid : int 985 921 517 482 1073 876 849 1074 629 529 ... ## $ shipmethodid : int 5 5 5 5 5 5 5 5 5 5 ... ## $ creditcardid : int 16281 5618 1346 10456 4322 806 15232 13349 10370 1566 ... ## $ creditcardapprovalcode: chr &quot;105041Vi84182&quot; &quot;115213Vi29411&quot; &quot;85274Vi6854&quot; &quot;125295Vi53935&quot; ... ## $ currencyrateid : int NA NA 4 4 NA NA NA NA NA 4 ... ## $ subtotal : num 20566 1294 32726 28833 419 ... ## $ taxamt : num 1971.5 124.2 3153.8 2775.2 40.3 ... ## $ freight : num 616.1 38.8 985.6 867.2 12.6 ... ## $ totaldue : num 23153 1457 36866 32475 472 ... ## $ comment : chr NA NA NA NA ... ## $ rowguid : chr &quot;79b65321-39ca-4115-9cba-8fe0903e12e6&quot; &quot;738dc42d-d03b-48a1-9822-f95a67ea7389&quot; &quot;d91b9131-18a4-4a11-bc3a-90b6f53e9d74&quot; &quot;4a1ecfc0-cc3a-4740-b028-1c50bb48711c&quot; ... ## $ modifieddate : POSIXct, format: &quot;2011-06-07&quot; &quot;2011-06-07&quot; ... That’s very simple, but if the table is very large it may not be a problem, since R is designed to keep the entire table in memory. The tables that are found in an enterprise database such as adventureworks may be large, they are most often records kept by people. That somewhat limits their size (relative to data generated by machines) and expands the possibilities for human error. Note that the first line of the str() output reports the total number of observations. Later on we’ll use this tibble to demonstrate several packages and functions, but use only the first 13 columns for simplicity. salesorderheader_tibble &lt;- salesorderheader_tibble[,1:13] 7.2.2 Create a pointer to a table that can be reused The dplyr::tbl function gives us more control over access to a table by enabling control over which columns and rows to download. It creates an object that might look like a data frame, but it’s actually a list object that dplyr uses for constructing queries and retrieving data from the DBMS. salesorderheader_table &lt;- dplyr::tbl(con, &quot;salesorderheader&quot;) class(salesorderheader_table) ## [1] &quot;tbl_PqConnection&quot; &quot;tbl_dbi&quot; &quot;tbl_sql&quot; ## [4] &quot;tbl_lazy&quot; &quot;tbl&quot; 7.2.3 Controlling the number of rows returned with collect() The collect function triggers the creation of a tibble and controls the number of rows that the DBMS sends to R. For more complex queries, the dplyr::collect() function provides a mechanism to indicate what’s processed on on the DBMS server and what’s processed by R on the local machine. The chapter on Lazy Evaluation and Execution Environment discusses this issue in detail. salesorderheader_table %&gt;% dplyr::collect(n = 3) %&gt;% dim() ## [1] 3 25 salesorderheader_table %&gt;% dplyr::collect(n = 500) %&gt;% dim() ## [1] 500 25 7.2.4 Retrieving random rows from the DBMS When the DBMS contains many rows, a sample of the data may be plenty for your purposes. Although dplyr has nice functions to sample a data frame that’s already in R (e.g., the sample_n and sample_frac functions), to get a sample from the DBMS we have to use dbGetQuery to send native SQL to the database. To peek ahead, here is one example of a query that retrieves 20 rows from a 1% sample: one_percent_sample &lt;- DBI::dbGetQuery( con, &quot;SELECT orderdate, subtotal, taxamt, freight, totaldue FROM salesorderheader TABLESAMPLE BERNOULLI(3) LIMIT 20; &quot; ) one_percent_sample ## orderdate subtotal taxamt freight totaldue ## 1 2011-05-31 5716.3102 546.8759 170.8987 6434.0848 ## 2 2011-05-31 20541.4072 1969.5564 615.4864 23126.4500 ## 3 2011-06-08 3399.9900 271.9992 84.9998 3756.9890 ## 4 2011-06-10 3578.2700 286.2616 89.4568 3953.9884 ## 5 2011-06-21 3399.9900 271.9992 84.9998 3756.9890 ## 6 2011-06-27 3578.2700 286.2616 89.4568 3953.9884 ## 7 2011-07-01 4049.9880 388.7988 121.4996 4560.2864 ## 8 2011-07-07 3578.2700 286.2616 89.4568 3953.9884 ## 9 2011-07-07 3578.2700 286.2616 89.4568 3953.9884 ## 10 2011-07-13 699.0982 55.9279 17.4775 772.5036 ## 11 2011-07-30 3399.9900 271.9992 84.9998 3756.9890 ## 12 2011-08-16 699.0982 55.9279 17.4775 772.5036 ## 13 2011-08-19 3578.2700 286.2616 89.4568 3953.9884 ## 14 2011-08-20 3578.2700 286.2616 89.4568 3953.9884 ## 15 2011-08-31 32383.1098 3108.9064 971.5333 36463.5495 ## 16 2011-09-03 3578.2700 286.2616 89.4568 3953.9884 ## 17 2011-09-09 3578.2700 286.2616 89.4568 3953.9884 ## 18 2011-09-15 3578.2700 286.2616 89.4568 3953.9884 ## 19 2011-10-01 22817.6077 2197.4479 686.7025 25701.7581 ## 20 2011-10-01 2146.9620 206.1084 64.4089 2417.4793 Exact sample of 100 records This technique depends on knowing the range of a record index, such as the businessentityid in the salesorderheader table of our adventureworks database. Start by finding the min and max values. DBI::dbListFields(con, &quot;salesorderheader&quot;) ## [1] &quot;salesorderid&quot; &quot;revisionnumber&quot; ## [3] &quot;orderdate&quot; &quot;duedate&quot; ## [5] &quot;shipdate&quot; &quot;status&quot; ## [7] &quot;onlineorderflag&quot; &quot;purchaseordernumber&quot; ## [9] &quot;accountnumber&quot; &quot;customerid&quot; ## [11] &quot;salespersonid&quot; &quot;territoryid&quot; ## [13] &quot;billtoaddressid&quot; &quot;shiptoaddressid&quot; ## [15] &quot;shipmethodid&quot; &quot;creditcardid&quot; ## [17] &quot;creditcardapprovalcode&quot; &quot;currencyrateid&quot; ## [19] &quot;subtotal&quot; &quot;taxamt&quot; ## [21] &quot;freight&quot; &quot;totaldue&quot; ## [23] &quot;comment&quot; &quot;rowguid&quot; ## [25] &quot;modifieddate&quot; salesorderheader_df &lt;- DBI::dbReadTable(con, &quot;salesorderheader&quot;) (max_id &lt;- max(salesorderheader_df$salesorderid)) ## [1] 75123 (min_id &lt;- min(salesorderheader_df$salesorderid)) ## [1] 43659 Set the random number seed and draw the sample. set.seed(123) sample_rows &lt;- sample(1:max(salesorderheader_df$salesorderid), 10) salesorderheader_table &lt;- dplyr::tbl(con, &quot;salesorderheader&quot;) Run query with the filter verb listing the randomly sampled rows to be retrieved: salesorderheader_sample &lt;- salesorderheader_table %&gt;% dplyr::filter(salesorderid %in% sample_rows) %&gt;% dplyr::collect() str(salesorderheader_sample) ## Classes &#39;tbl_df&#39;, &#39;tbl&#39; and &#39;data.frame&#39;: 7 obs. of 25 variables: ## $ salesorderid : int 45404 46435 51663 57870 62555 65161 68293 ## $ revisionnumber : int 8 8 8 8 8 8 8 ## $ orderdate : POSIXct, format: &quot;2012-01-10&quot; &quot;2012-05-06&quot; ... ## $ duedate : POSIXct, format: &quot;2012-01-22&quot; &quot;2012-05-18&quot; ... ## $ shipdate : POSIXct, format: &quot;2012-01-17&quot; &quot;2012-05-13&quot; ... ## $ status : int 5 5 5 5 5 5 5 ## $ onlineorderflag : logi TRUE TRUE TRUE TRUE TRUE FALSE ... ## $ purchaseordernumber : chr NA NA NA NA ... ## $ accountnumber : chr &quot;10-4030-011217&quot; &quot;10-4030-012251&quot; &quot;10-4030-016327&quot; &quot;10-4030-018572&quot; ... ## $ customerid : int 11217 12251 16327 18572 13483 29799 13239 ## $ salespersonid : int NA NA NA NA NA 281 NA ## $ territoryid : int 1 9 8 4 1 4 6 ## $ billtoaddressid : int 19321 24859 19265 16902 15267 997 27923 ## $ shiptoaddressid : int 19321 24859 19265 16902 15267 997 27923 ## $ shipmethodid : int 1 1 1 1 1 5 1 ## $ creditcardid : int 8241 13188 16357 1884 4409 12582 1529 ## $ creditcardapprovalcode: chr &quot;332581Vi42712&quot; &quot;635144Vi68383&quot; &quot;420152Vi84562&quot; &quot;1224478Vi9772&quot; ... ## $ currencyrateid : int NA 4121 NA NA NA NA 11581 ## $ subtotal : num 3578 3375 2466 14 57 ... ## $ taxamt : num 286.26 270 197.31 1.12 4.56 ... ## $ freight : num 89.457 84.375 61.658 0.349 1.424 ... ## $ totaldue : num 3954 3729.4 2725.3 15.4 63 ... ## $ comment : chr NA NA NA NA ... ## $ rowguid : chr &quot;358f91b2-dadd-4014-8d4f-7f9736cb664e&quot; &quot;eb312409-fcd5-4bac-bd3b-16d4bd7889db&quot; &quot;ddc60552-af98-4166-9249-d09d424d8430&quot; &quot;fe46e631-47b9-4e14-9da5-1e4a4a135364&quot; ... ## $ modifieddate : POSIXct, format: &quot;2012-01-17&quot; &quot;2012-05-13&quot; ... 7.2.5 Sub-setting variables A table in the DBMS may not only have many more rows than you want, but also many more columns. The select command controls which columns are retrieved. salesorderheader_table %&gt;% dplyr::select(orderdate, subtotal, taxamt, freight, totaldue) %&gt;% head() ## # Source: lazy query [?? x 5] ## # Database: postgres [postgres@localhost:5432/adventureworks] ## orderdate subtotal taxamt freight totaldue ## &lt;dttm&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 2011-05-31 00:00:00 20566. 1972. 616. 23153. ## 2 2011-05-31 00:00:00 1294. 124. 38.8 1457. ## 3 2011-05-31 00:00:00 32726. 3154. 986. 36866. ## 4 2011-05-31 00:00:00 28833. 2775. 867. 32475. ## 5 2011-05-31 00:00:00 419. 40.3 12.6 472. ## 6 2011-05-31 00:00:00 24433. 2345. 733. 27510. That’s exactly equivalent to submitting the following SQL commands directly: DBI::dbGetQuery( con, &#39;SELECT &quot;orderdate&quot;, &quot;subtotal&quot;, &quot;taxamt&quot;, &quot;freight&quot;, &quot;totaldue&quot; FROM &quot;salesorderheader&quot; LIMIT 6&#39;) ## orderdate subtotal taxamt freight totaldue ## 1 2011-05-31 20565.6206 1971.5149 616.0984 23153.2339 ## 2 2011-05-31 1294.2529 124.2483 38.8276 1457.3288 ## 3 2011-05-31 32726.4786 3153.7696 985.5530 36865.8012 ## 4 2011-05-31 28832.5289 2775.1646 867.2389 32474.9324 ## 5 2011-05-31 419.4589 40.2681 12.5838 472.3108 ## 6 2011-05-31 24432.6088 2344.9921 732.8100 27510.4109 We won’t discuss dplyr methods for sub-setting variables, deriving new ones, or sub-setting rows based on the values found in the table, because they are covered well in other places, including: Comprehensive reference: https://dplyr.tidyverse.org/ Good tutorial: https://suzan.rbind.io/tags/dplyr/ In practice we find that, renaming variables is often quite important because the names in an SQL database might not meet your needs as an analyst. In “the wild”, you will find names that are ambiguous or overly specified, with spaces in them, and other problems that will make them difficult to use in R. It is good practice to do whatever renaming you are going to do in a predictable place like at the top of your code. The names in the adventureworks database are simple and clear, but if they were not, you might rename them for subsequent use in this way: tbl(con, &quot;salesorderheader&quot;) %&gt;% dplyr::rename(order_date = orderdate, sub_total_amount = subtotal, tax_amount = taxamt, freight_amount = freight, total_due_amount = totaldue) %&gt;% dplyr::select(order_date, sub_total_amount, tax_amount, freight_amount, total_due_amount ) %&gt;% # head() show_query() ## &lt;SQL&gt; ## SELECT &quot;orderdate&quot; AS &quot;order_date&quot;, &quot;subtotal&quot; AS &quot;sub_total_amount&quot;, &quot;taxamt&quot; AS &quot;tax_amount&quot;, &quot;freight&quot; AS &quot;freight_amount&quot;, &quot;totaldue&quot; AS &quot;total_due_amount&quot; ## FROM &quot;salesorderheader&quot; That’s equivalent to the following SQL code: DBI::dbGetQuery( con, &#39;SELECT &quot;orderdate&quot; AS &quot;order_date&quot;, &quot;subtotal&quot; AS &quot;sub_total_amount&quot;, &quot;taxamt&quot; AS &quot;tax_amount&quot;, &quot;freight&quot; AS &quot;freight_amount&quot;, &quot;totaldue&quot; AS &quot;total_due_amount&quot; FROM &quot;salesorderheader&quot;&#39; ) %&gt;% head() ## order_date sub_total_amount tax_amount freight_amount total_due_amount ## 1 2011-05-31 20565.6206 1971.5149 616.0984 23153.2339 ## 2 2011-05-31 1294.2529 124.2483 38.8276 1457.3288 ## 3 2011-05-31 32726.4786 3153.7696 985.5530 36865.8012 ## 4 2011-05-31 28832.5289 2775.1646 867.2389 32474.9324 ## 5 2011-05-31 419.4589 40.2681 12.5838 472.3108 ## 6 2011-05-31 24432.6088 2344.9921 732.8100 27510.4109 The one difference is that the SQL code returns a regular data frame and the dplyr code returns a tibble. Notice that the seconds are grayed out in the tibble display. 7.3 Translating dplyr code to SQL queries Where did the translations we’ve shown above come from? The show_query function shows how dplyr is translating your query to the dialect of the target DBMS: salesorderheader_table %&gt;% dplyr::tally() %&gt;% dplyr::show_query() ## &lt;SQL&gt; ## SELECT COUNT(*) AS &quot;n&quot; ## FROM &quot;salesorderheader&quot; Here is an extensive discussion of how dplyr code is translated into SQL: https://dbplyr.tidyverse.org/articles/sql-translation.html If you prefer to use SQL directly, rather than dplyr, you can submit SQL code to the DBMS through the DBI::dbGetQuery function: DBI::dbGetQuery( con, &#39;SELECT COUNT(*) AS &quot;n&quot; FROM &quot;salesorderheader&quot; &#39; ) ## n ## 1 31465 When you create a report to run repeatedly, you might want to put that query into R markdown. That way you can also execute that SQL code in a chunk with the following header: {sql, connection=con, output.var = \"query_results\"} SELECT COUNT(*) AS &quot;n&quot; FROM &quot;salesorderheader&quot;; R markdown stores that query result in a tibble which can be printed by referring to it: query_results ## n ## 1 31465 7.4 Mixing dplyr and SQL When dplyr finds code that it does not know how to translate into SQL, it will simply pass it along to the DBMS. Therefore you can interleave native commands that your DBMS will understand in the middle of dplyr code. Consider this example that’s derived from (Ruiz 2019): salesorderheader_table %&gt;% dplyr::select_at(vars(subtotal, contains(&quot;date&quot;))) %&gt;% dplyr::mutate(today = now()) %&gt;% dplyr::show_query() ## &lt;SQL&gt; ## SELECT &quot;subtotal&quot;, &quot;orderdate&quot;, &quot;duedate&quot;, &quot;shipdate&quot;, &quot;modifieddate&quot;, CURRENT_TIMESTAMP AS &quot;today&quot; ## FROM &quot;salesorderheader&quot; That is native to PostgreSQL, not ANSI standard SQL. Verify that it works: salesorderheader_table %&gt;% dplyr::select_at(vars(subtotal, contains(&quot;date&quot;))) %&gt;% head() %&gt;% dplyr::mutate(today = now()) %&gt;% dplyr::collect() ## # A tibble: 6 x 6 ## subtotal orderdate duedate shipdate ## &lt;dbl&gt; &lt;dttm&gt; &lt;dttm&gt; &lt;dttm&gt; ## 1 20566. 2011-05-31 00:00:00 2011-06-12 00:00:00 2011-06-07 00:00:00 ## 2 1294. 2011-05-31 00:00:00 2011-06-12 00:00:00 2011-06-07 00:00:00 ## 3 32726. 2011-05-31 00:00:00 2011-06-12 00:00:00 2011-06-07 00:00:00 ## 4 28833. 2011-05-31 00:00:00 2011-06-12 00:00:00 2011-06-07 00:00:00 ## 5 419. 2011-05-31 00:00:00 2011-06-12 00:00:00 2011-06-07 00:00:00 ## 6 24433. 2011-05-31 00:00:00 2011-06-12 00:00:00 2011-06-07 00:00:00 ## # … with 2 more variables: modifieddate &lt;dttm&gt;, today &lt;dttm&gt; 7.5 Examining a single table with R Dealing with a large, complex database highlights the utility of specific tools in R. We include brief examples that we find to be handy: Base R structure: str Printing out some of the data: datatable, kable, and View Summary statistics: summary glimpse in the tibble package, which is included in the tidyverse skim in the skimr package 7.5.1 str - a base package workhorse str is a workhorse function that lists variables, their type and a sample of the first few variable values. str(salesorderheader_tibble) ## &#39;data.frame&#39;: 31465 obs. of 13 variables: ## $ salesorderid : int 43659 43660 43661 43662 43663 43664 43665 43666 43667 43668 ... ## $ revisionnumber : int 8 8 8 8 8 8 8 8 8 8 ... ## $ orderdate : POSIXct, format: &quot;2011-05-31&quot; &quot;2011-05-31&quot; ... ## $ duedate : POSIXct, format: &quot;2011-06-12&quot; &quot;2011-06-12&quot; ... ## $ shipdate : POSIXct, format: &quot;2011-06-07&quot; &quot;2011-06-07&quot; ... ## $ status : int 5 5 5 5 5 5 5 5 5 5 ... ## $ onlineorderflag : logi FALSE FALSE FALSE FALSE FALSE FALSE ... ## $ purchaseordernumber: chr &quot;PO522145787&quot; &quot;PO18850127500&quot; &quot;PO18473189620&quot; &quot;PO18444174044&quot; ... ## $ accountnumber : chr &quot;10-4020-000676&quot; &quot;10-4020-000117&quot; &quot;10-4020-000442&quot; &quot;10-4020-000227&quot; ... ## $ customerid : int 29825 29672 29734 29994 29565 29898 29580 30052 29974 29614 ... ## $ salespersonid : int 279 279 282 282 276 280 283 276 277 282 ... ## $ territoryid : int 5 5 6 6 4 1 1 4 3 6 ... ## $ billtoaddressid : int 985 921 517 482 1073 876 849 1074 629 529 ... 7.5.2 Always look at your data with head, View, or kable There is no substitute for looking at your data and R provides several ways to just browse it. The head function controls the number of rows that are displayed. Note that tail does not work against a database object. In every-day practice you would look at more than the default 6 rows, but here we wrap head around the data frame: sqlpetr::sp_print_df(head(salesorderheader_tibble)) 7.5.3 The summary function in base The base package’s summary function provides basic statistics that serve a unique diagnostic purpose in this context. For example, the following output shows that: * `businessentityid` is a number from 1 to 16,049. In a previous section, we ran the `str` function and saw that there are 16,044 observations in this table. Therefore, the `businessentityid` seems to be sequential from 1:16049, but there are 5 values missing from that sequence. _Exercise for the Reader_: Which 5 values from 1:16049 are missing from `businessentityid` values in the `salesorderheader` table? (_Hint_: In the chapter on SQL Joins, you will learn the functions needed to answer this question.) * The number of NA&#39;s in the `return_date` column is a good first guess as to the number of DVDs rented out or lost as of 2005-09-02 02:35:22. summary(salesorderheader_tibble) ## salesorderid revisionnumber orderdate ## Min. :43659 Min. :8.000 Min. :2011-05-31 00:00:00 ## 1st Qu.:51525 1st Qu.:8.000 1st Qu.:2013-06-20 00:00:00 ## Median :59391 Median :8.000 Median :2013-11-03 00:00:00 ## Mean :59391 Mean :8.001 Mean :2013-08-21 12:05:04 ## 3rd Qu.:67257 3rd Qu.:8.000 3rd Qu.:2014-02-28 00:00:00 ## Max. :75123 Max. :9.000 Max. :2014-06-30 00:00:00 ## ## duedate shipdate status ## Min. :2011-06-12 00:00:00 Min. :2011-06-07 00:00:00 Min. :5 ## 1st Qu.:2013-07-02 00:00:00 1st Qu.:2013-06-27 00:00:00 1st Qu.:5 ## Median :2013-11-15 00:00:00 Median :2013-11-10 00:00:00 Median :5 ## Mean :2013-09-02 12:05:41 Mean :2013-08-28 12:06:06 Mean :5 ## 3rd Qu.:2014-03-13 00:00:00 3rd Qu.:2014-03-08 00:00:00 3rd Qu.:5 ## Max. :2014-07-12 00:00:00 Max. :2014-07-07 00:00:00 Max. :5 ## ## onlineorderflag purchaseordernumber accountnumber customerid ## Mode :logical Length:31465 Length:31465 Min. :11000 ## FALSE:3806 Class :character Class :character 1st Qu.:14432 ## TRUE :27659 Mode :character Mode :character Median :19452 ## Mean :20170 ## 3rd Qu.:25994 ## Max. :30118 ## ## salespersonid territoryid billtoaddressid ## Min. :274.0 Min. : 1.000 Min. : 405 ## 1st Qu.:277.0 1st Qu.: 4.000 1st Qu.:14080 ## Median :279.0 Median : 6.000 Median :19449 ## Mean :280.6 Mean : 6.091 Mean :18263 ## 3rd Qu.:284.0 3rd Qu.: 9.000 3rd Qu.:24678 ## Max. :290.0 Max. :10.000 Max. :29883 ## NA&#39;s :27659 So the summary function is surprisingly useful as we first start to look at the table contents. 7.5.4 The glimpse function in the tibble package The tibble package’s glimpse function is a more compact version of str: tibble::glimpse(salesorderheader_tibble) ## Observations: 31,465 ## Variables: 13 ## $ salesorderid &lt;int&gt; 43659, 43660, 43661, 43662, 43663, 43664, 43… ## $ revisionnumber &lt;int&gt; 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,… ## $ orderdate &lt;dttm&gt; 2011-05-31, 2011-05-31, 2011-05-31, 2011-05… ## $ duedate &lt;dttm&gt; 2011-06-12, 2011-06-12, 2011-06-12, 2011-06… ## $ shipdate &lt;dttm&gt; 2011-06-07, 2011-06-07, 2011-06-07, 2011-06… ## $ status &lt;int&gt; 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,… ## $ onlineorderflag &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FA… ## $ purchaseordernumber &lt;chr&gt; &quot;PO522145787&quot;, &quot;PO18850127500&quot;, &quot;PO184731896… ## $ accountnumber &lt;chr&gt; &quot;10-4020-000676&quot;, &quot;10-4020-000117&quot;, &quot;10-4020… ## $ customerid &lt;int&gt; 29825, 29672, 29734, 29994, 29565, 29898, 29… ## $ salespersonid &lt;int&gt; 279, 279, 282, 282, 276, 280, 283, 276, 277,… ## $ territoryid &lt;int&gt; 5, 5, 6, 6, 4, 1, 1, 4, 3, 6, 1, 3, 1, 6, 2,… ## $ billtoaddressid &lt;int&gt; 985, 921, 517, 482, 1073, 876, 849, 1074, 62… 7.5.5 The skim function in the skimr package The skimr package has several functions that make it easy to examine an unknown data frame and assess what it contains. It is also extensible. skimr::skim(salesorderheader_tibble) ## Skim summary statistics ## n obs: 31465 ## n variables: 13 ## ## ── Variable type:character ───────────────────────────────────────────────────────── ## variable missing complete n min max empty n_unique ## accountnumber 0 31465 31465 14 14 0 19119 ## purchaseordernumber 27659 3806 31465 10 13 0 3806 ## ## ── Variable type:integer ─────────────────────────────────────────────────────────── ## variable missing complete n mean sd p0 p25 ## billtoaddressid 0 31465 31465 18263.15 8210.07 405 14080 ## customerid 0 31465 31465 20170.18 6261.73 11000 14432 ## revisionnumber 0 31465 31465 8 0.031 8 8 ## salesorderid 0 31465 31465 59391 9083.31 43659 51525 ## salespersonid 27659 3806 31465 280.61 4.85 274 277 ## status 0 31465 31465 5 0 5 5 ## territoryid 0 31465 31465 6.09 2.96 1 4 ## p50 p75 p100 hist ## 19449 24678 29883 ▆▁▁▇▇▇▇▇ ## 19452 25994 30118 ▇▆▅▅▃▃▅▇ ## 8 8 9 ▇▁▁▁▁▁▁▁ ## 59391 67257 75123 ▇▇▇▇▇▇▇▇ ## 279 284 290 ▇▆▅▅▃▁▂▅ ## 5 5 5 ▁▁▁▇▁▁▁▁ ## 6 9 10 ▃▁▅▁▃▂▂▇ ## ## ── Variable type:logical ─────────────────────────────────────────────────────────── ## variable missing complete n mean count ## onlineorderflag 0 31465 31465 0.88 TRU: 27659, FAL: 3806, NA: 0 ## ## ── Variable type:POSIXct ─────────────────────────────────────────────────────────── ## variable missing complete n min max median ## duedate 0 31465 31465 2011-06-12 2014-07-12 2013-11-15 ## orderdate 0 31465 31465 2011-05-31 2014-06-30 2013-11-03 ## shipdate 0 31465 31465 2011-06-07 2014-07-07 2013-11-10 ## n_unique ## 1124 ## 1124 ## 1124 skimr::skim_to_wide(salesorderheader_tibble) #skimr doesn&#39;t like certain kinds of columns ## # A tibble: 13 x 19 ## type variable missing complete n min max empty n_unique mean ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 char… account… 0 31465 31465 14 14 0 19119 &lt;NA&gt; ## 2 char… purchas… 27659 3806 31465 10 13 0 3806 &lt;NA&gt; ## 3 inte… billtoa… 0 31465 31465 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; 1826… ## 4 inte… custome… 0 31465 31465 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; 2017… ## 5 inte… revisio… 0 31465 31465 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &quot; … ## 6 inte… salesor… 0 31465 31465 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &quot;593… ## 7 inte… salespe… 27659 3806 31465 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &quot; 2… ## 8 inte… status 0 31465 31465 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &quot; … ## 9 inte… territo… 0 31465 31465 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &quot; … ## 10 logi… onlineo… 0 31465 31465 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; 0.88 ## 11 POSI… duedate 0 31465 31465 2011… 2014… &lt;NA&gt; 1124 &lt;NA&gt; ## 12 POSI… orderda… 0 31465 31465 2011… 2014… &lt;NA&gt; 1124 &lt;NA&gt; ## 13 POSI… shipdate 0 31465 31465 2011… 2014… &lt;NA&gt; 1124 &lt;NA&gt; ## # … with 9 more variables: sd &lt;chr&gt;, p0 &lt;chr&gt;, p25 &lt;chr&gt;, p50 &lt;chr&gt;, ## # p75 &lt;chr&gt;, p100 &lt;chr&gt;, hist &lt;chr&gt;, count &lt;chr&gt;, median &lt;chr&gt; 7.5.6 Close the connection and shut down adventureworks Where you place the collect function matters. DBI::dbDisconnect(con) sqlpetr::sp_docker_stop(&quot;adventureworks&quot;) 7.6 Additional reading (Wickham 2018) (Baumer 2018) References "],
["chapter-exploring-a-single-table.html", "Chapter 8 Exploring a Single Table (asking Business Questions) 8.1 Setup our standard working environment 8.2 The overall AdventureWorks sales picture 8.3 Annual sales 8.4 Monthly Sales 8.5 The effect of online sales 8.6 Impact of order type on monthly sales 8.7 Variation over time 8.8 These queries and graphs may be redundant 8.9 Monthly Sales Rep Performance Analysis 8.10 Close and clean up", " Chapter 8 Exploring a Single Table (asking Business Questions) This chapter demonstrates how to: Begin the process of investigating a database from a business perspective Dig into a single Adventureworks table containing sales data Investigate the data from a business value perspective The previous chapter has demonstrated some of the automated techniques for showing what’s in the table using specific R functions and packages. Now we demonstrate a step-by-step process of making sense of what’s in a table from a business perspective. We explore one table, illustrating the kind of detective work that’s often involved in understand the meaning of the data in a single table. We’ll investigate the salesorderheader table in the sales schema in this example with an eye on the AdventureWorks busieness’ sales. For this kind of detective work we are seeking to undertand the following elements separately and as they interact with each other (and they all do): The data that’s stored in the database The overall structure of the data (how different tables connect with each other and affect how to interpet each other) How the data is entered at a day-to-day level to represent business activities How the business itself is changing 8.1 Setup our standard working environment Use these libraries: library(tidyverse) library(DBI) library(RPostgres) library(glue) require(knitr) library(dbplyr) library(sqlpetr) library(bookdown) library(here) library(lubridate) library(scales) # ggplot xy scales theme_set(theme_light()) Connect to adventureworks: sp_docker_start(&quot;adventureworks&quot;) Sys.sleep(sleep_default) con &lt;- sp_get_postgres_connection( host = &quot;localhost&quot;, port = 5432, user = &quot;postgres&quot;, password = &quot;postgres&quot;, dbname = &quot;adventureworks&quot;, seconds_to_test = sleep_default, connection_tab = TRUE ) 8.2 The overall AdventureWorks sales picture On an annual basis, are sales dollars trending up, down or flat? We begin with total revenue and number of orders at different levels of detail. You will find that many columns have the same name in an enterprise database. For example, in the adventureworks database, almost all tables have columns named rowguid and modifieddate. Naming columns carefully (whether retrieved from the database or calculated) will pay off as our queries become more complex. Using soh to tag statistics that are derived from the salesorderheader table is one example of careful naming. A naming convention that reminds you of the original source of a column is a matter of applying some logic to how things are named; you, future you, and your collaborators will appreciate it although different naming conventions are completely valid. In the following example soh appears in different positions in the column name but it is easy to guess at a glance that the data comes from the salesorderheader table. 8.3 Annual sales annual_sales &lt;- tbl(con, in_schema(&quot;sales&quot;, &quot;salesorderheader&quot;)) %&gt;% mutate(year = substr(as.character(orderdate), 1, 4)) %&gt;% group_by(year) %&gt;% summarize( min_soh_orderdate = min(orderdate, na.rm = TRUE), max_soh_orderdate = max(orderdate, na.rm = TRUE), total_soh_dollars = round(sum(subtotal, na.rm = TRUE), 2), avg_total_soh_dollars = round(mean(subtotal, na.rm = TRUE),2), soh_count = n() ) %&gt;% arrange(year) %&gt;% select(year, min_soh_orderdate, max_soh_orderdate, total_soh_dollars, avg_total_soh_dollars, soh_count) %&gt;% show_query %&gt;% collect() ## &lt;SQL&gt; ## SELECT &quot;year&quot;, &quot;min_soh_orderdate&quot;, &quot;max_soh_orderdate&quot;, &quot;total_soh_dollars&quot;, &quot;avg_total_soh_dollars&quot;, &quot;soh_count&quot; ## FROM (SELECT * ## FROM (SELECT &quot;year&quot;, MIN(&quot;orderdate&quot;) AS &quot;min_soh_orderdate&quot;, MAX(&quot;orderdate&quot;) AS &quot;max_soh_orderdate&quot;, ROUND((SUM(&quot;subtotal&quot;)) :: numeric, 2) AS &quot;total_soh_dollars&quot;, ROUND((AVG(&quot;subtotal&quot;)) :: numeric, 2) AS &quot;avg_total_soh_dollars&quot;, COUNT(*) AS &quot;soh_count&quot; ## FROM (SELECT &quot;salesorderid&quot;, &quot;revisionnumber&quot;, &quot;orderdate&quot;, &quot;duedate&quot;, &quot;shipdate&quot;, &quot;status&quot;, &quot;onlineorderflag&quot;, &quot;purchaseordernumber&quot;, &quot;accountnumber&quot;, &quot;customerid&quot;, &quot;salespersonid&quot;, &quot;territoryid&quot;, &quot;billtoaddressid&quot;, &quot;shiptoaddressid&quot;, &quot;shipmethodid&quot;, &quot;creditcardid&quot;, &quot;creditcardapprovalcode&quot;, &quot;currencyrateid&quot;, &quot;subtotal&quot;, &quot;taxamt&quot;, &quot;freight&quot;, &quot;totaldue&quot;, &quot;comment&quot;, &quot;rowguid&quot;, &quot;modifieddate&quot;, SUBSTR(CAST(&quot;orderdate&quot; AS TEXT), 1, 4) AS &quot;year&quot; ## FROM sales.salesorderheader) &quot;dbplyr_001&quot; ## GROUP BY &quot;year&quot;) &quot;dbplyr_002&quot; ## ORDER BY &quot;year&quot;) &quot;dbplyr_003&quot; annual_sales ## # A tibble: 4 x 6 ## year min_soh_orderdate max_soh_orderdate total_soh_dolla… ## &lt;chr&gt; &lt;dttm&gt; &lt;dttm&gt; &lt;dbl&gt; ## 1 2011 2011-05-31 00:00:00 2011-12-31 00:00:00 12641672. ## 2 2012 2012-01-01 00:00:00 2012-12-31 00:00:00 33524301. ## 3 2013 2013-01-01 00:00:00 2013-12-31 00:00:00 43622479. ## 4 2014 2014-01-01 00:00:00 2014-06-30 00:00:00 20057929. ## # … with 2 more variables: avg_total_soh_dollars &lt;dbl&gt;, soh_count &lt;int64&gt; Both 2011 and 2014 are shorter time spans than the other two years, making comparison across the years more difficult. We might normalize the totals based on the number of months in each year, but we first graph total dollars: 8.3.1 Total sales by year min_soh_dt &lt;- min(annual_sales$min_soh_orderdate) max_soh_dt &lt;- max(annual_sales$max_soh_orderdate) ggplot(data = annual_sales, aes(x = year, y = total_soh_dollars)) + geom_col() + xlab(&quot;Year&quot;) + ylab(&quot;Sales $&quot;) + scale_y_continuous(labels = scales::dollar_format()) + ggtitle(paste(&quot;Adventure Works Sales Dollars by Year\\n &quot;, min_soh_dt, &quot; - &quot;, max_soh_dt)) From 2011 through 2013, sales are trending up. Are sales dollars for 2014 really down, is it a shorter year (are sales seasonal)? To see if the sales dollars are seasonal, we will drill in and look at the monthly sales. But first, let’s look at the number of orders and whether there’s a pattern in the sales data. 8.3.2 Total order volume Look at number of orders per year: ggplot(data = annual_sales, aes(x = year, y = as.numeric(soh_count))) + geom_col() + xlab(&quot;Year&quot;) + ylab(&quot;Total number of orders&quot;) + ggtitle(paste(&quot;Number of Orders per Year\\n&quot;, min_soh_dt, &quot; - &quot;, max_soh_dt)) That’s a huge jump in the number of orders between 2012 and 2013. Given the total annual dollars, we ask whether the size of a sale has changed. 8.3.3 Average dollars per sale ggplot(data = annual_sales, aes(x = year, y = avg_total_soh_dollars)) + geom_col() + xlab(&quot;Year&quot;) + ylab(&quot;Average sale amount&quot;) + scale_y_continuous(labels = scales::dollar_format()) + ggtitle(paste(&quot;Average Dollars per Sale\\n&quot;, min_soh_dt, &quot; - &quot;, max_soh_dt)) That’s a remarkable drop between average sale of more than $7,000 to less than $3,000. 8.3.4 Order volume and average sale together Looking at order volume and average sales size together suggests that indeed something big happened between 2012 and 2013. annual_sales %&gt;% arrange(min_soh_orderdate) %&gt;% ggplot(aes(x = avg_total_soh_dollars, y = as.numeric(soh_count))) + geom_point() + geom_text(aes(label = year, hjust = .5, vjust = 0)) + geom_path() + xlab(&quot;Average dollars per order&quot;) + ylab(&quot;Total number of orders&quot;) + scale_y_continuous(labels = scales::dollar_format()) + scale_x_continuous(labels = scales::dollar_format()) + ggtitle(paste(&quot;Number of Orders by Average Order Amount\\n&quot;, min_soh_dt, &quot; - &quot;, max_soh_dt)) From 2012 to 2013 the average dollars per order dropped from more than $8,500 to nearly $3,000 while the total number of order shot up from less than 4,000 to more than 14,000. Why are the number of orders increasing, but the average dollar amount of a sale is dropping? We need to drill down to look at monthly sales, adapting the first query to group by month and year. 8.4 Monthly Sales The next code block drills down from annual sales dollars to monthly sales dollars. We adopt a more R-like strategy; rather than yr as a character variable, we just download the orderdate. R handles the coversion from the postgres date-time to an R date-time. We then convert it to a simple date with a lubridate function. monthly_sales &lt;- tbl(con, in_schema(&quot;sales&quot;, &quot;salesorderheader&quot;)) %&gt;% select(orderdate, subtotal) %&gt;% collect() %&gt;% # From here on we&#39;re in R mutate(orderdate = date(orderdate), orderdate = round_date(orderdate, &quot;month&quot;)) %&gt;% # group_by( orderdate) %&gt;% summarize( min_soh_orderdate = min(orderdate, na.rm = TRUE), max_soh_orderdate = max(orderdate, na.rm = TRUE), total_soh_dollars = round(sum(subtotal, na.rm = TRUE), 2), avg_total_soh_dollars = round(mean(subtotal, na.rm = TRUE), 2), soh_count = n() ) Plotting the monthly sales data: ggplot(data = monthly_sales, aes(x = orderdate, y = total_soh_dollars)) + geom_col() + xlab(&quot;Month&quot;) + ylab(&quot;Sales Dollars&quot;) + scale_y_continuous(labels = dollar) + theme(plot.title = element_text(hjust = 0.5)) + # Center the title ggtitle(paste(&quot;Sales by Month\\n&quot;, min_soh_dt, &quot; - &quot;, max_soh_dt)) The total sales are trending up but suspiciously uneven. Looking at lags might confirm just how much month-to-month difference there is: lagged_monthly_sales &lt;- monthly_sales %&gt;% mutate(monthly_sales_change = (lag(total_soh_dollars, 1)) - total_soh_dollars) summary(lagged_monthly_sales$monthly_sales_change) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## -1667368.56 -1082792.47 52892.65 18287.07 816048.02 4399378.90 ## NA&#39;s ## 1 Although median monthly sales rise by about $53 thousand, it’s curious that in one fourth of the months the drop between one month and the next is between fifteen hundred and a million dollars. AventureWorks sales are very uneven. 8.4.1 Comparing dollars and orders to a baseline To look at dollars and the number of orders together, we compare the monthly data to the yearly average for 2011. start_year &lt;- monthly_sales %&gt;% mutate(yr = year(orderdate)) %&gt;% group_by(yr) %&gt;% summarize(total_soh_dollars = sum(total_soh_dollars), soh_count = sum(soh_count), n_months = n(), avg_dollars = total_soh_dollars / n_months, avg_count = soh_count / n_months) %&gt;% filter(yr == min(yr)) Use 2011 as a baseline: start_year ## # A tibble: 1 x 6 ## yr total_soh_dollars soh_count n_months avg_dollars avg_count ## &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 2011 12354206. 1513 7 1764887. 216. Re express monthly data in terms of the baseline and plot: normalized_monthly_sales &lt;- monthly_sales %&gt;% mutate(dollars = (100 * total_soh_dollars) / start_year$avg_dollars, number_of_orders = (100 * soh_count) / start_year$avg_count) %&gt;% ungroup() normalized_monthly_sales &lt;- normalized_monthly_sales %&gt;% select(orderdate, dollars, number_of_orders) %&gt;% pivot_longer(-orderdate, names_to = &quot;relative_to_2011_average&quot;, values_to = &quot;amount&quot; ) normalized_monthly_sales %&gt;% ggplot(aes(orderdate, amount, color = relative_to_2011_average)) + geom_line() + geom_hline(yintercept = 100) + xlab(&quot;Date&quot;) + ylab(&quot;&quot;) + scale_x_date(date_labels = &quot;%Y-%m&quot;, date_breaks = &quot;6 months&quot;) + ggtitle(paste(&quot;Adventureworks Normalized Monthly Sales\\n&quot;, &quot;Number of Sales Orders and Dollar Totals\\n&quot;, min_soh_dt, &quot; to &quot;, max_soh_dt)) 8.5 The effect of online sales We have suspected that the business has changed a lot with the advent of online orders so we check the impact of onlineorderflag on annual sales. 8.5.1 Add onlineorderflag to our query annual_sales &lt;- tbl(con, in_schema(&quot;sales&quot;, &quot;salesorderheader&quot;)) %&gt;% select(orderdate, subtotal, onlineorderflag) %&gt;% collect() %&gt;% mutate(orderdate = date(orderdate), orderdate = round_date(orderdate, &quot;year&quot;), onlineorderflag = if_else(onlineorderflag == FALSE, &quot;Sales Rep&quot;, &quot;Online&quot;), onlineorderflag = as.factor(onlineorderflag)) %&gt;% group_by(orderdate, onlineorderflag) %&gt;% summarize( min_soh_orderdate = min(orderdate, na.rm = TRUE), max_soh_orderdate = max(orderdate, na.rm = TRUE), total_soh_dollars = round(sum(subtotal, na.rm = TRUE), 2), avg_total_soh_dollars = round(mean(subtotal, na.rm = TRUE),2), soh_count = n() ) %&gt;% select(orderdate, onlineorderflag, min_soh_orderdate, max_soh_orderdate, total_soh_dollars, avg_total_soh_dollars, soh_count) 8.5.2 Annual Sales comparison Start by looking at total sales. ggplot(data = annual_sales, aes(x = orderdate, y = total_soh_dollars)) + geom_col() + xlab(&quot;Year&quot;) + ylab(&quot;Sales $&quot;) + scale_y_continuous(labels = scales::dollar_format()) + facet_wrap(&quot;onlineorderflag&quot;) + ggtitle(paste(&quot;Adventure Works Sales Dollars by Year\\n &quot;, min_soh_dt, &quot; - &quot;, max_soh_dt)) Indeed the total sales are quite different as are the number of orders and the average order size! 8.5.3 Order volume comparison Look at number of orders per year: ggplot(data = annual_sales, aes(x = orderdate, y = as.numeric(soh_count))) + geom_col() + xlab(&quot;Year&quot;) + facet_wrap(&quot;onlineorderflag&quot;) + ylab(&quot;Total number of orders&quot;) + ggtitle(paste(&quot;Number of Orders per Year\\n&quot;, min_soh_dt, &quot; - &quot;, max_soh_dt)) 8.5.4 Average sale comparison ggplot(data = annual_sales, aes(x = orderdate, y = avg_total_soh_dollars)) + geom_col() + xlab(&quot;Year&quot;) + ylab(&quot;Average sale amount&quot;) + facet_wrap(&quot;onlineorderflag&quot;) + scale_y_continuous(labels = scales::dollar_format()) + ggtitle(paste(&quot;Average Dollars per Sale\\n&quot;, min_soh_dt, &quot; - &quot;, max_soh_dt)) 8.5.5 Comparing order volume and average sale together Look at number of orders by the the average sales per order for the four years: annual_sales %&gt;% arrange(orderdate) %&gt;% ggplot(aes(x = avg_total_soh_dollars, y = as.numeric(soh_count))) + geom_point() + geom_text(aes(label = orderdate, hjust = .5, vjust = 0)) + geom_path() + xlab(&quot;Average dollars per order&quot;) + ylab(&quot;Total number of orders&quot;) + facet_wrap(&quot;onlineorderflag&quot;, scales = &quot;free&quot;) + scale_y_continuous(labels = scales::dollar_format()) + scale_x_continuous(labels = scales::dollar_format()) + ggtitle(paste(&quot;Number of Orders by Average Order Amount\\n&quot;, min_soh_dt, &quot; - &quot;, max_soh_dt)) From 2012 to 2013 the average dollars per order dropped from more than $8,500 to nearly $3,000 while the total number of order shot up from less than 4,000 to more than 14,000. Why are the number of orders increasing, but the average order dollar amount dropping? 8.6 Impact of order type on monthly sales Digging into the difference between Sales Rep and Online sales. 8.6.1 Retrieve monthly sales with the onlineorderflag This query puts the collect statement earlier than the previous queries. monthly_sales &lt;- tbl(con, in_schema(&quot;sales&quot;, &quot;salesorderheader&quot;)) %&gt;% select(orderdate, subtotal, onlineorderflag) %&gt;% collect() %&gt;% # From here on we&#39;re in R mutate(orderdate = date(orderdate), orderdate = round_date(orderdate, &quot;month&quot;), onlineorderflag = if_else(onlineorderflag == FALSE, &quot;Sales Rep&quot;, &quot;Online&quot;),) %&gt;% # group_by(orderdate, onlineorderflag) %&gt;% summarize( min_soh_orderdate = min(orderdate, na.rm = TRUE), max_soh_orderdate = max(orderdate, na.rm = TRUE), total_soh_dollars = round(sum(subtotal, na.rm = TRUE), 2), avg_total_soh_dollars = round(mean(subtotal, na.rm = TRUE), 2), soh_count = n() ) 8.6.2 Monthly variation compared to a trend line Jumping to the trend line comparison, we see that the variation # sp_print_df(monthly_sales) ggplot( data = monthly_sales, aes( x = orderdate, y = total_soh_dollars)) + geom_line() + geom_smooth(se = FALSE) + xlab(&quot;Month&quot;) + ylab(&quot;Sales Dollars&quot;) + facet_wrap(&quot;onlineorderflag&quot;) + scale_y_continuous(labels = dollar) + scale_x_date(date_breaks = &quot;year&quot;, date_labels = &quot;%Y&quot;, date_minor_breaks = &quot;3 months&quot;) + theme(plot.title = element_text(hjust = .5)) + # Center ggplot title ggtitle(paste(&quot;Sales by Month by Year\\n&quot;, &quot;With Number of Sales Orders\\nAnd Average SO $ Amount\\n&quot;, min_soh_dt, &quot; - &quot;, max_soh_dt)) ## `geom_smooth()` using method = &#39;loess&#39; and formula &#39;y ~ x&#39; Figure 8.1: SO, SO Dollars, and Average SO Dollars-b A couple of things jump out from the graph. 2012 and 2013 have similar sales dollar plots and peak every three months. This may reflect the closing as many sales orders as possible to make the quarterly sales numbers look good. 2011 has more variation than 2012 and 2013 and peaks every two months. 2014 has the most variation and also peaks every two months. Both the number of sales, 939, and the average sales order size, $52.19 plummet in June 2014. 8.6.3 Looking at lagged data lagged &lt;- monthly_sales %&gt;% group_by(onlineorderflag) %&gt;% mutate(pct_yearly_soh_dollar_change = total_soh_dollars / (lag(total_soh_dollars, 12)) * 100, pct_yearly_soh_count_change = soh_count / (lag(soh_count, 12)) * 100) ggplot(lagged, aes(x = orderdate, y = pct_yearly_soh_dollar_change)) + scale_x_date(date_breaks = &quot;year&quot;, date_labels = &quot;%Y&quot;, date_minor_breaks = &quot;3 months&quot;) + facet_wrap(&quot;onlineorderflag&quot;) + geom_line() + xlab(&quot;Month&quot;) + ylab(&quot;% Dollar Change&quot;) + theme(plot.title = element_text(hjust = .5)) + # Center ggplot title ggtitle(paste(&quot;Year on Year Total Monthly Sales Change \\n&quot;, &quot;Comparing Online to Sales Rep Sales\\n&quot;, min_soh_dt, &quot; - &quot;, max_soh_dt)) ## Warning: Removed 12 rows containing missing values (geom_path). ggplot(lagged, aes(x = orderdate, y = pct_yearly_soh_count_change)) + scale_x_date(date_breaks = &quot;year&quot;, date_labels = &quot;%Y&quot;, date_minor_breaks = &quot;3 months&quot;) + facet_wrap(&quot;onlineorderflag&quot;) + geom_line() + xlab(&quot;Month&quot;) + ylab(&quot;% Dollar Change&quot;) + theme(plot.title = element_text(hjust = .5)) + # Center ggplot title ggtitle(paste(&quot;Year on Year Monthly Order Volume Change \\n&quot;, &quot;Comparing Online to Sales Rep Sales\\n&quot;, min_soh_dt, &quot; - &quot;, max_soh_dt)) ## Warning: Removed 12 rows containing missing values (geom_path). Comparing the number of sales orders year over year by month for 2013 and 2012, one can see that the 2013 sales are between 1.2 and 1.8 times larger than the corresponding month of 2012 from January through June. In July the 2013 sales are 5 to 6 times the 2012 sales orders. This trend continues into 2014 before the number of sales plummet to just 1.3 time in June. We suspect that the business has changed a lot with the adventn of online orders. 8.7 Variation over time sales_by_day_of_month &lt;- tbl(con, in_schema(&quot;sales&quot;, &quot;salesorderheader&quot;)) %&gt;% mutate(day_of_month = day(orderdate)) %&gt;% group_by(onlineorderflag, day_of_month) %&gt;% summarize( total_soh_dollars = round(sum(subtotal, na.rm = TRUE), 2), avg_total_soh_dollars = round(mean(subtotal, na.rm = TRUE),2), soh_count = n() ) %&gt;% arrange(day_of_month) %&gt;% collect() %&gt;% ungroup() %&gt;% mutate(onlineorderflag = if_else(onlineorderflag == FALSE, &quot;Sales Rep&quot;, &quot;Online&quot;), onlineorderflag = as.factor(onlineorderflag), day_of_month = as.numeric(day_of_month), soh_count = as.numeric(soh_count) ) ggplot(sales_by_day_of_month, aes(x = day_of_month, y = soh_count)) + # scale_x_date(date_breaks = &quot;year&quot;, date_labels = &quot;%Y&quot;, date_minor_breaks = &quot;3 months&quot;) + facet_wrap(&quot;onlineorderflag&quot;) + geom_col() + xlab(&quot;Day of the Month&quot;) + ylab(&quot;Recorded Sales&quot;) + theme(plot.title = element_text(hjust = .5)) + # Center ggplot title ggtitle(paste(&quot;Transactions Entered by Day of Month \\n&quot;, &quot;Comparing Online to Sales Rep Sales\\n&quot;, min_soh_dt, &quot; - &quot;, max_soh_dt)) 8.8 These queries and graphs may be redundant mo_onl_pct &lt;- dbGetQuery( con, &quot; SELECT * ,round(orders/mo_orders * 100.0,2) mo_pct ,round(sales_dollars/mo_sales * 100.0,2) mo_dlr_pct FROM (SELECT EXTRACT(MONTH FROM orderdate) mo, EXTRACT(YEAR FROM orderdate) yr , min(orderdate)::DATE min_soh_orderdate, max(orderdate)::DATE max_soh_orderdate , round(sum(subtotal), 2) sales_dollars , round(sum(sum(subtotal)) over (partition by EXTRACT(MONTH FROM orderdate),EXTRACT(YEAR FROM orderdate) order by EXTRACT(YEAR FROM orderdate)),2) mo_sales , count(*) * 1.0 orders , sum(count(*)) over (partition by EXTRACT(MONTH FROM orderdate),EXTRACT(YEAR FROM orderdate) order by EXTRACT(YEAR FROM orderdate)) mo_orders , case when sh.onlineorderflag then &#39;online&#39; else &#39;sales rep&#39; end sales_type FROM sales.salesorderheader sh GROUP BY EXTRACT(MONTH FROM orderdate), EXTRACT(YEAR FROM orderdate), case when sh.onlineorderflag then &#39;online&#39; else &#39;sales rep&#39; end ) as src ORDER BY mo, yr, sales_type &quot; ) sp_print_df(head(mo_onl_pct)) mo_onl_pct$mo &lt;- as.factor(mo_onl_pct$mo) mo_onl_pct$yr &lt;- as.factor(mo_onl_pct$yr) mo_onl_pct$sales_type &lt;- as.factor(mo_onl_pct$sales_type) mo_2011 &lt;- mo_onl_pct %&gt;% filter(yr == 2011) mo_2012 &lt;- mo_onl_pct %&gt;% filter(yr == 2012) mo_2013 &lt;- mo_onl_pct %&gt;% filter(yr == 2013) mo_2014 &lt;- mo_onl_pct %&gt;% filter(yr == 2014) ggplot(data = NULL) + # data=mo_2011 first results in the x axis months out of order. geom_line(data = mo_2012, aes(x = mo, y = mo_pct, color = yr, group = sales_type)) + geom_line(data = mo_2011, aes(x = mo, y = mo_pct, color = yr, group = sales_type)) + geom_line(data = mo_2013, aes(x = mo, y = mo_pct, color = yr, group = sales_type)) + geom_line(data = mo_2014, aes(x = mo, y = mo_pct, color = yr, group = sales_type)) + geom_point(data = mo_2011, aes(x = mo, y = mo_pct, color = sales_type)) + geom_point(data = mo_2012, aes(x = mo, y = mo_pct, color = sales_type)) + geom_point(data = mo_2013, aes(x = mo, y = mo_pct, color = sales_type)) + geom_point(data = mo_2014, aes(x = mo, y = mo_pct, color = sales_type)) + geom_text( data = mo_2011, aes(x = mo, y = mo_pct, label = paste(orders, &quot;:$&quot;, sales_dollars, &quot;:&quot;, mo_dlr_pct, &quot;%&quot;)), position = position_dodge(.3), size = 2.5, hjust = 0 ) + geom_text( data = mo_2012, aes(x = mo, y = mo_pct, label = paste(orders, &quot;:$&quot;, sales_dollars, &quot;:&quot;, mo_dlr_pct, &quot;%&quot;)), position = position_dodge(.3), size = 2.5, hjust = 0 ) + geom_text( data = mo_2013, aes(x = mo, y = mo_pct, label = paste(orders, &quot;:$&quot;, sales_dollars, &quot;:&quot;, mo_dlr_pct, &quot;%&quot;)), position = position_dodge(.3), size = 2.5, hjust = 0 ) + geom_text( data = mo_2014, aes(x = mo, y = mo_pct, label = paste(orders, &quot;:$&quot;, sales_dollars, &quot;:&quot;, mo_dlr_pct, &quot;%&quot;)), color = &quot;lightblue&quot;, position = position_dodge(.3), size = 2.5, hjust = 0, vjust = 1.5 ) + xlab(&quot;Month&quot;) + ylab(&quot;% Online Sales\\nvs\\n%Rep Sales&quot;) + theme(plot.title = element_text(hjust = .50)) + ggtitle(paste( &quot;Sales by Month\\n&quot;, &quot;Online Orders Versus Rep Orders\\n&quot;, min_soh_dt, &quot; - &quot;, max_soh_dt, &quot; &quot;, &quot;Each Point shows Number of Orders: $ Amount: % of Total $ For the Month&quot; )) The sales rep orders brought in over half the monthly sales dollars for every month except three, February, April, and June of 2014. The monthly sales rep orders for those months are 3, 2, and 0 respectively. 8.9 Monthly Sales Rep Performance Analysis mo_soh_sreps &lt;- dbGetQuery( con, &quot; SELECT * ,round(orders/mo_orders * 100.0,2) mo_pct ,round(sales_dollars/mo_sales * 100.0,2) mo_dlr_pct FROM (SELECT EXTRACT(MONTH FROM orderdate) mo, EXTRACT(YEAR FROM orderdate) yr , min(orderdate)::DATE min_soh_orderdate, max(orderdate)::DATE max_soh_orderdate , round(sum(subtotal), 2) sales_dollars , round(sum(sum(subtotal)) over (partition by EXTRACT(MONTH FROM orderdate),EXTRACT(YEAR FROM orderdate) order by EXTRACT(YEAR FROM orderdate)),2) mo_sales , count(*) * 1.0 orders , sum(count(*)) over (partition by EXTRACT(MONTH FROM orderdate),EXTRACT(YEAR FROM orderdate) order by EXTRACT(YEAR FROM orderdate)) mo_orders , case when sh.onlineorderflag then &#39;online&#39; else &#39;sales rep&#39; end sales_type FROM sales.salesorderheader sh INNER JOIN sales.salesorderdetail sd ON sh.salesorderid = sd.salesorderid WHERE not sh.onlineorderflag GROUP BY EXTRACT(MONTH FROM orderdate), EXTRACT(YEAR FROM orderdate), case when sh.onlineorderflag then &#39;online&#39; else &#39;sales rep&#39; end ) as src ORDER BY mo, yr, sales_type &quot; ) sp_print_df(head(mo_soh_sreps)) monthly_sales_online &lt;- dbGetQuery( con, &quot; SELECT EXTRACT(MONTH FROM orderdate) mo, EXTRACT(YEAR FROM orderdate) yr , min(orderdate)::DATE min_soh_orderdate, max(orderdate)::DATE max_soh_orderdate , so.category , round(sum(subtotal), 2) sales_dollars , count(*) * 1.0 orders FROM sales.salesorderheader sh JOIN sales.salesorderdetail sd ON SH.salesorderid = sd.salesorderid JOIN sales.specialoffer so ON Sd.specialofferid = so.specialofferid GROUP BY EXTRACT(MONTH FROM orderdate), EXTRACT(YEAR FROM orderdate), so.category ORDER BY mo, yr &quot; ) sp_print_df(head(monthly_sales_online)) Figure 8.2: caption goes here ggplot(data = monthly_sales_online, aes(x = factor(mo), y = sales_dollars, fill = factor(yr))) + geom_col(position = &quot;dodge&quot;, color = &quot;black&quot;) + # unstack columns and outline in black xlab(&quot;Month&quot;) + ylab(&quot;Sales Dollars&quot;) + scale_y_continuous(labels = dollar) + geom_text(aes(label = category), size = 2.5 # ,color = &#39;black&#39; , vjust = 1.5, position = position_dodge(.9) ) + # orders =&gt; avg so $ amt theme(plot.title = element_text(hjust = .50)) + # Center ggplot title ggtitle(paste(&quot;Sales by Month\\nBy Online Flag&quot;)) Figure 8.2: caption goes here 8.9.1 Effect of Late Entries on Sales Rep data Correcting for the first of the month makes the Sales Rep data look more normal. That could be the end of this chapter. monthly_sales_onl_pct &lt;- dbGetQuery( con, &quot; select EXTRACT(MONTH FROM orderdate) mo ,EXTRACT(YEAR FROM orderdate) yr ,sum(ORDERQTY) ,sum(case when salespersonid is null and onlineorderflag then 1 else 0 end) onl ,sum(case when salespersonid is not null and not onlineorderflag then 1 else 0 end) sp ,round(sum(case when onlineorderflag then 1 else 0 end )*1.0/count(*) * 100.0,2) onl_pct ,round(sum(case when not onlineorderflag then 1 else 0 end )*1.0/count(*) * 100.0,2) sp_pct ,onlineorderflag ,count(*) FROM sales.salesorderheader sh INNER JOIN sales.salesorderdetail sd ON sh.salesorderid = sd.salesorderid INNER JOIN production.product p ON sd.productid = p.productid INNER JOIN sales.specialoffer so ON sd.specialofferid = so.specialofferid LEFT OUTER JOIN sales.specialofferproduct sop ON sd.specialofferid = sop.specialofferid and sd.productid = sop.productid WHERE sop.productid is not null group by EXTRACT(MONTH FROM orderdate) ,EXTRACT(YEAR FROM orderdate) ,onlineorderflag order by mo,yr &quot; ) sp_print_df(head(monthly_sales_onl_pct)) mo_onl_pct &lt;- dbGetQuery( con, &quot; SELECT * ,round(orders/mo_orders * 100.0,2) mo_pct ,round(sales_dollars/mo_sales * 100.0,2) mo_dlr_pct FROM (SELECT EXTRACT(MONTH FROM orderdate) mo, EXTRACT(YEAR FROM orderdate) yr , min(orderdate)::DATE min_soh_orderdate, max(orderdate)::DATE max_soh_orderdate , round(sum(subtotal), 2) sales_dollars , round(sum(sum(subtotal)) over (partition by EXTRACT(MONTH FROM orderdate),EXTRACT(YEAR FROM orderdate) order by EXTRACT(YEAR FROM orderdate)),2) mo_sales , count(*) * 1.0 orders , sum(count(*)) over (partition by EXTRACT(MONTH FROM orderdate),EXTRACT(YEAR FROM orderdate) order by EXTRACT(YEAR FROM orderdate)) mo_orders , case when sh.onlineorderflag then &#39;online&#39; else &#39;sales rep&#39; end sales_type FROM sales.salesorderheader sh GROUP BY EXTRACT(MONTH FROM orderdate), EXTRACT(YEAR FROM orderdate), case when sh.onlineorderflag then &#39;online&#39; else &#39;sales rep&#39; end ) as src ORDER BY mo, yr, sales_type &quot; ) sp_print_df(head(mo_onl_pct)) mo_onl_pct$mo &lt;- as.factor(mo_onl_pct$mo) mo_onl_pct$yr &lt;- as.factor(mo_onl_pct$yr) mo_onl_pct$sales_type &lt;- as.factor(mo_onl_pct$sales_type) mo_2011 &lt;- mo_onl_pct %&gt;% filter(yr == 2011) mo_2012 &lt;- mo_onl_pct %&gt;% filter(yr == 2012) mo_2013 &lt;- mo_onl_pct %&gt;% filter(yr == 2013) mo_2014 &lt;- mo_onl_pct %&gt;% filter(yr == 2014) ggplot(data = NULL) + # data=mo_2011 first results in the x axis months out of order. geom_line(data = mo_2012, aes(x = mo, y = mo_pct, color = yr, group = sales_type)) + geom_line(data = mo_2011, aes(x = mo, y = mo_pct, color = yr, group = sales_type)) + geom_line(data = mo_2013, aes(x = mo, y = mo_pct, color = yr, group = sales_type)) + geom_line(data = mo_2014, aes(x = mo, y = mo_pct, color = yr, group = sales_type)) + geom_point(data = mo_2011, aes(x = mo, y = mo_pct, color = sales_type)) + geom_point(data = mo_2012, aes(x = mo, y = mo_pct, color = sales_type)) + geom_point(data = mo_2013, aes(x = mo, y = mo_pct, color = sales_type)) + geom_point(data = mo_2014, aes(x = mo, y = mo_pct, color = sales_type)) + geom_text( data = mo_2011, aes(x = mo, y = mo_pct, label = paste(orders, &quot;:$&quot;, sales_dollars, &quot;:&quot;, mo_dlr_pct, &quot;%&quot;)), position = position_dodge(.3), size = 2.25, hjust = 1.0 ) + geom_text( data = mo_2012, aes(x = mo, y = mo_pct, label = paste(orders, &quot;:$&quot;, sales_dollars, &quot;:&quot;, mo_dlr_pct, &quot;%&quot;)), position = position_dodge(.3), size = 2.25, hjust = 1.0 ) + geom_text( data = mo_2013, aes(x = mo, y = mo_pct, label = paste(orders, &quot;:$&quot;, sales_dollars, &quot;:&quot;, mo_dlr_pct, &quot;%&quot;)), position = position_dodge(.3), size = 2.25, hjust = 1.0 ) + geom_text( data = mo_2014, aes(x = mo, y = mo_pct, label = paste(orders, &quot;:$&quot;, sales_dollars, &quot;:&quot;, mo_dlr_pct, &quot;%&quot;)), color = &quot;lightblue&quot;, position = position_dodge(.3), size = 2.25, hjust = 1.0, vjust = 1.5 ) + xlab(&quot;Month&quot;) + ylab(&quot;% Online Sales\\nvs\\n%Rep Sales&quot;) + theme(plot.title = element_text(hjust = .50)) + ggtitle(paste( &quot;Sales by Month\\n&quot;, &quot;Online Orders Versus Rep Orders\\n&quot;, min_soh_dt, &quot; - &quot;, max_soh_dt, &quot; &quot;, &quot;Each Point shows Number of Orders: $ Amount: % of Total $ For the Month&quot; )) This plot is much easier to read, but the sales orders =&gt; avg_s From the tidy point of view overview, https://tidyr.tidyverse.org/: Tidy data is data where: Each variable is in a column. Each observation is a row. Each value is a cell. The gather command throws the following warning: attributes are not identical across measure variables; they will be dropped 8.10 Close and clean up dbDisconnect(con) sp_docker_stop(&quot;adventureworks&quot;) "],
["chapter-sales-forecasting.html", "Chapter 9 Sales Forecasting 9.1 Setup 9.2 Exploring the sales data 9.3 Forecasting", " Chapter 9 Sales Forecasting This chapter demonstrates how to: Use tidyverts packages to explore and forecast sales data. 9.1 Setup The following packages are used in this chapter: library(tidyverse) library(DBI) library(RPostgres) require(knitr) library(bookdown) library(sqlpetr) library(tsibble) library(fable) Analyzing sales time series, in particular determining seasonality and forecasting future sales, is a common activty in business management. A collection of packages called tidyverst is designed to do this in a tidy data framework. First, we make sure the Docker container is ready and connect to the adventureworks database. sqlpetr::sp_docker_start(&quot;adventureworks&quot;) con &lt;- sqlpetr::sp_get_postgres_connection( user = Sys.getenv(&quot;DEFAULT_POSTGRES_USER_NAME&quot;), password = Sys.getenv(&quot;DEFAULT_POSTGRES_PASSWORD&quot;), dbname = &quot;adventureworks&quot;, port = 5432, seconds_to_test = 20, connection_tab = FALSE ) Next, we retrieve the “sales order header” table from the database, close the connection and stop the container. dbExecute(con, &quot;set search_path to sales;&quot;) ## [1] 0 salesorderheader_tibble &lt;- DBI::dbReadTable(con, &quot;salesorderheader&quot;) DBI::dbDisconnect(con) sqlpetr::sp_docker_stop(&quot;adventureworks&quot;) 9.2 Exploring the sales data Some assumptions: 1. The business requirement is to analyze / forecast revenue. 2. The revenue figures we care about are those in the subtotal column. The shipping and tax numbers are costs, not revenue. 3. The values in column subtotal have been converted to the common currency of the Adventureworks headquarters. In a real-world setting, the analyst would need to validate these assumptions. Given that, our first task is to create a tsibble - a time series tibble - of monthly revenue figures. 9.2.1 Creating a tsibble monthly_tsibble &lt;- salesorderheader_tibble %&gt;% dplyr::mutate( origin = ifelse(onlineorderflag, &quot;online&quot;, &quot;sales_rep&quot;), month_start = lubridate::floor_date(shipdate, unit = &quot;months&quot;) ) %&gt;% dplyr::group_by(origin, month_start) %&gt;% dplyr::summarize(orders = n(), total_revenue = sum(subtotal)) %&gt;% dplyr::ungroup() %&gt;% tsibble::as_tsibble(key = origin, index = month_start) Note that there are two more months - June and July of 2014 - for the online data, and the revenue values are suspiciously low. We remove them for consistency. monthly_tsibble &lt;- monthly_tsibble %&gt;% dplyr::filter(month_start &lt; &#39;2014-06-01&#39;) 9.2.2 Exploring the data First, let’s look at orders for online and sales representative sales: monthly_tsibble %&gt;% autoplot(orders) Wow! Online orders really took off in the late spring - early summer of 2013! How about revenues? monthly_tsibble %&gt;% autoplot(total_revenue) There’s an increase, but the sales representatives always brought in more revenue than the online platform. And there’s a pronounced variation in the revenue from sales representatives on a month-to-month basis. Before moving on, let’s look at revenue per order. monthly_tsibble %&gt;% autoplot(total_revenue / orders) For the sales representatives, there’s still a month-to-month variation but the revenue per order appears to be bounded both below and above. However, the online revenue per order is decreasing. 9.3 Forecasting "],
["chapter-lazy-evaluation-queries.html", "Chapter 10 Lazy Evaluation and Lazy Queries 10.1 Setup 10.2 R is lazy and comes with guardrails 10.3 Lazy evaluation and lazy queries 10.3.3 Source: lazy query [?? x 4] 10.3.3 Database: postgres 10.3.3 [postgres@localhost:5432/adventureworks] 10.4 Other resources", " Chapter 10 Lazy Evaluation and Lazy Queries This chapter: Reviews lazy loading, lazy evaluation and lazy query execution Demonstrates how dplyr code gets executed (and how R determines what is translated to SQL and what is processed locally by R) Offers some further resources on lazy loading, evaluation, execution, etc. 10.1 Setup The following packages are used in this chapter: library(tidyverse) library(DBI) library(RPostgres) library(dbplyr) require(knitr) library(bookdown) library(sqlpetr) Start your adventureworks container: sqlpetr::sp_docker_start(&quot;adventureworks&quot;) Connect to the database: con &lt;- sqlpetr::sp_get_postgres_connection( user = Sys.getenv(&quot;DEFAULT_POSTGRES_USER_NAME&quot;), password = Sys.getenv(&quot;DEFAULT_POSTGRES_PASSWORD&quot;), dbname = &quot;adventureworks&quot;, port = 5432, seconds_to_test = 20, connection_tab = TRUE ) 10.2 R is lazy and comes with guardrails By design, R is both a language and an interactive development environment (IDE). As a language, R tries to be as efficient as possible. As an IDE, R creates some guardrails to make it easy and safe to work with your data. For example getOption(\"max.print\") prevents R from printing more rows of data than you want to handle in an interactive session, with a default of 99999 lines, which may or may not suit you. On the other hand SQL is a “Structured Query Language (SQL): a standard computer language for relational database management and data manipulation.”.1 SQL has various database-specific Interactive Development Environments (IDEs), such as pgAdmin for PostgreSQL. Roger Peng explains in R Programming for Data Science that: R has maintained the original S philosophy, which is that it provides a language that is both useful for interactive work, but contains a powerful programming language for developing new tools. This is complicated when R interacts with SQL. In a vignette for dbplyr Hadley Wickham explains: The most important difference between ordinary data frames and remote database queries is that your R code is translated into SQL and executed in the database on the remote server, not in R on your local machine. When working with databases, dplyr tries to be as lazy as possible: It never pulls data into R unless you explicitly ask for it. It delays doing any work until the last possible moment: it collects together everything you want to do and then sends it to the database in one step. Exactly when, which, and how much data is returned from the dbms is the topic of this chapter. Exactly how the data is represented in the dbms and then translated to a data frame is discussed in the DBI specification. Eventually, if you are interacting with a dbms from R you will need to understand the differences between lazy loading, lazy evaluation, and lazy queries. 10.2.1 Lazy loading “Lazy loading is always used for code in packages but is optional (selected by the package maintainer) for datasets in packages.”2 Lazy loading means that the code for a particular function doesn’t actually get loaded into memory until the last minute – when it’s actually being used. 10.2.2 Lazy evaluation Essentially “Lazy evaluation is a programming strategy that allows a symbol to be evaluated only when needed.”3 That means that lazy evaluation is about symbols such as function arguments4 when they are evaluated. Tidy evaluation complicates lazy evaluation.5 10.2.3 Lazy Queries “When you create a \"lazy\" query, you’re creating a pointer to a set of conditions on the database, but the query isn’t actually run and the data isn’t actually loaded until you call \"next\" or some similar method to actually fetch the data and load it into an object.”6 10.3 Lazy evaluation and lazy queries When does a lazy query trigger data retrieval? It depends on a lot of factors, as we explore below: 10.3.1 Create a black box query for experimentation Define the three tables discussed in the previous chapter to build a black box query: sales_person_table &lt;- tbl(con, in_schema(&quot;sales&quot;, &quot;salesperson&quot;)) %&gt;% select(-rowguid) %&gt;% rename(sale_info_updated = modifieddate) employee_table &lt;- tbl(con, in_schema(&quot;humanresources&quot;, &quot;employee&quot;)) %&gt;% select(-modifieddate, -rowguid) person_table &lt;- tbl(con, in_schema(&quot;person&quot;, &quot;person&quot;)) %&gt;% select(-modifieddate, -rowguid) Here is a typical string of dplyr verbs strung together with the magrittr %&gt;% pipe command that will be used to tease out the several different behaviors that a lazy query has when passed to different R functions. This query joins three connection objects into a query we’ll call Q: Q &lt;- sales_person_table %&gt;% dplyr::left_join(employee_table, by = c(&quot;businessentityid&quot; = &quot;businessentityid&quot;)) %&gt;% dplyr::left_join(person_table , by = c(&quot;businessentityid&quot; = &quot;businessentityid&quot;)) %&gt;% dplyr::select(firstname, lastname, salesytd, birthdate) The str function gives us a hint at how R is collecting information that can be used to construct and execute a query later on: str(Q, max.level = 2) ## List of 2 ## $ src:List of 2 ## ..$ con :Formal class &#39;PqConnection&#39; [package &quot;RPostgres&quot;] with 3 slots ## ..$ disco: NULL ## ..- attr(*, &quot;class&quot;)= chr [1:4] &quot;src_PqConnection&quot; &quot;src_dbi&quot; &quot;src_sql&quot; &quot;src&quot; ## $ ops:List of 4 ## ..$ name: chr &quot;select&quot; ## ..$ x :List of 4 ## .. ..- attr(*, &quot;class&quot;)= chr [1:3] &quot;op_join&quot; &quot;op_double&quot; &quot;op&quot; ## ..$ dots: list() ## ..$ args:List of 1 ## ..- attr(*, &quot;class&quot;)= chr [1:3] &quot;op_select&quot; &quot;op_single&quot; &quot;op&quot; ## - attr(*, &quot;class&quot;)= chr [1:5] &quot;tbl_PqConnection&quot; &quot;tbl_dbi&quot; &quot;tbl_sql&quot; &quot;tbl_lazy&quot; ... 10.3.2 Experiment overview Think of Q as a black box for the moment. The following examples will show how Q is interpreted differently by different functions. It’s important to remember in the following discussion that the “and then” operator (%&gt;%) actually wraps the subsequent code inside the preceding code so that Q %&gt;% print() is equivalent to print(Q). Notation A single green check indicates that some rows are returned. Two green checks indicate that all the rows are returned. The red X indicates that no rows are returned. R code Result Q %&gt;% print() Prints x rows; same as just entering Q Q %&gt;% dplyr::as_tibble() Forces Q to be a tibble Q %&gt;% head() Prints the first 6 rows Q %&gt;% tail() Error: tail() is not supported by sql sources Q %&gt;% length() Counts the rows in Q Q %&gt;% str() Shows the top 3 levels of the object Q Q %&gt;% nrow() Attempts to determine the number of rows Q %&gt;% dplyr::tally() Counts all the rows – on the dbms side Q %&gt;% dplyr::collect(n = 20) Prints 20 rows Q %&gt;% dplyr::collect(n = 20) %&gt;% head() Prints 6 rows Q %&gt;% ggplot Plots a barchart Q %&gt;% dplyr::show_query() Translates the lazy query object into SQL The next chapter will discuss how to build queries and how to explore intermediate steps. But first, the following subsections provide a more detailed discussion of each row in the preceding table. 10.3.3 Q %&gt;% print() Remember that Q %&gt;% print() is equivalent to print(Q) and the same as just entering Q on the command line. We use the magrittr pipe operator here, because chaining functions highlights how the same object behaves differently in each use. Q %&gt;% print() ## # Source: lazy query [?? x 4] ## # Database: postgres [postgres@localhost:5432/adventureworks] ## firstname lastname salesytd birthdate ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;date&gt; ## 1 Stephen Jiang 559698. 1951-10-17 ## 2 Michael Blythe 3763178. 1968-12-25 ## 3 Linda Mitchell 4251369. 1980-02-27 ## 4 Jillian Carson 3189418. 1962-08-29 ## 5 Garrett Vargas 1453719. 1975-02-04 ## 6 Tsvi Reiter 2315186. 1974-01-18 ## 7 Pamela Ansman-Wolfe 1352577. 1974-12-06 ## 8 Shu Ito 2458536. 1968-03-09 ## 9 José Saraiva 2604541. 1963-12-11 ## 10 David Campbell 1573013. 1974-02-11 ## # … with more rows R retrieves 10 observations and 3 columns. In its role as IDE, R has provided nicely formatted output that is similar to what it prints for a tibble, with descriptive information about the dataset and each column: 10.3.3 Source: lazy query [?? x 4] 10.3.3 Database: postgres 10.3.3 [postgres@localhost:5432/adventureworks] firstname lastname salesytd birthdate R has not determined how many rows are left to retrieve as it shows with [?? x 4] and ... with more rows in the data summary. 10.3.4 Q %&gt;% dplyr::as_tibble() In contrast to print(), the as_tibble() function causes R to download the whole table, using tibble’s default of displaying only the first 10 rows. Q %&gt;% dplyr::as_tibble() ## # A tibble: 17 x 4 ## firstname lastname salesytd birthdate ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;date&gt; ## 1 Stephen Jiang 559698. 1951-10-17 ## 2 Michael Blythe 3763178. 1968-12-25 ## 3 Linda Mitchell 4251369. 1980-02-27 ## 4 Jillian Carson 3189418. 1962-08-29 ## 5 Garrett Vargas 1453719. 1975-02-04 ## 6 Tsvi Reiter 2315186. 1974-01-18 ## 7 Pamela Ansman-Wolfe 1352577. 1974-12-06 ## 8 Shu Ito 2458536. 1968-03-09 ## 9 José Saraiva 2604541. 1963-12-11 ## 10 David Campbell 1573013. 1974-02-11 ## 11 Tete Mensa-Annan 1576562. 1978-01-05 ## 12 Syed Abbas 172524. 1975-01-11 ## 13 Lynn Tsoflias 1421811. 1977-02-14 ## 14 Amy Alberts 519906. 1957-09-20 ## 15 Rachel Valdez 1827067. 1975-07-09 ## 16 Jae Pak 4116871. 1968-03-17 ## 17 Ranjit Varkey Chudukatil 3121616. 1975-09-30 10.3.5 Q %&gt;% head() The head() function is very similar to print but has a different “max.print” value. Q %&gt;% head() ## # Source: lazy query [?? x 4] ## # Database: postgres [postgres@localhost:5432/adventureworks] ## firstname lastname salesytd birthdate ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;date&gt; ## 1 Stephen Jiang 559698. 1951-10-17 ## 2 Michael Blythe 3763178. 1968-12-25 ## 3 Linda Mitchell 4251369. 1980-02-27 ## 4 Jillian Carson 3189418. 1962-08-29 ## 5 Garrett Vargas 1453719. 1975-02-04 ## 6 Tsvi Reiter 2315186. 1974-01-18 10.3.6 Q %&gt;% tail() Produces an error, because Q does not hold all of the data, so it is not possible to list the last few items from the table: try( Q %&gt;% tail(), silent = FALSE, outFile = stdout() ) ## Error : tail() is not supported by sql sources 10.3.7 Q %&gt;% length() Because the Q object is relatively complex, using str() on it prints many lines. You can glimpse what’s going on with length(): Q %&gt;% length() ## [1] 2 10.3.8 Q %&gt;% str() Looking inside shows some of what’s going on (three levels deep): Q %&gt;% str(max.level = 3) ## List of 2 ## $ src:List of 2 ## ..$ con :Formal class &#39;PqConnection&#39; [package &quot;RPostgres&quot;] with 3 slots ## ..$ disco: NULL ## ..- attr(*, &quot;class&quot;)= chr [1:4] &quot;src_PqConnection&quot; &quot;src_dbi&quot; &quot;src_sql&quot; &quot;src&quot; ## $ ops:List of 4 ## ..$ name: chr &quot;select&quot; ## ..$ x :List of 4 ## .. ..$ name: chr &quot;join&quot; ## .. ..$ x :List of 2 ## .. .. ..- attr(*, &quot;class&quot;)= chr [1:5] &quot;tbl_PqConnection&quot; &quot;tbl_dbi&quot; &quot;tbl_sql&quot; &quot;tbl_lazy&quot; ... ## .. ..$ y :List of 2 ## .. .. ..- attr(*, &quot;class&quot;)= chr [1:5] &quot;tbl_PqConnection&quot; &quot;tbl_dbi&quot; &quot;tbl_sql&quot; &quot;tbl_lazy&quot; ... ## .. ..$ args:List of 4 ## .. ..- attr(*, &quot;class&quot;)= chr [1:3] &quot;op_join&quot; &quot;op_double&quot; &quot;op&quot; ## ..$ dots: list() ## ..$ args:List of 1 ## .. ..$ vars:List of 4 ## ..- attr(*, &quot;class&quot;)= chr [1:3] &quot;op_select&quot; &quot;op_single&quot; &quot;op&quot; ## - attr(*, &quot;class&quot;)= chr [1:5] &quot;tbl_PqConnection&quot; &quot;tbl_dbi&quot; &quot;tbl_sql&quot; &quot;tbl_lazy&quot; ... 10.3.9 Q %&gt;% nrow() Notice the difference between nrow() and tally(). The nrow functions returns NA and does not execute a query: Q %&gt;% nrow() ## [1] NA 10.3.10 Q %&gt;% dplyr::tally() The tally function actually counts all the rows. Q %&gt;% dplyr::tally() ## # Source: lazy query [?? x 1] ## # Database: postgres [postgres@localhost:5432/adventureworks] ## n ## &lt;int64&gt; ## 1 17 The nrow() function knows that Q is a list. On the other hand, the tally() function tells SQL to go count all the rows. Notice that Q results in 1,000 rows – the same number of rows as film. 10.3.11 Q %&gt;% dplyr::collect() The dplyr::collect function triggers a call to the DBI:dbFetch() function behind the scenes, which forces R to download a specified number of rows: Q %&gt;% dplyr::collect(n = 20) ## # A tibble: 17 x 4 ## firstname lastname salesytd birthdate ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;date&gt; ## 1 Stephen Jiang 559698. 1951-10-17 ## 2 Michael Blythe 3763178. 1968-12-25 ## 3 Linda Mitchell 4251369. 1980-02-27 ## 4 Jillian Carson 3189418. 1962-08-29 ## 5 Garrett Vargas 1453719. 1975-02-04 ## 6 Tsvi Reiter 2315186. 1974-01-18 ## 7 Pamela Ansman-Wolfe 1352577. 1974-12-06 ## 8 Shu Ito 2458536. 1968-03-09 ## 9 José Saraiva 2604541. 1963-12-11 ## 10 David Campbell 1573013. 1974-02-11 ## 11 Tete Mensa-Annan 1576562. 1978-01-05 ## 12 Syed Abbas 172524. 1975-01-11 ## 13 Lynn Tsoflias 1421811. 1977-02-14 ## 14 Amy Alberts 519906. 1957-09-20 ## 15 Rachel Valdez 1827067. 1975-07-09 ## 16 Jae Pak 4116871. 1968-03-17 ## 17 Ranjit Varkey Chudukatil 3121616. 1975-09-30 Q %&gt;% dplyr::collect(n = 20) %&gt;% head() ## # A tibble: 6 x 4 ## firstname lastname salesytd birthdate ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;date&gt; ## 1 Stephen Jiang 559698. 1951-10-17 ## 2 Michael Blythe 3763178. 1968-12-25 ## 3 Linda Mitchell 4251369. 1980-02-27 ## 4 Jillian Carson 3189418. 1962-08-29 ## 5 Garrett Vargas 1453719. 1975-02-04 ## 6 Tsvi Reiter 2315186. 1974-01-18 The dplyr::collect function triggers the creation of a tibble and controls the number of rows that the DBMS sends to R. Notice that head only prints 6 of the 20 rows that R has retrieved. If you do not provide a value for the n argument, all of the rows will be retrieved into your R workspace. 10.3.12 Q %&gt;% ggplot Passing the Q object to ggplot executes the query and plots the result. Q %&gt;% ggplot2::ggplot(aes(birthdate, salesytd)) + geom_point() * Rewrite previous query and this comment with adventureworks in mind. Comment on the plot… 10.3.13 Q %&gt;% dplyr::show_query() Q %&gt;% dplyr::show_query() ## &lt;SQL&gt; ## SELECT &quot;firstname&quot;, &quot;lastname&quot;, &quot;salesytd&quot;, &quot;birthdate&quot; ## FROM (SELECT &quot;LHS&quot;.&quot;businessentityid&quot; AS &quot;businessentityid&quot;, &quot;LHS&quot;.&quot;territoryid&quot; AS &quot;territoryid&quot;, &quot;LHS&quot;.&quot;salesquota&quot; AS &quot;salesquota&quot;, &quot;LHS&quot;.&quot;bonus&quot; AS &quot;bonus&quot;, &quot;LHS&quot;.&quot;commissionpct&quot; AS &quot;commissionpct&quot;, &quot;LHS&quot;.&quot;salesytd&quot; AS &quot;salesytd&quot;, &quot;LHS&quot;.&quot;saleslastyear&quot; AS &quot;saleslastyear&quot;, &quot;LHS&quot;.&quot;sale_info_updated&quot; AS &quot;sale_info_updated&quot;, &quot;LHS&quot;.&quot;nationalidnumber&quot; AS &quot;nationalidnumber&quot;, &quot;LHS&quot;.&quot;loginid&quot; AS &quot;loginid&quot;, &quot;LHS&quot;.&quot;jobtitle&quot; AS &quot;jobtitle&quot;, &quot;LHS&quot;.&quot;birthdate&quot; AS &quot;birthdate&quot;, &quot;LHS&quot;.&quot;maritalstatus&quot; AS &quot;maritalstatus&quot;, &quot;LHS&quot;.&quot;gender&quot; AS &quot;gender&quot;, &quot;LHS&quot;.&quot;hiredate&quot; AS &quot;hiredate&quot;, &quot;LHS&quot;.&quot;salariedflag&quot; AS &quot;salariedflag&quot;, &quot;LHS&quot;.&quot;vacationhours&quot; AS &quot;vacationhours&quot;, &quot;LHS&quot;.&quot;sickleavehours&quot; AS &quot;sickleavehours&quot;, &quot;LHS&quot;.&quot;currentflag&quot; AS &quot;currentflag&quot;, &quot;LHS&quot;.&quot;organizationnode&quot; AS &quot;organizationnode&quot;, &quot;RHS&quot;.&quot;persontype&quot; AS &quot;persontype&quot;, &quot;RHS&quot;.&quot;namestyle&quot; AS &quot;namestyle&quot;, &quot;RHS&quot;.&quot;title&quot; AS &quot;title&quot;, &quot;RHS&quot;.&quot;firstname&quot; AS &quot;firstname&quot;, &quot;RHS&quot;.&quot;middlename&quot; AS &quot;middlename&quot;, &quot;RHS&quot;.&quot;lastname&quot; AS &quot;lastname&quot;, &quot;RHS&quot;.&quot;suffix&quot; AS &quot;suffix&quot;, &quot;RHS&quot;.&quot;emailpromotion&quot; AS &quot;emailpromotion&quot;, &quot;RHS&quot;.&quot;additionalcontactinfo&quot; AS &quot;additionalcontactinfo&quot;, &quot;RHS&quot;.&quot;demographics&quot; AS &quot;demographics&quot; ## FROM (SELECT &quot;LHS&quot;.&quot;businessentityid&quot; AS &quot;businessentityid&quot;, &quot;LHS&quot;.&quot;territoryid&quot; AS &quot;territoryid&quot;, &quot;LHS&quot;.&quot;salesquota&quot; AS &quot;salesquota&quot;, &quot;LHS&quot;.&quot;bonus&quot; AS &quot;bonus&quot;, &quot;LHS&quot;.&quot;commissionpct&quot; AS &quot;commissionpct&quot;, &quot;LHS&quot;.&quot;salesytd&quot; AS &quot;salesytd&quot;, &quot;LHS&quot;.&quot;saleslastyear&quot; AS &quot;saleslastyear&quot;, &quot;LHS&quot;.&quot;sale_info_updated&quot; AS &quot;sale_info_updated&quot;, &quot;RHS&quot;.&quot;nationalidnumber&quot; AS &quot;nationalidnumber&quot;, &quot;RHS&quot;.&quot;loginid&quot; AS &quot;loginid&quot;, &quot;RHS&quot;.&quot;jobtitle&quot; AS &quot;jobtitle&quot;, &quot;RHS&quot;.&quot;birthdate&quot; AS &quot;birthdate&quot;, &quot;RHS&quot;.&quot;maritalstatus&quot; AS &quot;maritalstatus&quot;, &quot;RHS&quot;.&quot;gender&quot; AS &quot;gender&quot;, &quot;RHS&quot;.&quot;hiredate&quot; AS &quot;hiredate&quot;, &quot;RHS&quot;.&quot;salariedflag&quot; AS &quot;salariedflag&quot;, &quot;RHS&quot;.&quot;vacationhours&quot; AS &quot;vacationhours&quot;, &quot;RHS&quot;.&quot;sickleavehours&quot; AS &quot;sickleavehours&quot;, &quot;RHS&quot;.&quot;currentflag&quot; AS &quot;currentflag&quot;, &quot;RHS&quot;.&quot;organizationnode&quot; AS &quot;organizationnode&quot; ## FROM (SELECT &quot;businessentityid&quot;, &quot;territoryid&quot;, &quot;salesquota&quot;, &quot;bonus&quot;, &quot;commissionpct&quot;, &quot;salesytd&quot;, &quot;saleslastyear&quot;, &quot;modifieddate&quot; AS &quot;sale_info_updated&quot; ## FROM sales.salesperson) &quot;LHS&quot; ## LEFT JOIN (SELECT &quot;businessentityid&quot;, &quot;nationalidnumber&quot;, &quot;loginid&quot;, &quot;jobtitle&quot;, &quot;birthdate&quot;, &quot;maritalstatus&quot;, &quot;gender&quot;, &quot;hiredate&quot;, &quot;salariedflag&quot;, &quot;vacationhours&quot;, &quot;sickleavehours&quot;, &quot;currentflag&quot;, &quot;organizationnode&quot; ## FROM humanresources.employee) &quot;RHS&quot; ## ON (&quot;LHS&quot;.&quot;businessentityid&quot; = &quot;RHS&quot;.&quot;businessentityid&quot;) ## ) &quot;LHS&quot; ## LEFT JOIN (SELECT &quot;businessentityid&quot;, &quot;persontype&quot;, &quot;namestyle&quot;, &quot;title&quot;, &quot;firstname&quot;, &quot;middlename&quot;, &quot;lastname&quot;, &quot;suffix&quot;, &quot;emailpromotion&quot;, &quot;additionalcontactinfo&quot;, &quot;demographics&quot; ## FROM person.person) &quot;RHS&quot; ## ON (&quot;LHS&quot;.&quot;businessentityid&quot; = &quot;RHS&quot;.&quot;businessentityid&quot;) ## ) &quot;dbplyr_009&quot; Hand-written SQL code to do the same job will probably look a lot nicer and could be more efficient, but functionally dplyr does the job. DBI::dbDisconnect(con) sqlpetr::sp_docker_stop(&quot;adventureworks&quot;) 10.4 Other resources Benjamin S. Baumer. 2017. A Grammar for Reproducible and Painless Extract-Transform-Load Operations on Medium Data. https://arxiv.org/abs/1708.07073 dplyr Reference documentation: Remote tables. https://dplyr.tidyverse.org/reference/index.html#section-remote-tables Data Carpentry. SQL Databases and R. https://datacarpentry.org/R-ecology-lesson/05-r-and-databases.html "],
["chapter-lazy-evaluation-and-timing.html", "Chapter 11 Lazy Evaluation and Execution Environment 11.1 Setup 11.2 Other resources", " Chapter 11 Lazy Evaluation and Execution Environment This chapter: Builds on the lazy loading discussion in the previous chapter Demonstrates how the use of the dplyr::collect() creates a boundary between code that is sent to a dbms and code that is executed locally 11.1 Setup The following packages are used in this chapter: library(tidyverse) library(DBI) library(RPostgres) library(dbplyr) require(knitr) library(bookdown) library(sqlpetr) If you have not yet set up the Docker container with PostgreSQL and the dvdrental database, go back to [those instructions][Build the pet-sql Docker Image] to configure your environment. Otherwise, start your adventureworks container: sqlpetr::sp_docker_start(&quot;adventureworks&quot;) Connect to the database: con &lt;- sqlpetr::sp_get_postgres_connection( user = Sys.getenv(&quot;DEFAULT_POSTGRES_USER_NAME&quot;), password = Sys.getenv(&quot;DEFAULT_POSTGRES_PASSWORD&quot;), dbname = &quot;adventureworks&quot;, port = 5432, seconds_to_test = 20, connection_tab = TRUE ) Here is a simple string of dplyr verbs similar to the query used to illustrate issues in the last chapter: Note that in the previous example we follow this book’s convention of creating a connection object to each table and fully qualifying function names (e.g., specifying the package). In practice, it’s possible and convenient to use more abbreviated notation. Q &lt;- tbl(con, in_schema(&quot;sales&quot;, &quot;salesperson&quot;)) %&gt;% left_join(tbl(con, in_schema(&quot;humanresources&quot;, &quot;employee&quot;)), by = c(&quot;businessentityid&quot; = &quot;businessentityid&quot;)) %&gt;% select(birthdate, saleslastyear) Q ## # Source: lazy query [?? x 2] ## # Database: postgres [postgres@localhost:5432/adventureworks] ## birthdate saleslastyear ## &lt;date&gt; &lt;dbl&gt; ## 1 1951-10-17 0 ## 2 1968-12-25 1750406. ## 3 1980-02-27 1439156. ## 4 1962-08-29 1997186. ## 5 1975-02-04 1620277. ## 6 1974-01-18 1849641. ## 7 1974-12-06 1927059. ## 8 1968-03-09 2073506. ## 9 1963-12-11 2038235. ## 10 1974-02-11 1371635. ## # … with more rows 11.1.1 Experiment overview Think of Q as a black box for the moment. The following examples will show how Q is interpreted differently by different functions. It’s important to remember in the following discussion that the “and then” operator (%&gt;%) actually wraps the subsequent code inside the preceding code so that Q %&gt;% print() is equivalent to print(Q). Notation Symbol Explanation A single green check indicates that some rows are returned. Two green checks indicate that all the rows are returned. The red X indicates that no rows are returned. R code Result Time-based, execution environment issues Qc &lt;- Q %&gt;% count(saleslastyear, sort = TRUE) Extends the lazy query object The next chapter will discuss how to build queries and how to explore intermediate steps. But first, the following subsections provide a more detailed discussion of each row in the preceding table. 11.1.2 Time-based, execution environment issues Remember that if the expression is assigned to an object, it is not executed. If an expression is entered on the command line or appears in your script by itself, a print() function is implied. These two are different: Q %&gt;% sum(saleslastyear) Q_query &lt;- Q %&gt;% sum(saleslastyear) This behavior is the basis of a useful debugging and development process where queries are built up incrementally. 11.1.3 Q %&gt;% more dplyr Because the following statement implies a print() function at the end, we can run it repeatedly, adding dplyr expressions, and only get 10 rows back. Every time we add a dplyr expression to a chain, R will rewrite the SQL code. For example: As we understand more about the data, we simply add dplyr expressions to pinpoint what we are looking for: Q %&gt;% filter(saleslastyear &gt; 40) %&gt;% arrange(desc(saleslastyear)) ## # Source: lazy query [?? x 2] ## # Database: postgres [postgres@localhost:5432/adventureworks] ## # Ordered by: desc(saleslastyear) ## birthdate saleslastyear ## &lt;date&gt; &lt;dbl&gt; ## 1 1975-09-30 2396540. ## 2 1977-02-14 2278549. ## 3 1968-03-09 2073506. ## 4 1963-12-11 2038235. ## 5 1962-08-29 1997186. ## 6 1974-12-06 1927059. ## 7 1974-01-18 1849641. ## 8 1968-12-25 1750406. ## 9 1968-03-17 1635823. ## 10 1975-02-04 1620277. ## # … with more rows Q %&gt;% summarize(total_sales = sum(saleslastyear, na.rm = TRUE), sales_persons_count = n()) ## # Source: lazy query [?? x 2] ## # Database: postgres [postgres@localhost:5432/adventureworks] ## total_sales sales_persons_count ## &lt;dbl&gt; &lt;int64&gt; ## 1 23685964. 17 When all the accumulated dplyr verbs are executed, they are submitted to the dbms and the number of rows that are returned follow the same rules as discussed above. ### Interspersing SQL and dplyr Q %&gt;% # mutate(birthdate = date(birthdate)) %&gt;% show_query() ## &lt;SQL&gt; ## SELECT &quot;birthdate&quot;, &quot;saleslastyear&quot; ## FROM (SELECT &quot;LHS&quot;.&quot;businessentityid&quot; AS &quot;businessentityid&quot;, &quot;LHS&quot;.&quot;territoryid&quot; AS &quot;territoryid&quot;, &quot;LHS&quot;.&quot;salesquota&quot; AS &quot;salesquota&quot;, &quot;LHS&quot;.&quot;bonus&quot; AS &quot;bonus&quot;, &quot;LHS&quot;.&quot;commissionpct&quot; AS &quot;commissionpct&quot;, &quot;LHS&quot;.&quot;salesytd&quot; AS &quot;salesytd&quot;, &quot;LHS&quot;.&quot;saleslastyear&quot; AS &quot;saleslastyear&quot;, &quot;LHS&quot;.&quot;rowguid&quot; AS &quot;rowguid.x&quot;, &quot;LHS&quot;.&quot;modifieddate&quot; AS &quot;modifieddate.x&quot;, &quot;RHS&quot;.&quot;nationalidnumber&quot; AS &quot;nationalidnumber&quot;, &quot;RHS&quot;.&quot;loginid&quot; AS &quot;loginid&quot;, &quot;RHS&quot;.&quot;jobtitle&quot; AS &quot;jobtitle&quot;, &quot;RHS&quot;.&quot;birthdate&quot; AS &quot;birthdate&quot;, &quot;RHS&quot;.&quot;maritalstatus&quot; AS &quot;maritalstatus&quot;, &quot;RHS&quot;.&quot;gender&quot; AS &quot;gender&quot;, &quot;RHS&quot;.&quot;hiredate&quot; AS &quot;hiredate&quot;, &quot;RHS&quot;.&quot;salariedflag&quot; AS &quot;salariedflag&quot;, &quot;RHS&quot;.&quot;vacationhours&quot; AS &quot;vacationhours&quot;, &quot;RHS&quot;.&quot;sickleavehours&quot; AS &quot;sickleavehours&quot;, &quot;RHS&quot;.&quot;currentflag&quot; AS &quot;currentflag&quot;, &quot;RHS&quot;.&quot;rowguid&quot; AS &quot;rowguid.y&quot;, &quot;RHS&quot;.&quot;modifieddate&quot; AS &quot;modifieddate.y&quot;, &quot;RHS&quot;.&quot;organizationnode&quot; AS &quot;organizationnode&quot; ## FROM sales.salesperson AS &quot;LHS&quot; ## LEFT JOIN humanresources.employee AS &quot;RHS&quot; ## ON (&quot;LHS&quot;.&quot;businessentityid&quot; = &quot;RHS&quot;.&quot;businessentityid&quot;) ## ) &quot;dbplyr_006&quot; # Need to come up with a different example illustrating where # the `collect` statement goes. # sales_person_table %&gt;% # mutate(birthdate = date(birthdate)) # # try(sales_person_table %&gt;% # mutate(birthdate = lubridate::date(birthdate)) # ) # # sales_person_table %&gt;% collect() %&gt;% # mutate(birthdate = lubridate::date(birthdate)) This may not be relevant in the context where it turns out that dates in adventureworks come through as date! The idea is to show how functions are interpreted BEFORE sending to the SQL translator. to_char &lt;- function(date, fmt) {return(fmt)} # sales_person_table %&gt;% # mutate(birthdate = to_char(birthdate, &quot;YYYY-MM&quot;)) %&gt;% # show_query() # # sales_person_table %&gt;% # mutate(birthdate = to_char(birthdate, &quot;YYYY-MM&quot;)) 11.1.4 Many handy R functions can’t be translated to SQL It just so happens that PostgreSQL has a date function that does the same thing as the date function in the lubridate package. In the following code the date function is executed by PostreSQL. # sales_person_table %&gt;% mutate(birthdate = date(birthdate)) If we specify that we want to use the lubridate version (or any number of other R functions) they are passed to the dbms unless we explicitly tell dplyr to stop translating and bring the results back to the R environment for local processing. try(sales_person_table %&gt;% collect() %&gt;% mutate(birthdate = lubridate::date(birthdate))) ## Error in eval(lhs, parent, parent) : ## object &#39;sales_person_table&#39; not found 11.1.5 Further lazy execution examples See more examples of lazy execution here. DBI::dbDisconnect(con) sqlpetr::sp_docker_stop(&quot;adventureworks&quot;) 11.2 Other resources Benjamin S. Baumer. 2017. A Grammar for Reproducible and Painless Extract-Transform-Load Operations on Medium Data. https://arxiv.org/abs/1708.07073 dplyr Reference documentation: Remote tables. https://dplyr.tidyverse.org/reference/index.html#section-remote-tables Data Carpentry. SQL Databases and R. https://datacarpentry.org/R-ecology-lesson/05-r-and-databases.html "],
["chapter-postgresql-metadata.html", "Chapter 12 Getting metadata about and from PostgreSQL 12.1 Views trick parked here for the time being 12.2 Database contents and structure 12.3 What columns do those tables contain? 12.4 Characterizing how things are named 12.5 Database keys 12.6 Creating your own data dictionary 12.7 Save your work!", " Chapter 12 Getting metadata about and from PostgreSQL This chapter demonstrates: What kind of data about the database is contained in a dbms Several methods for obtaining metadata from the dbms The following packages are used in this chapter: library(tidyverse) library(DBI) library(RPostgres) library(glue) library(here) require(knitr) library(dbplyr) library(sqlpetr) Assume that the Docker container with PostgreSQL and the dvdrental database are ready to go. sp_docker_start(&quot;adventureworks&quot;) Connect to the database: con &lt;- sqlpetr::sp_get_postgres_connection( user = Sys.getenv(&quot;DEFAULT_POSTGRES_USER_NAME&quot;), password = Sys.getenv(&quot;DEFAULT_POSTGRES_PASSWORD&quot;), dbname = &quot;adventureworks&quot;, port = 5432, seconds_to_test = 20, connection_tab = TRUE ) 12.1 Views trick parked here for the time being 12.1.1 Explore the vsalelsperson and vsalespersonsalesbyfiscalyearsdata views The following trick goes later in the book, where it’s used to prove the finding that to make sense of othe data you need to cat(unlist(dbGetQuery(con, &quot;select pg_get_viewdef(&#39;sales.vsalesperson&#39;, true)&quot;))) ## SELECT s.businessentityid, ## p.title, ## p.firstname, ## p.middlename, ## p.lastname, ## p.suffix, ## e.jobtitle, ## pp.phonenumber, ## pnt.name AS phonenumbertype, ## ea.emailaddress, ## p.emailpromotion, ## a.addressline1, ## a.addressline2, ## a.city, ## sp.name AS stateprovincename, ## a.postalcode, ## cr.name AS countryregionname, ## st.name AS territoryname, ## st.&quot;group&quot; AS territorygroup, ## s.salesquota, ## s.salesytd, ## s.saleslastyear ## FROM sales.salesperson s ## JOIN humanresources.employee e ON e.businessentityid = s.businessentityid ## JOIN person.person p ON p.businessentityid = s.businessentityid ## JOIN person.businessentityaddress bea ON bea.businessentityid = s.businessentityid ## JOIN person.address a ON a.addressid = bea.addressid ## JOIN person.stateprovince sp ON sp.stateprovinceid = a.stateprovinceid ## JOIN person.countryregion cr ON cr.countryregioncode::text = sp.countryregioncode::text ## LEFT JOIN sales.salesterritory st ON st.territoryid = s.territoryid ## LEFT JOIN person.emailaddress ea ON ea.businessentityid = p.businessentityid ## LEFT JOIN person.personphone pp ON pp.businessentityid = p.businessentityid ## LEFT JOIN person.phonenumbertype pnt ON pnt.phonenumbertypeid = pp.phonenumbertypeid; ## pg_get_viewdef ## 1 SELECT granular.salespersonid,\\n granular.fullname,\\n granular.jobtitle,\\n granular.salesterritory,\\n sum(granular.subtotal) AS salestotal,\\n granular.fiscalyear\\n FROM ( SELECT soh.salespersonid,\\n ((p.firstname::text || &#39; &#39;::text) || COALESCE(p.middlename::text || &#39; &#39;::text, &#39;&#39;::text)) || p.lastname::text AS fullname,\\n e.jobtitle,\\n st.name AS salesterritory,\\n soh.subtotal,\\n date_part(&#39;year&#39;::text, soh.orderdate + &#39;6 mons&#39;::interval) AS fiscalyear\\n FROM sales.salesperson sp\\n JOIN sales.salesorderheader soh ON sp.businessentityid = soh.salespersonid\\n JOIN sales.salesterritory st ON sp.territoryid = st.territoryid\\n JOIN humanresources.employee e ON soh.salespersonid = e.businessentityid\\n JOIN person.person p ON p.businessentityid = sp.businessentityid) granular\\n GROUP BY granular.salespersonid, granular.fullname, granular.jobtitle, granular.salesterritory, granular.fiscalyear; 12.2 Database contents and structure After just looking at the data you seek, it might be worthwhile stepping back and looking at the big picture. 12.2.1 Database structure For large or complex databases you need to use both the available documentation for your database (e.g., the dvdrental database) and the other empirical tools that are available. For example it’s worth learning to interpret the symbols in an Entity Relationship Diagram: The information_schema is a trove of information about the database. Its format is more or less consistent across the different SQL implementations that are available. Here we explore some of what’s available using several different methods. PostgreSQL stores a lot of metadata. 12.2.2 Contents of the information_schema For this chapter R needs the dbplyr package to access alternate schemas. A schema is an object that contains one or more tables. Most often there will be a default schema, but to access the metadata, you need to explicitly specify which schema contains the data you want. 12.2.3 What tables are in the database? The simplest way to get a list of tables is with … NO LONGER WORKS: schema_list &lt;- tbl(con, in_schema(&quot;information_schema&quot;, &quot;schemata&quot;)) %&gt;% select(catalog_name, schema_name, schema_owner) %&gt;% collect() sp_print_df(head(schema_list)) ### Digging into the information_schema We usually need more detail than just a list of tables. Most SQL databases have an information_schema that has a standard structure to describe and control the database. The information_schema is in a different schema from the default, so to connect to the tables table in the information_schema we connect to the database in a different way: table_info_schema_table &lt;- tbl(con, dbplyr::in_schema(&quot;information_schema&quot;, &quot;tables&quot;)) The information_schema is large and complex and contains 343 tables. So it’s easy to get lost in it. This query retrieves a list of the tables in the database that includes additional detail, not just the name of the table. table_info &lt;- table_info_schema_table %&gt;% # filter(table_schema == &quot;public&quot;) %&gt;% select(table_catalog, table_schema, table_name, table_type) %&gt;% arrange(table_type, table_name) %&gt;% collect() sp_print_df(head(table_info)) In this context table_catalog is synonymous with database. Notice that VIEWS are composites made up of one or more BASE TABLES. The SQL world has its own terminology. For example rs is shorthand for result set. That’s equivalent to using df for a data frame. The following SQL query returns the same information as the previous dplyr code. rs &lt;- dbGetQuery( con, &quot;select table_catalog, table_schema, table_name, table_type from information_schema.tables where table_schema not in (&#39;pg_catalog&#39;,&#39;information_schema&#39;) order by table_type, table_name ;&quot; ) sp_print_df(head(rs)) 12.3 What columns do those tables contain? Of course, the DBI package has a dbListFields function that provides the simplest way to get the minimum, a list of column names: # DBI::dbListFields(con, &quot;rental&quot;) But the information_schema has a lot more useful information that we can use. columns_info_schema_table &lt;- tbl(con, dbplyr::in_schema(&quot;information_schema&quot;, &quot;columns&quot;)) Since the information_schema contains 2961 columns, we are narrowing our focus to just one table. This query retrieves more information about the rental table: columns_info_schema_info &lt;- columns_info_schema_table %&gt;% # filter(table_schema == &quot;public&quot;) %&gt;% select( table_catalog, table_schema, table_name, column_name, data_type, ordinal_position, character_maximum_length, column_default, numeric_precision, numeric_precision_radix ) %&gt;% collect(n = Inf) %&gt;% mutate(data_type = case_when( data_type == &quot;character varying&quot; ~ paste0(data_type, &quot; (&quot;, character_maximum_length, &quot;)&quot;), data_type == &quot;real&quot; ~ paste0(data_type, &quot; (&quot;, numeric_precision, &quot;,&quot;, numeric_precision_radix, &quot;)&quot;), TRUE ~ data_type )) %&gt;% # filter(table_name == &quot;rental&quot;) %&gt;% select(-table_schema, -numeric_precision, -numeric_precision_radix) glimpse(columns_info_schema_info) ## Observations: 2,961 ## Variables: 7 ## $ table_catalog &lt;chr&gt; &quot;adventureworks&quot;, &quot;adventureworks&quot;, &quot;ad… ## $ table_name &lt;chr&gt; &quot;pg_proc&quot;, &quot;pg_proc&quot;, &quot;pg_proc&quot;, &quot;pg_pr… ## $ column_name &lt;chr&gt; &quot;proname&quot;, &quot;pronamespace&quot;, &quot;proowner&quot;, … ## $ data_type &lt;chr&gt; &quot;name&quot;, &quot;oid&quot;, &quot;oid&quot;, &quot;oid&quot;, &quot;real (24,… ## $ ordinal_position &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, … ## $ character_maximum_length &lt;int&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,… ## $ column_default &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,… sp_print_df(head(columns_info_schema_info)) 12.3.1 What is the difference between a VIEW and a BASE TABLE? The BASE TABLE has the underlying data in the database table_info_schema_table %&gt;% filter( table_type == &quot;BASE TABLE&quot;) %&gt;% # filter(table_schema == &quot;public&quot; &amp; table_type == &quot;BASE TABLE&quot;) %&gt;% select(table_name, table_type) %&gt;% left_join(columns_info_schema_table, by = c(&quot;table_name&quot; = &quot;table_name&quot;)) %&gt;% select( table_type, table_name, column_name, data_type, ordinal_position, column_default ) %&gt;% collect(n = Inf) %&gt;% filter(str_detect(table_name, &quot;cust&quot;)) %&gt;% head() %&gt;% sp_print_df() Probably should explore how the VIEW is made up of data from BASE TABLEs. table_info_schema_table %&gt;% filter( table_type == &quot;VIEW&quot;) %&gt;% # filter(table_schema == &quot;public&quot; &amp; table_type == &quot;VIEW&quot;) %&gt;% select(table_name, table_type) %&gt;% left_join(columns_info_schema_table, by = c(&quot;table_name&quot; = &quot;table_name&quot;)) %&gt;% select( table_type, table_name, column_name, data_type, ordinal_position, column_default ) %&gt;% collect(n = Inf) %&gt;% filter(str_detect(table_name, &quot;cust&quot;)) %&gt;% head() %&gt;% sp_print_df() 12.3.2 What data types are found in the database? columns_info_schema_info %&gt;% count(data_type) %&gt;% head() %&gt;% sp_print_df() 12.4 Characterizing how things are named Names are the handle for accessing the data. Tables and columns may or may not be named consistently or in a way that makes sense to you. You should look at these names as data. 12.4.1 Counting columns and name reuse Pull out some rough-and-ready but useful statistics about your database. Since we are in SQL-land we talk about variables as columns. this is wrong! public_tables &lt;- columns_info_schema_table %&gt;% # filter(str_detect(table_name, &quot;pg_&quot;) == FALSE) %&gt;% # filter(table_schema == &quot;public&quot;) %&gt;% collect() public_tables %&gt;% count(table_name, sort = TRUE) %&gt;% head(n = 15) %&gt;% sp_print_df() How many column names are shared across tables (or duplicated)? public_tables %&gt;% count(column_name, sort = TRUE) %&gt;% filter(n &gt; 1) %&gt;% head() ## # A tibble: 6 x 2 ## column_name n ## &lt;chr&gt; &lt;int&gt; ## 1 modifieddate 140 ## 2 rowguid 61 ## 3 id 60 ## 4 name 59 ## 5 businessentityid 49 ## 6 productid 32 How many column names are unique? public_tables %&gt;% count(column_name) %&gt;% filter(n == 1) %&gt;% count() %&gt;% head() ## # A tibble: 1 x 1 ## n ## &lt;int&gt; ## 1 882 12.5 Database keys 12.5.1 Direct SQL How do we use this output? Could it be generated by dplyr? rs &lt;- dbGetQuery( con, &quot; --SELECT conrelid::regclass as table_from select table_catalog||&#39;.&#39;||table_schema||&#39;.&#39;||table_name table_name , conname, pg_catalog.pg_get_constraintdef(r.oid, true) as condef FROM information_schema.columns c,pg_catalog.pg_constraint r WHERE 1 = 1 --r.conrelid = &#39;16485&#39; AND r.contype in (&#39;f&#39;,&#39;p&#39;) ORDER BY 1 ;&quot; ) glimpse(rs) ## Observations: 467,838 ## Variables: 3 ## $ table_name &lt;chr&gt; &quot;adventureworks.hr.d&quot;, &quot;adventureworks.hr.d&quot;, &quot;advent… ## $ conname &lt;chr&gt; &quot;FK_SalesOrderDetail_SpecialOfferProduct_SpecialOffer… ## $ condef &lt;chr&gt; &quot;FOREIGN KEY (specialofferid, productid) REFERENCES s… sp_print_df(head(rs)) The following is more compact and looks more useful. What is the difference between the two? rs &lt;- dbGetQuery( con, &quot;select conrelid::regclass as table_from ,c.conname ,pg_get_constraintdef(c.oid) from pg_constraint c join pg_namespace n on n.oid = c.connamespace where c.contype in (&#39;f&#39;,&#39;p&#39;) and n.nspname = &#39;public&#39; order by conrelid::regclass::text, contype DESC; &quot; ) glimpse(rs) ## Observations: 0 ## Variables: 3 ## $ table_from &lt;chr&gt; ## $ conname &lt;chr&gt; ## $ pg_get_constraintdef &lt;chr&gt; sp_print_df(head(rs)) dim(rs)[1] ## [1] 0 12.5.2 Database keys with dplyr This query shows the primary and foreign keys in the database. tables &lt;- tbl(con, dbplyr::in_schema(&quot;information_schema&quot;, &quot;tables&quot;)) table_constraints &lt;- tbl(con, dbplyr::in_schema(&quot;information_schema&quot;, &quot;table_constraints&quot;)) key_column_usage &lt;- tbl(con, dbplyr::in_schema(&quot;information_schema&quot;, &quot;key_column_usage&quot;)) referential_constraints &lt;- tbl(con, dbplyr::in_schema(&quot;information_schema&quot;, &quot;referential_constraints&quot;)) constraint_column_usage &lt;- tbl(con, dbplyr::in_schema(&quot;information_schema&quot;, &quot;constraint_column_usage&quot;)) keys &lt;- tables %&gt;% left_join(table_constraints, by = c( &quot;table_catalog&quot; = &quot;table_catalog&quot;, &quot;table_schema&quot; = &quot;table_schema&quot;, &quot;table_name&quot; = &quot;table_name&quot; )) %&gt;% # table_constraints %&gt;% filter(constraint_type %in% c(&quot;FOREIGN KEY&quot;, &quot;PRIMARY KEY&quot;)) %&gt;% left_join(key_column_usage, by = c( &quot;table_catalog&quot; = &quot;table_catalog&quot;, &quot;constraint_catalog&quot; = &quot;constraint_catalog&quot;, &quot;constraint_schema&quot; = &quot;constraint_schema&quot;, &quot;table_name&quot; = &quot;table_name&quot;, &quot;table_schema&quot; = &quot;table_schema&quot;, &quot;constraint_name&quot; = &quot;constraint_name&quot; ) ) %&gt;% # left_join(constraint_column_usage) %&gt;% # does this table add anything useful? select(table_name, table_type, constraint_name, constraint_type, column_name, ordinal_position) %&gt;% arrange(table_name) %&gt;% collect() glimpse(keys) ## Observations: 190 ## Variables: 6 ## $ table_name &lt;chr&gt; &quot;address&quot;, &quot;address&quot;, &quot;addresstype&quot;, &quot;billofmat… ## $ table_type &lt;chr&gt; &quot;BASE TABLE&quot;, &quot;BASE TABLE&quot;, &quot;BASE TABLE&quot;, &quot;BASE… ## $ constraint_name &lt;chr&gt; &quot;FK_Address_StateProvince_StateProvinceID&quot;, &quot;PK… ## $ constraint_type &lt;chr&gt; &quot;FOREIGN KEY&quot;, &quot;PRIMARY KEY&quot;, &quot;PRIMARY KEY&quot;, &quot;F… ## $ column_name &lt;chr&gt; &quot;stateprovinceid&quot;, &quot;addressid&quot;, &quot;addresstypeid&quot;… ## $ ordinal_position &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 2, 1, 1,… sp_print_df(head(keys)) What do we learn from the following query? How is it useful? rs &lt;- dbGetQuery( con, &quot;SELECT r.*, pg_catalog.pg_get_constraintdef(r.oid, true) as condef FROM pg_catalog.pg_constraint r WHERE 1=1 --r.conrelid = &#39;16485&#39; AND r.contype = &#39;f&#39; ORDER BY 1; &quot; ) head(rs) ## conname connamespace contype condeferrable ## 1 cardinal_number_domain_check 12771 c FALSE ## 2 yes_or_no_check 12771 c FALSE ## 3 CK_Employee_BirthDate 16386 c FALSE ## 4 CK_Employee_Gender 16386 c FALSE ## 5 CK_Employee_HireDate 16386 c FALSE ## 6 CK_Employee_MaritalStatus 16386 c FALSE ## condeferred convalidated conrelid contypid conindid conparentid ## 1 FALSE TRUE 0 12785 0 0 ## 2 FALSE TRUE 0 12797 0 0 ## 3 FALSE TRUE 16450 0 0 0 ## 4 FALSE TRUE 16450 0 0 0 ## 5 FALSE TRUE 16450 0 0 0 ## 6 FALSE TRUE 16450 0 0 0 ## confrelid confupdtype confdeltype confmatchtype conislocal coninhcount ## 1 0 TRUE 0 ## 2 0 TRUE 0 ## 3 0 TRUE 0 ## 4 0 TRUE 0 ## 5 0 TRUE 0 ## 6 0 TRUE 0 ## connoinherit conkey confkey conpfeqop conppeqop conffeqop conexclop ## 1 FALSE &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 2 FALSE &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 3 FALSE {5} &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 4 FALSE {7} &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 5 FALSE {8} &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 6 FALSE {6} &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## conbin ## 1 {OPEXPR :opno 525 :opfuncid 150 :opresulttype 16 :opretset false :opcollid 0 :inputcollid 0 :args ({COERCETODOMAINVALUE :typeId 23 :typeMod -1 :collation 0 :location 195} {CONST :consttype 23 :consttypmod -1 :constcollid 0 :constlen 4 :constbyval true :constisnull false :location 204 :constvalue 4 [ 0 0 0 0 0 0 0 0 ]}) :location 201} ## 2 {SCALARARRAYOPEXPR :opno 98 :opfuncid 67 :useOr true :inputcollid 100 :args ({RELABELTYPE :arg {COERCETODOMAINVALUE :typeId 1043 :typeMod 7 :collation 100 :location 121} :resulttype 25 :resulttypmod -1 :resultcollid 100 :relabelformat 2 :location -1} {ARRAYCOERCEEXPR :arg {ARRAY :array_typeid 1015 :array_collid 100 :element_typeid 1043 :elements ({CONST :consttype 1043 :consttypmod -1 :constcollid 100 :constlen -1 :constbyval false :constisnull false :location 131 :constvalue 7 [ 28 0 0 0 89 69 83 ]} {CONST :consttype 1043 :consttypmod -1 :constcollid 100 :constlen -1 :constbyval false :constisnull false :location 138 :constvalue 6 [ 24 0 0 0 78 79 ]}) :multidims false :location -1} :elemexpr {RELABELTYPE :arg {CASETESTEXPR :typeId 1043 :typeMod -1 :collation 0} :resulttype 25 :resulttypmod -1 :resultcollid 100 :relabelformat 2 :location -1} :resulttype 1009 :resulttypmod -1 :resultcollid 100 :coerceformat 2 :location -1}) :location 127} ## 3 {BOOLEXPR :boolop and :args ({OPEXPR :opno 1098 :opfuncid 1090 :opresulttype 16 :opretset false :opcollid 0 :inputcollid 0 :args ({VAR :varno 1 :varattno 5 :vartype 1082 :vartypmod -1 :varcollid 0 :varlevelsup 0 :varnoold 1 :varoattno 5 :location 804} {CONST :consttype 1082 :consttypmod -1 :constcollid 0 :constlen 4 :constbyval true :constisnull false :location 817 :constvalue 4 [ 33 -100 -1 -1 -1 -1 -1 -1 ]}) :location 814} {OPEXPR :opno 2359 :opfuncid 2352 :opresulttype 16 :opretset false :opcollid 0 :inputcollid 0 :args ({VAR :varno 1 :varattno 5 :vartype 1082 :vartypmod -1 :varcollid 0 :varlevelsup 0 :varnoold 1 :varoattno 5 :location 842} {OPEXPR :opno 1329 :opfuncid 1190 :opresulttype 1184 :opretset false :opcollid 0 :inputcollid 0 :args ({FUNCEXPR :funcid 1299 :funcresulttype 1184 :funcretset false :funcvariadic false :funcformat 0 :funccollid 0 :inputcollid 0 :args &lt;&gt; :location 856} {CONST :consttype 1186 :consttypmod -1 :constcollid 0 :constlen 16 :constbyval false :constisnull false :location 864 :constvalue 16 [ 0 0 0 0 0 0 0 0 0 0 0 0 -40 0 0 0 ]}) :location 862}) :location 852}) :location 837} ## 4 {SCALARARRAYOPEXPR :opno 98 :opfuncid 67 :useOr true :inputcollid 100 :args ({FUNCEXPR :funcid 871 :funcresulttype 25 :funcretset false :funcvariadic false :funcformat 0 :funccollid 100 :inputcollid 100 :args ({FUNCEXPR :funcid 401 :funcresulttype 25 :funcretset false :funcvariadic false :funcformat 1 :funccollid 100 :inputcollid 100 :args ({VAR :varno 1 :varattno 7 :vartype 1042 :vartypmod 5 :varcollid 100 :varlevelsup 0 :varnoold 1 :varoattno 7 :location 941}) :location 948}) :location 934} {ARRAY :array_typeid 1009 :array_collid 100 :element_typeid 25 :elements ({CONST :consttype 25 :consttypmod -1 :constcollid 100 :constlen -1 :constbyval false :constisnull false :location 969 :constvalue 5 [ 20 0 0 0 77 ]} {CONST :consttype 25 :consttypmod -1 :constcollid 100 :constlen -1 :constbyval false :constisnull false :location 980 :constvalue 5 [ 20 0 0 0 70 ]}) :multidims false :location 963}) :location 956} ## 5 {BOOLEXPR :boolop and :args ({OPEXPR :opno 1098 :opfuncid 1090 :opresulttype 16 :opretset false :opcollid 0 :inputcollid 0 :args ({VAR :varno 1 :varattno 8 :vartype 1082 :vartypmod -1 :varcollid 0 :varlevelsup 0 :varnoold 1 :varoattno 8 :location 1042} {CONST :consttype 1082 :consttypmod -1 :constcollid 0 :constlen 4 :constbyval true :constisnull false :location 1054 :constvalue 4 [ 1 -5 -1 -1 -1 -1 -1 -1 ]}) :location 1051} {OPEXPR :opno 2359 :opfuncid 2352 :opresulttype 16 :opretset false :opcollid 0 :inputcollid 0 :args ({VAR :varno 1 :varattno 8 :vartype 1082 :vartypmod -1 :varcollid 0 :varlevelsup 0 :varnoold 1 :varoattno 8 :location 1079} {OPEXPR :opno 1327 :opfuncid 1189 :opresulttype 1184 :opretset false :opcollid 0 :inputcollid 0 :args ({FUNCEXPR :funcid 1299 :funcresulttype 1184 :funcretset false :funcvariadic false :funcformat 0 :funccollid 0 :inputcollid 0 :args &lt;&gt; :location 1092} {CONST :consttype 1186 :consttypmod -1 :constcollid 0 :constlen 16 :constbyval false :constisnull false :location 1100 :constvalue 16 [ 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 ]}) :location 1098}) :location 1088}) :location 1074} ## 6 {SCALARARRAYOPEXPR :opno 98 :opfuncid 67 :useOr true :inputcollid 100 :args ({FUNCEXPR :funcid 871 :funcresulttype 25 :funcretset false :funcvariadic false :funcformat 0 :funccollid 100 :inputcollid 100 :args ({FUNCEXPR :funcid 401 :funcresulttype 25 :funcretset false :funcvariadic false :funcformat 1 :funccollid 100 :inputcollid 100 :args ({VAR :varno 1 :varattno 6 :vartype 1042 :vartypmod 5 :varcollid 100 :varlevelsup 0 :varnoold 1 :varoattno 6 :location 1181}) :location 1195}) :location 1174} {ARRAY :array_typeid 1009 :array_collid 100 :element_typeid 25 :elements ({CONST :consttype 25 :consttypmod -1 :constcollid 100 :constlen -1 :constbyval false :constisnull false :location 1216 :constvalue 5 [ 20 0 0 0 77 ]} {CONST :consttype 25 :consttypmod -1 :constcollid 100 :constlen -1 :constbyval false :constisnull false :location 1227 :constvalue 5 [ 20 0 0 0 83 ]}) :multidims false :location 1210}) :location 1203} ## consrc ## 1 (VALUE &gt;= 0) ## 2 ((VALUE)::text = ANY ((ARRAY[&#39;YES&#39;::character varying, &#39;NO&#39;::character varying])::text[])) ## 3 ((birthdate &gt;= &#39;1930-01-01&#39;::date) AND (birthdate &lt;= (now() - &#39;18 years&#39;::interval))) ## 4 (upper((gender)::text) = ANY (ARRAY[&#39;M&#39;::text, &#39;F&#39;::text])) ## 5 ((hiredate &gt;= &#39;1996-07-01&#39;::date) AND (hiredate &lt;= (now() + &#39;1 day&#39;::interval))) ## 6 (upper((maritalstatus)::text) = ANY (ARRAY[&#39;M&#39;::text, &#39;S&#39;::text])) ## condef ## 1 CHECK (VALUE &gt;= 0) ## 2 CHECK (VALUE::text = ANY (ARRAY[&#39;YES&#39;::character varying, &#39;NO&#39;::character varying]::text[])) ## 3 CHECK (birthdate &gt;= &#39;1930-01-01&#39;::date AND birthdate &lt;= (now() - &#39;18 years&#39;::interval)) ## 4 CHECK (upper(gender::text) = ANY (ARRAY[&#39;M&#39;::text, &#39;F&#39;::text])) ## 5 CHECK (hiredate &gt;= &#39;1996-07-01&#39;::date AND hiredate &lt;= (now() + &#39;1 day&#39;::interval)) ## 6 CHECK (upper(maritalstatus::text) = ANY (ARRAY[&#39;M&#39;::text, &#39;S&#39;::text])) 12.6 Creating your own data dictionary If you are going to work with a database for an extended period it can be useful to create your own data dictionary. This can take the form of keeping detaild notes as well as extracting metadata from the dbms. Here is an illustration of the idea. This probably doens’t work anymore # some_tables &lt;- c(&quot;rental&quot;, &quot;city&quot;, &quot;store&quot;) # # all_meta &lt;- map_df(some_tables, sp_get_dbms_data_dictionary, con = con) # # all_meta # # glimpse(all_meta) # # sp_print_df(head(all_meta)) 12.7 Save your work! The work you do to understand the structure and contents of a database can be useful for others (including future-you). So at the end of a session, you might look at all the data frames you want to save. Consider saving them in a form where you can add notes at the appropriate level (as in a Google Doc representing table or columns that you annotate over time). ls() ## [1] &quot;columns_info_schema_info&quot; &quot;columns_info_schema_table&quot; ## [3] &quot;con&quot; &quot;constraint_column_usage&quot; ## [5] &quot;key_column_usage&quot; &quot;keys&quot; ## [7] &quot;public_tables&quot; &quot;referential_constraints&quot; ## [9] &quot;rs&quot; &quot;schema_list&quot; ## [11] &quot;table_constraints&quot; &quot;table_info&quot; ## [13] &quot;table_info_schema_table&quot; &quot;tables&quot; "],
["appendix-background-basic-concepts.html", "A Background and Basic Concepts A.1 The big picture: R and the Docker / PostgreSQL playground on your machine A.2 Your computer and its operating system A.3 R A.4 Our sqlpetr package A.5 Docker A.6 ‘Normal’ and ‘normalized’ data A.7 SQL Language A.8 Enterprise DBMS", " A Background and Basic Concepts This Appendix describes: The overall structure of our Docker-based PostgreSQL sandbox Basic concepts around each of the elements that make up our sandbox: tidy data, pipes, Docker, PostgreSQL, data representation, and our petsqlr package. A.1 The big picture: R and the Docker / PostgreSQL playground on your machine Here is an overview of how R and Docker fit on your operating system in this book’s sandbox: R and Docker You run R from RStudio to set up Docker, launch PostgreSQL inside it and then send queries directly to PostgreSQL from R. (We provide more details about our sandbox environment in the chapter on mapping your environment. A.2 Your computer and its operating system The playground that we construct in this book is designed so that some of the mysteries of accessing a corporate database are more visible – it’s all happening on your computer. The challenge, however, is that we know very little about your computer and its operating system. In the workshops we’ve given about this book, the details of individual computers have turned out to be diverse and difficult to pin down in advance. So there can be many issues, but not many basic concepts that we can highlight in advance. A.3 R We assume a general familiarity with R and RStudio. RStudio’s Big Data workshop at the 2019 RStudio has an abundance of introductory material (Ruiz 2019). This book is Tidyverse-oriented, so we assume familiarity with the pipe operator, tidy data (Wickham 2014), dplyr, and techniques for tidying data (Wickham 2018). R connects to a database by means of a series of packages that work together. The following diagram from a big data workshop at the 2019 RStudio conference shows the big picture. The biggest difference in terms of retrieval strategies is between writing dplyr and native SQL code. Dplyr generates SQL-92 standard code; whereas you can write SQL code that leverages the specific language features of your DBMS when you write SQL code yourself. Rstudio’s DBMS architecture - slide # 33 A.4 Our sqlpetr package The sqlpetr package is the companion R package for this database tutorial. It has two classes of functions: Functions to install the dependencies needed to build the book and perform the operations covered in the tutorial, and Utilities for dealing with Docker and the PostgreSQL Docker image we use. sqlpetr has a pkgdown site at https://smithjd.github.io/sqlpetr/. A.5 Docker Docker and the DevOps tools surrounding it have fostered a revolution in the way services are delivered over the internet. In this book, we’re piggybacking on a small piece of that revolution, Docker on the desktop. A.5.1 Virtual machines and hypervisors A virtual machine is a machine that is running purely as software hosted by another real machine. To the user, a virtual machine looks just like a real one. But it has no processors, memory or I/O devices of its own - all of those are supplied and managed by the host. A virtual machine can run any operating system that will run on the host’s hardware. A Linux host can run a Windows virtual machine and vice versa. A hypervisor is the component of the host system software that manages virtual machines, usually called guests. Linux systems have a native hypervisor called Kernel Virtual Machine (kvm). And laptop, desktop and server processors from Intel and Advanced Micro Devices (AMD) have hardware that makes this hypervisor more efficient. Windows servers and Windows 10 Pro have a hypervisor called Hyper-V. Like kvm, Hyper-V can take advantage of the hardware in Intel and AMD processors. On Macintosh, there is a Hypervisor Framework (https://developer.apple.com/documentation/hypervisor) and other tools build on that. If this book is about Docker, why do we care about virtual machines and hypervisors? Docker is a Linux subsystem - it only runs on Linux laptops, desktops and servers. As we’ll see shortly, if we want to run Docker on Windows or MacOS, we’ll need a hypervisor, a Linux virtual machine and some “glue logic” to provide a Docker user experience equivalent to the one on a Linux system. A.5.2 Containers A container is a set of processes running in an operating system. The host operating system is usually Linux, but other operating systems also can host containers. Unlike a virtual machine, the container has no operating system kernel of its own. If the host is running the Linux kernel, so is the container. And since the container OS is the same as the host OS, there’s no need for a hypervisor or hardware to support the hypervisor. So a container is more efficient than a virtual machine. A container does have its own file system. From inside the container, this file system looks like a Linux file system, but it can use any Linux distro. For example, you can have an Ubuntu 18.04 LTS host running Ubuntu 14.04 LTS or Fedora 28 or CentOS 7 containers. The kernel will always be the host kernel, but the utilities and applications will be those from the container. A.5.3 Docker itself While there are both older (lxc) and newer container tools, the one that has caught on in terms of widespread use is Docker (Docker 2019a). Docker is widely used on cloud providers to deploy services of all kinds. Using Docker on the desktop to deliver standardized packages, as we are doing in this book, is a secondary use case, but a common one. If you’re using a Linux laptop / desktop, all you need to do is install Docker CE (Docker 2018a). However, most laptops and desktops don’t run Linux - they run Windows or MacOS. As noted above, to use Docker on Windows or MacOS, you need a hypervisor and a Linux virtual machine. A.5.4 Docker objects The Docker subsystem manages several kinds of objects - containers, images, volumes and networks. In this book, we are only using the basic command line tools to manage containers, images and volumes. Docker images are files that define a container’s initial file system. You can find pre-built images on Docker Hub and the Docker Store - the base PostgreSQL image we use comes from Docker Hub (https://hub.docker.com/_/postgres/). If there isn’t a Docker image that does exactly what you want, you can build your own by creating a Dockerfile and running docker build. We do this in [Build the pet-sql Docker Image]. Docker volumes – explain mount. A.5.5 Hosting Docker on Windows machines There are two ways to get Docker on Windows. For Windows 10 Home and older versions of Windows, you need Docker Toolbox (Docker 2019e). Note that for Docker Toolbox, you need a 64-bit AMD or Intel processor with the virtualization hardware installed and enabled in the BIOS. For Windows 10 Pro, you have the Hyper-V virtualizer as standard equipment, and can use Docker for Windows (Docker 2019c). A.5.6 Hosting Docker on macOS machines As with Windows, there are two ways to get Docker. For older Intel systems, you’ll need Docker Toolbox (Docker 2019d). Newer systems (2010 or later running at least macOS El Capitan 10.11) can run Docker for Mac (Docker 2019b). A.5.7 Hosting Docker on UNIX machines Unix was the original host for both R and Docker. Unix-like commands show up. A.6 ‘Normal’ and ‘normalized’ data A.6.1 Tidy data Tidy data (Wickham 2014) is well-behaved from the point of view of analysis and tools in the Tidyverse (RStudio 2019). Tidy data is easier to think about and it is usually worthwhile to make the data tidy (Wickham 2018). Tidy data is roughly equivalent to third normal form as discussed below. A.6.2 Design of “normal data” Data in a database is most often optimized to minimize storage space and increase performance while preserving integrity when adding, changing, or deleting data. The Wikipedia article on Database Normalization has a good introduction to the characteristics of “normal” data and the process of re-organizing it to meet those desirable criteria (Wikipedia 2019). The bottom line is that “data normalization is practical” although there are mathematical arguments for normalization based on the preservation of data integrity. A.7 SQL Language SQL stands for Structured Query Language. It is a database language where we can perform certain operations on the existing database and we can use it create a new database. There are four main categories where the SQL commands fall into: DML, DDL, DCL, and TCL. A.7.1 Data Manipulation Langauge (DML) These four SQL commands deal with the manipulation of data in the database. For everyday analytical work, these are the commands that you will use the most. 1. SELECT 2. INSERT 3. UPDATE 4. DELETE A.7.2 Data Definition Langauge (DDL) It consists of the SQL commands that can be used to define a database schema. The DDL commands include: 1. CREATE 2. ALTER 3. TRUNCATE 4. COMMENT 5. RENAME 6. DROP A.7.3 Data Control Language (DCL) The DCL commands deals with user rights, permissions and other controls in database management system. 1. GRANT 2. REVOKE A.7.4 Transaction Control Language (TCL) These commands deal with the control over transaction within the database. Transaction combines a set of tasks into single execution. 1. SET TRANSACTION 2. SAVEPOINT 3. ROLLBACK 4. COMMIT A.8 Enterprise DBMS The organizational context of a database matters just as much as its design characteristics. The design of a database (or data model) may have been purchased from an external vendor or developed in-house. In either case time has a tendency to erode the original design concept so that the data you find in a DBMS may not quite match the original design specification. And the original design may or may not be well reflected in the current naming of tables, columns and other objects. It’s a naive misconception to think that the data you are analyzing just “comes from the database”, although that’s literally true and may be the step that happens before you get your hands on it. In fact it comes from the people who design, enter, manage, protect, and use your organization’s data. In practice, a database administrator (DBA) is often a key point of contact in terms of access and may have stringent criteria for query performance. Make friends with your DBA. A.8.1 SQL databases Although there are ANSI standards for SQL syntax, different implementations vary in enough details that R’s ability to customize queries for those implementations is very helpful. The tables in a DBMS correspond to a data frame in R, so interaction with a DBMS is fairly natural for useRs. SQL code is characterized by the fact that it describes what to retrieve, leaving the DBMS back end to determine how to do it. Therefore it has a batch feel. The pipe operator (%&gt;%, which is read as and then) is inherently procedural when it’s used with dplyr: it can be used to construct queries step-by-step. Once a test dplyr query has been executed, it is easy to inspect the results and add steps with the pipe operator to refine or expand the query. A.8.2 Data mapping between R vs SQL data types The following code shows how different elements of the R bestiary are translated to and from ANSI standard data types. Note that R factors are translated as TEXT so that missing levels are ignored on the SQL side. library(DBI) dbDataType(ANSI(), 1:5) ## [1] &quot;INT&quot; dbDataType(ANSI(), 1) ## [1] &quot;DOUBLE&quot; dbDataType(ANSI(), TRUE) ## [1] &quot;SMALLINT&quot; dbDataType(ANSI(), Sys.Date()) ## [1] &quot;DATE&quot; dbDataType(ANSI(), Sys.time()) ## [1] &quot;TIMESTAMP&quot; dbDataType(ANSI(), Sys.time() - as.POSIXct(Sys.Date())) ## [1] &quot;TIME&quot; dbDataType(ANSI(), c(&quot;x&quot;, &quot;abc&quot;)) ## [1] &quot;TEXT&quot; dbDataType(ANSI(), list(raw(10), raw(20))) ## [1] &quot;BLOB&quot; dbDataType(ANSI(), I(3)) ## [1] &quot;DOUBLE&quot; dbDataType(ANSI(), iris) ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## &quot;DOUBLE&quot; &quot;DOUBLE&quot; &quot;DOUBLE&quot; &quot;DOUBLE&quot; &quot;TEXT&quot; The DBI specification provides extensive documentation that is worth digesting if you intend to work with a DBMS from R. As you work through the examples in this book, you will also want to refer to the following resources: RStudio’s Databases using R site describes many of the technical details involved. The RStudio community is an excellent place to ask questions or study what has been discussed previously. A.8.3 PostgreSQL and connection parameters An important detail: We use a PostgreSQL database server running in a Docker container for the database functions. It is installed inside Docker, so you do not have to download or install it yourself. To connect to it, you have to define some parameters. These parameters are used in two places: When the Docker container is created, they’re used to initialize the database, and Whenever we connect to the database, we need to specify them to authenticate. We define the parameters in an environment file that R reads when starting up. The file is called .Renviron, and is located in your home directory. See the discussion of securing and using dbms credentials. A.8.4 Connecting the R and DBMS environments Although everything happens on one machine in our Docker / PostgreSQL playground, in real life R and PostgreSQL (or other DBMS) will be in different environments on separate machines. How R connects them gives you control over where the work happens. You need to be aware of the differences beween the R and DBMS environments as well as how you can leverage the strengths of each one. Characteristics of local vs. server processing Dimension Local Remote Design purpose The R environment on your local machine is designed to be flexible and easy to use; ideal for data investigation. The DBMS environment is designed for large and complex databases where data integrity is more important than flexibility or ease of use. Processor power Your local machine has less memory, speed, and storage than the typical database server. Database servers are specialized, more expensive, and have more power. Memory constraint In R, query results must fit into memory. Servers have a lot of memory and write intermediate results to disk if needed without you knowing about it. Data crunching Data lives in the DBMS, so crunching it down locally requires you to pull it over the network. A DBMS has powerful data crunching capabilities once you know what you want and moves data over the server backbone to crunch it. Security Local control. Whether it is good or not depends on you. Responsibility of database administrators who set the rules. You play by their rules. Storage of intermediate results Very easy to save a data frame with intermediate results locally. May require extra privileges to save results in the database. Analytical resources Ecosystem of available R packages Extending SQL instruction set involves dbms-specific functions or R pseudo functions Collaboration One person working on a few data.frames. Many people collaborating on many tables. A.8.5 Using SQLite to simulate an enterprise DBMS SQLite engine is embedded in one file, so that many tables are stored together in one object. SQL commands can run against an SQLite database as demonstrated in how many uses of SQLite are in the RStudio dbplyr documentation. References "],
["chapter-appendix-setup-instructions.html", "B - Setup instructions B.1 Sandbox prerequisites B.2 R, RStudio and Git B.3 Install Docker", " B - Setup instructions This appendix explains: Hardware and software prerequisites for setting up the sandbox used in this book Documentation for all of the elements used in this sandbox B.1 Sandbox prerequisites The sandbox environment requires: A computer running Windows (Windows 7 64-bit or later - Windows 10-Pro is recommended), MacOS, or Linux (any Linux distro that will run Docker Community Edition, R and RStudio will work) Current versions of R and RStudio [Vargas (2018)) required. Docker (instructions below) Our companion package sqlpetr (Borasky et al. 2018) The database we use is PostgreSQL 11, but you do not need to install it - it’s installed via a Docker image. In addition to the current version of R and RStudio, you will need current versions of the following packages: DBI (R Special Interest Group on Databases (R-SIG-DB), Wickham, and Müller 2018) DiagrammeR (Iannone 2019) RPostgres (Wickham, Ooms, and Müller 2018) dbplyr (Wickham and Ruiz 2019) devtools (Wickham, Hester, and Chang 2019) downloader (Chang 2015) glue (Hester 2019) here (Müller 2017) knitr (Xie 2019b) skimr (Quinn et al. 2019) tidyverse (Wickham 2017) bookdown (Xie 2019a) (for compiling the book, if you want to) B.2 R, RStudio and Git Most readers will probably have these already, but if not: If you do not have R: Go to https://cran.rstudio.com/ (R Core Team 2018). Select the download link for your system. For Linux, choose your distro. We recommend Ubuntu 18.04 LTS “Bionic Beaver”. It’s much easier to find support answers on the web for Ubuntu than other distros. Follow the instructions. Note: if you already have R, make sure it’s upgraded to R 3.5.1. We don’t test on older versions! If you do not have RStudio: go to https://www.rstudio.com/products/rstudio/download/#download. Make sure you have version 1.1.463 or later. If you do not have Git: On Windows, go to https://git-scm.com/download/win and follow instructions. There are a lot of options. Just pick the defaults!!! On MacOS, go to https://sourceforge.net/projects/git-osx-installer/files/ and follow instructions. On Linux, install Git from your distribution. B.3 Install Docker Installation depends on your operating system and we have found that it can be somewhat intricate. You will need Docker Community Edition (Docker CE): For Windows, consider these issues and follow these instructions: Go to https://store.docker.com/editions/community/docker-ce-desktop-windows. If you don’t have a Docker Store log in, you’ll need to create one. Then: If you have Windows 10 Pro, download and install Docker for Windows. If you have an older version of Windows, download and install Docker Toolbox (https://docs.docker.com/toolbox/overview/). Note that both versions require 64-bit hardware and the virtualization needs to be enabled in the firmware. On a Mac (Docker 2018c): Go to https://store.docker.com/editions/community/docker-ce-desktop-mac. If you don’t have a Docker Store login, you’ll need to create one. Then download and install Docker for Mac. Your MacOS must be at least release Yosemite (10.10.3). On UNIX flavors (Docker 2018a): note that, as with Windows and MacOS, you’ll need a Docker Store loin. Although most Linux distros ship with some version of Docker, chances are it’s not the same as the official Docker CE version. Ubuntu: https://store.docker.com/editions/community/docker-ce-server-ubuntu, Fedora: https://store.docker.com/editions/community/docker-ce-server-fedora, Cent OS: https://store.docker.com/editions/community/docker-ce-server-centos, Debian: https://store.docker.com/editions/community/docker-ce-server-debian. Note that on Linux, you will need to be a member of the docker group to use Docker. To do that, execute sudo usermod -aG docker ${USER}. Then, log out and back in again. References "],
["chapter-appendix-postgres-local-db-installation.html", "C Appendix E - Install adventureworks on your own machine C.1 Overview C.2 Resources", " C Appendix E - Install adventureworks on your own machine This appendix demonstrates how to: Setup the adventureworks database locally on your machine Connect to the adventureworks database These instructions should be tested by a Windows user The PostgreSQL tutorial links do not work, despite being pasted from the site C.1 Overview This appendix details the process to download and restore the adventureworks database so that you can work with the database locally on your own machine. This tutorial assumes that (1) you have PostgreSQL installed on your computer, and (2) that you have configured your system to run psql at the command line. Installation of PostgreSQL and configuration of psql are outside the scope of this book. C.1.1 Download the adventureworks database Download the adventureworks database from here. C.1.2 Restore the dvdrental database at the command line Launch the psql tool Enter account information to log into the PostgreSQL database server, if prompted Enter the following command to create a new database CREATE DATABASE adventureworks; Open a new terminal window (not in psql) and navigate to the folder where the adventureworks.sql file is located. Use the cd command in the terminal, followed by the file path to change directories to the location of adventureworks.sql. For example: cd /Users/username/Documents/adventureworks. Enter the following command prompt: pg_restore -d adventureworks -f -U postgres adventureworks.sql C.1.3 Restore the adventureworks database using pgAdmin Another option to restore the adventureworks database locally on your machine is with the pgAdmin graphical user interface. However, we highly recommend using the command line methods detailed above. Installation and configuration of pgAdmin is outside the scope of this book. C.2 Resources Instructions by PostgreSQL Tutorial to load the dvdrental database. (PostgreSQL Tutorial Website 2019). Windows installation of PostgreSQL by PostgreSQL Tutorial. (PostgreSQL Tutorial Website 2019). Installation of PostgreSQL on a Mac using Postgres.app. (Postgres.app 2019). Command line configuration of PosgreSQL on a Mac with Postgres.app. (Postgres.app 2019). Installing PostgreSQL for Linux, Arch Linux, Windows, Mac and other operating systems, by Postgres Guide. (Postgres Guide Website 2019). "],
["chapter-windows-tech-details.html", "D Appendix B - Additional technical details for Windows users D.1 Hardware requirements D.2 Software requirements D.3 Docker for Windows settings D.4 Git, GitHub and line endings", " D Appendix B - Additional technical details for Windows users This chapter explains: How to setup your environment for Windows How to use Git and GitHub effectively on Windows Skip these instructions if your computer has either OSX or a Unix variant. D.1 Hardware requirements You will need an Intel or AMD processor with 64-bit hardware and the hardware virtualization feature. Most machines you buy today will have that, but older ones may not. You will need to go into the BIOS / firmware and enable the virtualization feature. You will need at least 4 gigabytes of RAM! D.2 Software requirements You will need Windows 7 64-bit or later. If you can afford it, I highly recommend upgrading to Windows 10 Pro. D.2.1 Windows 7, 8, 8.1 and Windows 10 Home (64 bit) Install Docker Toolbox. The instructions are here: https://docs.docker.com/toolbox/toolbox_install_windows/. Make sure you try the test cases and they work! D.2.2 Windows 10 Pro Install Docker for Windows stable. The instructions are here: https://docs.docker.com/docker-for-windows/install/#start-docker-for-windows. Again, make sure you try the test cases and they work. D.3 Docker for Windows settings D.3.1 Shared drives If you’re going to mount host files into container file systems (as we do in the following chapters), you need to set up shared drives. Open the Docker settings dialog and select Shared Drives. Check the drives you want to share. In this screenshot, the D: drive is my 1 terabyte hard drive. D.3.2 Kubernetes Kubernetes is a container orchestration / cloud management package that’s a major DevOps tool. It’s heavily supported by Red Hat and Google, and as a result is becoming a required skill for DevOps. However, it’s overkill for this project at the moment. So you should make sure it’s not enabled. Go to the Kubernetes dialog and make sure the Enable Kubernetes checkbox is cleared. D.4 Git, GitHub and line endings Git was originally developed for Linux - in fact, it was created by Linus Torvalds to manage hundreds of different versions of the Linux kernel on different machines all around the world. As usage has grown, Git has achieved a huge following and is the version control system used by most large open source projects, including this one. If you’re on Windows, there are some things about Git and GitHub you need to watch. First of all, there are quite a few tools for running Git on Windows, but the RStudio default and recommended one is Git for Windows (https://git-scm.com/download/win). By default, text files on Linux end with a single linefeed (\\n) character. But on Windows, text files end with a carriage return and a line feed (\\r\\n). See https://en.wikipedia.org/wiki/Newline for the gory details. Git defaults to checking files out in the native mode. So if you’re on Linux, a text file will show up with the Linux convention, and if you’re on Windows, it will show up with the Windows convention. Most of the time this doesn’t cause any problems. But Docker containers usually run Linux, and if you have files from a repository on Windows that you’ve sent to the container, the container may malfunction or give weird results. This kind of situation has caused a lot of grief for contributors to this project, so beware. In particular, executable sh or bash scripts will fail in a Docker container if they have Windows line endings. You may see an error message with \\r in it, which means the shell saw the carriage return (\\r) and gave up. But often you’ll see no hint at all what the problem was. So you need a way to tell Git that some files need to be checked out with Linux line endings. See https://help.github.com/articles/dealing-with-line-endings/ for the details. Summary: You’ll need a .gitattributes file in the root of the repository. In that file, all text files (scripts, program source, data, etc.) that are destined for a Docker container will need to have the designator &lt;spec&gt; text eol=lf, where &lt;spec&gt; is the file name specifier, for example, *.sh. This repo includes a sample: .gitattributes "],
["chapter-appendix-postresql-authentication.html", "E Appendix C - PostgreSQL Authentication E.1 Introduction E.2 Password authentication on the PostgreSQL Docker image E.3 Adding roles", " E Appendix C - PostgreSQL Authentication E.1 Introduction PostgreSQL has a very robust and flexible set of authentication methods (PostgreSQL Global Development Group 2018a). In most production environments, these will be managed by the database administrator (DBA) on a need-to-access basis. People and programs will be granted access only to a minimum set of capabilities required to function, and nothing more. In this book, we are using a PostgreSQL Docker image (Docker 2018d). When we create a container from that image, we use its native mechanism to create the postgres database superuser with a password specified in an R environment file ~/.Renviron. See Securing and using your dbms log-in credentials for how we do this. What that means is that you are the DBA - the database superuser - for the PostgreSQL database cluster running in the container! You can create and destroy databases, schemas, tables, views, etc. You can also create and destroy users - called roles in PostgreSQL, and GRANT or REVOKE their privileges with great precision. You don’t have to do that to use this book. But if you want to experiment with it, feel free! E.2 Password authentication on the PostgreSQL Docker image Of the many PostgreSQL authentication mechanisms, the simplest that’s universallly available is password authentication (PostgreSQL Global Development Group 2018c). That’s what we use for the postgres database superuser, and what we recommend for any roles you may create. Once a role has been created, you need five items to open a connection to the PostgreSQL database cluster: The host. This is a name or IP address that your network can access. In this book, with the database running in a Docker container, that’s usually localhost. The port. This is the port the server is listening on. It’s usually the default, 5439, and that’s what we use. But in a secure environment, it will often be some random number to lower the chances that an attacker can find the database server. And if you have more than one server on the network, you’ll need to use different ports for each of them. The dbname to connect to. This database must exist or the connection attempt will fail. The user. This user must exist in the database cluster and be allowed to access the database. We are using the database superuser postgres in this book. The password. This is set by the DBA for the user. In this book we use the password defined in Securing and using your dbms log-in credentials. E.3 Adding roles As noted above, PostgreSQL has a very flexible fine-grained access permissions system. We can’t cover all of it; see PostgreSQL Global Development Group (2018b) for the full details. But we can give an example. E.3.1 Setting up Docker First, we need to make sure we don’t have any other databases listening on the default port 5439. sqlpetr::sp_check_that_docker_is_up() ## [1] &quot;Docker is up, running these containers:&quot; ## [2] &quot;CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES&quot; ## [3] &quot;ee0f6ee5a1cc postgres:11 \\&quot;docker-entrypoint.s…\\&quot; About a minute ago Up 14 seconds 0.0.0.0:5432-&gt;5432/tcp adventureworks&quot; sqlpetr::sp_docker_remove_container(&quot;cattle&quot;) ## [1] 0 # in case you&#39;ve been doing things out of order, stop a container named &#39;adventureworks&#39; if it exists: sqlpetr::sp_docker_stop(&quot;adventureworks&quot;) E.3.2 Creating a new container We’ll create a “cattle” container with a default PostgreSQL 10 database cluster. sqlpetr::sp_make_simple_pg(&quot;cattle&quot;) con &lt;- sqlpetr::sp_get_postgres_connection( host = &quot;localhost&quot;, port = 5439, dbname = &quot;postgres&quot;, user = &quot;postgres&quot;, password = &quot;postgres&quot;, seconds_to_test = 30 ) E.3.3 Adding a role Now, let’s add a role. We’ll add a role that can log in and create databases, but isn’t a superuser. Since this is a demo and not a real production database cluster, we’ll specify a password in plaintext. And we’ll create a database for our new user. Create the role: DBI::dbExecute( con, &quot;CREATE ROLE charlie LOGIN CREATEDB PASSWORD &#39;chaplin&#39;;&quot; ) ## [1] 0 Create the database: DBI::dbExecute( con, &quot;CREATE DATABASE charlie OWNER = charlie&quot;) ## [1] 0 E.3.4 Did it work? DBI::dbDisconnect(con) con &lt;- sqlpetr::sp_get_postgres_connection( host = &quot;localhost&quot;, port = 5439, dbname = &quot;postgres&quot;, user = &quot;charlie&quot;, password = &quot;chaplin&quot;, seconds_to_test = 30 ) OK, we can connect. Let’s do some stuff! data(&quot;iris&quot;) dbCreateTable creates the table with columns matching the data frame. But it does not send data to the table. DBI::dbCreateTable(con, &quot;iris&quot;, iris) To send data, we use dbAppendTable. DBI::dbAppendTable(con, &quot;iris&quot;, iris) ## Warning: Factors converted to character ## [1] 150 DBI::dbListTables(con) ## [1] &quot;iris&quot; head(DBI::dbReadTable(con, &quot;iris&quot;)) ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## 1 5.1 3.5 1.4 0.2 setosa ## 2 4.9 3.0 1.4 0.2 setosa ## 3 4.7 3.2 1.3 0.2 setosa ## 4 4.6 3.1 1.5 0.2 setosa ## 5 5.0 3.6 1.4 0.2 setosa ## 6 5.4 3.9 1.7 0.4 setosa DBI::dbDisconnect(con) E.3.5 Remove the container sqlpetr::sp_docker_remove_container(&quot;cattle&quot;) ## [1] 0 References "],
["chapter-appendix-dplyr-functions.html", "F Dplyr functions and SQL cross-walk", " F Dplyr functions and SQL cross-walk Where are these covered and should they be included? Dplyr Function description SQL Clause Where Category all_equal() all.equal() Flexible equality comparison for data frames Two-table verbs all_vars() any_vars() Apply predicate to all variables scoped-Operate on a selection of variables arrange() Arrange rows by variables ORDER BY 13.1.4 (21) Basic single-table verbs arrange_all() arrange_at() arrange_if() Arrange rows by a selection of variables ORDER BY scoped-Operate on a selection of variables auto_copy() Copy tables to same source, if necessary Remote tables between() Do values in a numeric vector fall in specified range? Vector functions bind_rows() bind_cols() combine() Efficiently bind multiple data frames by row and column Two-table verbs case_when() A general vectorised if Vector functions coalesce() Find first non-missing element Vector functions compute() collect() collapse() Force computation of a database query Remote tables copy_to() Copy a local data frame to a remote src Remote tables cumall() cumany() cummean() Cumulativate versions of any, all, and mean Vector functions desc() Descending order Vector functions distinct() Return rows with matching conditions SELECT distinct * Basic single-table verbs distinct() Select distinct/unique rows SELECT distinct {colname1,…colnamen} Basic single-table verbs do() Do anything NA Basic single-table verbs explain() show_query() Explain details of a tbl Remote tables filter_all() filter_if() filter_at() Filter within a selection of variables scoped-Operate on a selection of variables funs() Create a list of functions calls. scoped-Operate on a selection of variables group_by() ungroup() Objects exported from other packages GROUP BY no ungroup Basic single-table verbs group_by_all() group_by_at() group_by_if() Group by a selection of variables scoped-Operate on a selection of variables groups() group_vars() Return grouping variables Metadata ident() Flag a character vector as SQL identifiers Remote tables if_else() Vectorised if Vector functions inner_join() left_join() right_join() full_join() semi_join() anti_join() Join two tbls together Two-table verbs inner_join()left_join() right_join() full_join() semi_join() anti_join() Join data frame tbls Two-table verbs intersect() union() union_all() setdiff() setequal() Set operations Two-table verbs lead() lag() Lead and lag. Vector functions mutate() transmute() Add new variables SELECT computed_value computed_name 11.5.2 (13) Basic single-table verbs n() The number of observations in the current group. Vector functions n_distinct() Efficiently count the number of unique values in a set of vector Vector functions na_if() Convert values to NA Vector functions near() Compare two numeric vectors Vector functions nth() first() last() Extract the first, last or nth value from a vector Vector functions order_by() A helper function for ordering window function output Vector functions pull() Pull out a single variable SELECT column_name; Basic single-table verbs recode() recode_factor() Recode values Vector functions row_number() ntile() min_rank() dense_rank() percent_rank() cume_dist() Windowed rank functions. Vector functions rowwise() Group input by rows Other backends sample_n() sample_frac() Sample n rows from a table ORDER BY RANDOM() LIMIT 10 Basic single-table verbs select() rename() Select/rename variables by name SELECT column_name alias_name 9.1.8 (11) Basic single-table verbs select_all() rename_all() select_if() rename_if() select_at() rename_at() Select and rename a selection of variables scoped-Operate on a selection of variables slice() Select rows by position SELECT row_number() over (partition by expression(s) order_by exp) Basic single-table verbs sql() SQL escaping. Remote tables src_mysql() src_postgres() src_sqlite() Source for database backends Remote tables summarise_all() summarise_if() summarise_at() summarize_all() summarize_if() summarize_at() mutate_all() mutate_if() mutate_at() transmute_all() transmute_if() transmute_at() Summarise and mutate multiple columns. scoped-Operate on a selection of variables summarize() Reduces multiple values down to a single value SELECT aggregate_functions GROUP BY 11.5.1 (13) Basic single-table verbs tally() count()add_tally() add_count() Count/tally observations by group GROUP BY 9.1.6 (11) Single-table helpers tbl() is.tbl() as.tbl() Create a table from a data source Remote tables top_n() Select top (or bottom) n rows (by value) ORDER BY VALUE {DESC} LIMIT 10 Single-table helpers vars() Select variables scoped-Operate on a selection of variables "],
["chapter-appendix-dbi-index.html", "G DBI package functions - INDEX", " G DBI package functions - INDEX Where are these covered and should the by included? DBI 1st time Call Example/Notes DBIConnct 6.3.2 (04) in sp_get_postgres_connection dbAppendTable dbCreateTable dbDisconnect 6.4n (04) dbDisconnect(con) dbExecute 10.4.2 (13) Executes a statement and returns the number of rows affected. dbExecute() comes with a default implementation (which should work with most backends) that calls dbSendStatement(), then dbGetRowsAffected(), ensuring that the result is always free-d by dbClearResult(). dbExistsTable dbExistsTable(con,‘actor’) dbFetch 17.1 (72) dbFetch(rs) dbGetException dbGetInfo dbGetInfo(con) dbGetQuery 10.4.1 (13) dbGetQuery(con,‘select * from store;’) dbIsReadOnly dbIsReadOnly(con) dbIsValid dbIsValid(con) dbListFields 6.3.3 (04) DBI::dbListFields(con, “mtcars”) dbListObjects dbListObjects(con) dbListTables 6.3.2 (04) DBI::dbListTables(con, con) dbReadTable 8.1.2 DBI::dbReadTable(con, “rental”) dbRemoveTable dbSendQuery 17.1 (72) rs &lt;- dbSendQuery(con, “SELECT * FROM mtcars WHERE cyl = 4”) dbSendStatement The dbSendStatement() method only submits and synchronously executes the SQL data manipulation statement (e.g., UPDATE, DELETE, INSERT INTO, DROP TABLE, …) to the database engine. dbWriteTable 6.3.3 (04) dbWriteTable(con, “mtcars”, mtcars, overwrite = TRUE) "],
["chapter-appendix-dplyr-to-postres-translation.html", "H Appendix _ Dplyr to SQL translations H.1 Overview", " H Appendix _ Dplyr to SQL translations You may be interested in exactly how the DBI package translates R functions into their SQL quivalents – and in which functions are translated and which are not. This Appendix answers those questions. It is based on the work of Dewey Dunnington (@paleolimbot) which he published here: https://apps.fishandwhistle.net/archives/1503 https://rud.is/b/2019/04/10/lost-in-sql-translation-charting-dbplyr-mapped-sql-function-support-across-all-backends/ H.1 Overview These packages are called below: library(tidyverse) library(dbplyr) library(gt) library(here) library(sqlpetr) list the DBI functions that are available: names(sql_translate_env(simulate_dbi())) ## [1] &quot;-&quot; &quot;:&quot; &quot;!&quot; ## [4] &quot;!=&quot; &quot;(&quot; &quot;[&quot; ## [7] &quot;[[&quot; &quot;{&quot; &quot;*&quot; ## [10] &quot;/&quot; &quot;&amp;&quot; &quot;&amp;&amp;&quot; ## [13] &quot;%%&quot; &quot;%&gt;%&quot; &quot;%in%&quot; ## [16] &quot;^&quot; &quot;+&quot; &quot;&lt;&quot; ## [19] &quot;&lt;=&quot; &quot;==&quot; &quot;&gt;&quot; ## [22] &quot;&gt;=&quot; &quot;|&quot; &quot;||&quot; ## [25] &quot;$&quot; &quot;abs&quot; &quot;acos&quot; ## [28] &quot;as_date&quot; &quot;as_datetime&quot; &quot;as.character&quot; ## [31] &quot;as.Date&quot; &quot;as.double&quot; &quot;as.integer&quot; ## [34] &quot;as.integer64&quot; &quot;as.logical&quot; &quot;as.numeric&quot; ## [37] &quot;as.POSIXct&quot; &quot;asin&quot; &quot;atan&quot; ## [40] &quot;atan2&quot; &quot;between&quot; &quot;bitwAnd&quot; ## [43] &quot;bitwNot&quot; &quot;bitwOr&quot; &quot;bitwShiftL&quot; ## [46] &quot;bitwShiftR&quot; &quot;bitwXor&quot; &quot;c&quot; ## [49] &quot;case_when&quot; &quot;ceil&quot; &quot;ceiling&quot; ## [52] &quot;coalesce&quot; &quot;cos&quot; &quot;cosh&quot; ## [55] &quot;cot&quot; &quot;coth&quot; &quot;day&quot; ## [58] &quot;desc&quot; &quot;exp&quot; &quot;floor&quot; ## [61] &quot;hour&quot; &quot;if&quot; &quot;if_else&quot; ## [64] &quot;ifelse&quot; &quot;is.na&quot; &quot;is.null&quot; ## [67] &quot;log&quot; &quot;log10&quot; &quot;mday&quot; ## [70] &quot;minute&quot; &quot;month&quot; &quot;na_if&quot; ## [73] &quot;nchar&quot; &quot;now&quot; &quot;paste&quot; ## [76] &quot;paste0&quot; &quot;pmax&quot; &quot;pmin&quot; ## [79] &quot;qday&quot; &quot;round&quot; &quot;second&quot; ## [82] &quot;sign&quot; &quot;sin&quot; &quot;sinh&quot; ## [85] &quot;sql&quot; &quot;sqrt&quot; &quot;str_c&quot; ## [88] &quot;str_conv&quot; &quot;str_count&quot; &quot;str_detect&quot; ## [91] &quot;str_dup&quot; &quot;str_extract&quot; &quot;str_extract_all&quot; ## [94] &quot;str_flatten&quot; &quot;str_glue&quot; &quot;str_glue_data&quot; ## [97] &quot;str_interp&quot; &quot;str_length&quot; &quot;str_locate&quot; ## [100] &quot;str_locate_all&quot; &quot;str_match&quot; &quot;str_match_all&quot; ## [103] &quot;str_order&quot; &quot;str_pad&quot; &quot;str_remove&quot; ## [106] &quot;str_remove_all&quot; &quot;str_replace&quot; &quot;str_replace_all&quot; ## [109] &quot;str_replace_na&quot; &quot;str_sort&quot; &quot;str_split&quot; ## [112] &quot;str_split_fixed&quot; &quot;str_squish&quot; &quot;str_sub&quot; ## [115] &quot;str_subset&quot; &quot;str_to_lower&quot; &quot;str_to_title&quot; ## [118] &quot;str_to_upper&quot; &quot;str_trim&quot; &quot;str_trunc&quot; ## [121] &quot;str_view&quot; &quot;str_view_all&quot; &quot;str_which&quot; ## [124] &quot;str_wrap&quot; &quot;substr&quot; &quot;switch&quot; ## [127] &quot;tan&quot; &quot;tanh&quot; &quot;today&quot; ## [130] &quot;tolower&quot; &quot;toupper&quot; &quot;trimws&quot; ## [133] &quot;wday&quot; &quot;xor&quot; &quot;yday&quot; ## [136] &quot;year&quot; &quot;cume_dist&quot; &quot;cummax&quot; ## [139] &quot;cummean&quot; &quot;cummin&quot; &quot;cumsum&quot; ## [142] &quot;dense_rank&quot; &quot;first&quot; &quot;lag&quot; ## [145] &quot;last&quot; &quot;lead&quot; &quot;max&quot; ## [148] &quot;mean&quot; &quot;median&quot; &quot;min&quot; ## [151] &quot;min_rank&quot; &quot;n&quot; &quot;n_distinct&quot; ## [154] &quot;nth&quot; &quot;ntile&quot; &quot;order_by&quot; ## [157] &quot;percent_rank&quot; &quot;quantile&quot; &quot;rank&quot; ## [160] &quot;row_number&quot; &quot;sum&quot; &quot;var&quot; ## [163] &quot;cume_dist&quot; &quot;cummax&quot; &quot;cummean&quot; ## [166] &quot;cummin&quot; &quot;cumsum&quot; &quot;dense_rank&quot; ## [169] &quot;first&quot; &quot;lag&quot; &quot;last&quot; ## [172] &quot;lead&quot; &quot;max&quot; &quot;mean&quot; ## [175] &quot;median&quot; &quot;min&quot; &quot;min_rank&quot; ## [178] &quot;n&quot; &quot;n_distinct&quot; &quot;nth&quot; ## [181] &quot;ntile&quot; &quot;order_by&quot; &quot;percent_rank&quot; ## [184] &quot;quantile&quot; &quot;rank&quot; &quot;row_number&quot; ## [187] &quot;sum&quot; &quot;var&quot; sql_translate_env(simulate_dbi()) ## &lt;sql_variant&gt; ## scalar: -, :, !, !=, (, [, [[, {, *, /, &amp;, &amp;&amp;, %%, %&gt;%, %in%, ## scalar: ^, +, &lt;, &lt;=, ==, &gt;, &gt;=, |, ||, $, abs, acos, as_date, ## scalar: as_datetime, as.character, as.Date, as.double, ## scalar: as.integer, as.integer64, as.logical, as.numeric, ## scalar: as.POSIXct, asin, atan, atan2, between, bitwAnd, ## scalar: bitwNot, bitwOr, bitwShiftL, bitwShiftR, bitwXor, c, ## scalar: case_when, ceil, ceiling, coalesce, cos, cosh, cot, ## scalar: coth, day, desc, exp, floor, hour, if, if_else, ifelse, ## scalar: is.na, is.null, log, log10, mday, minute, month, na_if, ## scalar: nchar, now, paste, paste0, pmax, pmin, qday, round, ## scalar: second, sign, sin, sinh, sql, sqrt, str_c, str_conv, ## scalar: str_count, str_detect, str_dup, str_extract, ## scalar: str_extract_all, str_flatten, str_glue, str_glue_data, ## scalar: str_interp, str_length, str_locate, str_locate_all, ## scalar: str_match, str_match_all, str_order, str_pad, ## scalar: str_remove, str_remove_all, str_replace, ## scalar: str_replace_all, str_replace_na, str_sort, str_split, ## scalar: str_split_fixed, str_squish, str_sub, str_subset, ## scalar: str_to_lower, str_to_title, str_to_upper, str_trim, ## scalar: str_trunc, str_view, str_view_all, str_which, str_wrap, ## scalar: substr, switch, tan, tanh, today, tolower, toupper, ## scalar: trimws, wday, xor, yday, year ## aggregate: cume_dist, cummax, cummean, cummin, cumsum, dense_rank, ## aggregate: first, lag, last, lead, max, mean, median, min, ## aggregate: min_rank, n, n_distinct, nth, ntile, order_by, ## aggregate: percent_rank, quantile, rank, row_number, sum, var ## window: cume_dist, cummax, cummean, cummin, cumsum, dense_rank, ## window: first, lag, last, lead, max, mean, median, min, ## window: min_rank, n, n_distinct, nth, ntile, order_by, ## window: percent_rank, quantile, rank, row_number, sum, var source(here(&quot;book-src&quot;, &quot;dbplyr-sql-function-translation.R&quot;)) ## Warning: The `.drop` argument of `unnest()` is deprecated as of tidyr 1.0.0. ## All list-columns are now preserved. ## This warning is displayed once per session. ## Call `lifecycle::last_warnings()` to see where this warning was generated. Each of the following dbplyr back ends may have a slightly different translation: translations %&gt;% filter(!is.na(sql)) %&gt;% count(variant) ## # A tibble: 11 x 2 ## variant n ## &lt;chr&gt; &lt;int&gt; ## 1 access 193 ## 2 dbi 183 ## 3 hive 187 ## 4 impala 190 ## 5 mssql 196 ## 6 mysql 194 ## 7 odbc 186 ## 8 oracle 184 ## 9 postgres 204 ## 10 sqlite 183 ## 11 teradata 196 Only one postgres translation produces an output: psql &lt;- translations %&gt;% filter(!is.na(sql), variant == &quot;postgres&quot;) %&gt;% select(r, n_args, sql) %&gt;% arrange(r) # sp_print_df(head(psql, n = 40)) sp_print_df(psql) "],
["chapter-appendix-additional-resources.html", "I Appendix Additional resources I.1 Editing this book I.2 Docker alternatives I.3 Docker and R I.4 Documentation for Docker and PostgreSQL I.5 SQL and dplyr I.6 More Resources", " I Appendix Additional resources I.1 Editing this book Here are instructions for editing this book I.2 Docker alternatives Choosing between Docker and Vagrant (Zait 2017) I.3 Docker and R Noam Ross’ talk on Docker for the UseR (Ross 2018b) and his Slides (Ross 2018a) give a lot of context and tips. Good Docker tutorials An introductory Docker tutorial (Srivastav 2018) A Docker curriculum (Hall 2018) Scott Came’s materials about Docker and R on his website (Came 2018) and at the 2018 UseR Conference focus on R inside Docker. It’s worth studying the ROpensci Docker tutorial (ROpenSciLabs 2018) I.4 Documentation for Docker and PostgreSQL The Postgres image documentation (Docker 2018d) PostgreSQL &amp; Docker documentation (Docker 2018d) Dockerize PostgreSQL (Docker 2018b) Usage examples of PostgreSQL with Docker WARNING-EXPIRED CERTIFICATE 2018-12-20 I.5 SQL and dplyr Why SQL is not for analysis but dplyr is (Nishida 2016) Data Manipulation with dplyr (With 50 Examples) (ListenData.com 2016) I.6 More Resources David Severski describes some key elements of connecting to databases with R for MacOS users (Severski 2018) This tutorial picks up ideas and tips from Ed Borasky’s Data Science pet containers (Borasky 2018), which creates a framework based on that Hack Oregon example and explains why this repo is named pet-sql. References "],
["references.html", "References", " References "]
]
