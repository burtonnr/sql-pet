[
["index.html", "Exploring Enterprise Databases with R: A Tidyverse Approach Chapter 1 Introduction 1.1 Using R to query a DBMS in your organization 1.2 Docker as a tool for UseRs 1.3 Alternatives to Docker 1.4 Packages used in this book 1.5 Who are we? 1.6 How did this project come about? 1.7 Navigation", " Exploring Enterprise Databases with R: A Tidyverse Approach John David Smith, Sophie Yang, M. Edward (Ed) Borasky, Jim Tyhurst, Scott Came, Mary Anne Thygesen, and Ian Frantz 2020-01-27 Chapter 1 Introduction This chapter introduces: The motivation for this book and the strategies we have adopted Our approach to exploring issues “behind the enterprise firewall” using Docker to demonstrate access to a service like PostgreSQL from R Our team and how this project came about 1.1 Using R to query a DBMS in your organization Many R users (or useRs) live a dual life: in the vibrant open-source R community where R is created, improved, discussed, and taught. And then they go to work in a secured, complex, closed organizational environment where they may be on their own. Here is a request on the Rstudio community site for help that has been lightly edited to emphasize the generality that we see: I’m trying to migrate some inherited scripts that […] to connect to a […] database to […] instead. I’ve reviewed the https://db.rstudio.com docs and tried a number of configurations but haven’t been able to connect. I’m in uncharted territory within my org, so haven’t been able to get much help internally. This book will help you create a hybrid environment on your machine that can mimic some of the uncharted territory in your organization. It goes far beyond the basic connection issues and covers issues that you face when you are finding your way around or writing queries to your organization’s databases, not just when maintaining inherited scripts. Technology hurdles. The interfaces (passwords, packages, etc.) and gaps between R and a back end database are hidden from public view as a matter of security, so pinpointing exactly where a problem is can be difficult. A simulated environment such as we offer here can be an important learning resource. Scale issues. We see at least two types of scale issues. Handling large volumes of data so that performance issues must be a consideration requires a basic understanding of what’s happening in “the back end” (which is necessarily hidden from view). Therefore mastering techniques for drawing samples or small batches of data are essential. In addition to their size, your organization’s databases will often have structural characteristics that are complex and obscure. Data documentation is often incomplete and emphasizes operational characteristics, rather than analytic opportunities. A careful useR often needs to confirm the documentation on the fly and de-normalize data carefully. Use cases. R users frequently need to make sense of an organization’s complex data structures and coding schemes to address incompletely formed questions so that informal exploratory data analysis has to be intuitive and fast. The technology details should not get in the way. Sharing and discussing exploratory and diagnostic retrieval techniquesis best in public, but is constrained by organizational requirements. We have found that PostgreSQL in a Docker container solves many of the foregoing problems. 1.2 Docker as a tool for UseRs Noam Ross’s “Docker for the UseR” (Ross 2018a) suggests that there are four distinct Docker use-cases for useRs. Make a fixed working environment for reproducible analysis Access a service outside of R (e.g., PostgreSQL) Create an R based service (e.g., with plumber) Send our compute jobs to the cloud with minimal reconfiguration or revision This book explores #2 because it allows us to work on the database access issues described above and to practice on an industrial-scale DBMS. Docker is a comparatively easy way to simulate the relationship between an R/RStudio session and a database – all on on your machine (provided you have Docker installed and running). Running PostgreSQL on a Docker container avoids OS or system dependencies or conflicts that cause confusion and limit reproducibility. A Docker environment consumes relatively few resources. Our sandbox does much less but only includes PostgreSQL and sample data, so it takes up about 5% of the space taken up by the Vagrant environment that inspired this project. (Makubuya 2018) A simple Docker container such as the one used in our sandbox is easy to use and could be extended for other uses. Docker is a widely used technology for deploying applications in the cloud, so for many useRs it’s worth mastering. 1.3 Alternatives to Docker We have found Docker to be a great tool for simulating the complexities of an enterprise environment. However, installing Docker can be challenging, especially for Windows users. Therefore the code in this book depends on PostgreSQL(Group 2019) in a Docker container, but it can all be readily adapted to either SQLite(Consortium 2019), PostgreSQL running natively on your computer, or even PostgreSQL running in the cloud. The technical details of these alternatives are all in separate chapters. 1.4 Packages used in this book The following packages are used in this book: bookdown DBI dbplyr devtools DiagrammeR downloader glue here knitr RPostgres skimr sqlpetr (installs with: remotes::install_github(&quot;smithjd/sqlpetr&quot;, force = TRUE, quiet = TRUE, build = TRUE, build_opts = &quot;&quot;)) tidyverse Note that when you install sqlpetr, it will install all the other packages you need as dependencies. 1.5 Who are we? We have been collaborating on this book since the Summer of 2018, each of us chipping into the project as time permits: Ian Franz - @ianfrantz Jim Tyhurst - @jimtyhurst John David Smith - @smithjd M. Edward (Ed) Borasky - @znmeb Maryanne Thygesen @maryannet Scott Came - @scottcame Sophie Yang - @SophieMYang 1.6 How did this project come about? We trace this book back to the June 2, 2018 Cascadia R Conf where Aaron Makubuya gave a presentation using Vagrant hosting (Makubuya 2018). After that John Smith, Ian Franz, and Sophie Yang had discussions after the monthly Data Discussion Meetups about the difficulties around setting up Vagrant (a virtual environment), connecting to an enterprise database, and having realistic public environment to demo or practice the issues that come up behind corporate firewalls. Scott Came’s tutorial on R and Docker (Came 2018) (an alternative to Vagrant) at the 2018 UseR Conference in Melbourne was provocative and it turned out he lived nearby. We re-connected with M. Edward (Ed) Borasky who had done extensive development for a Hack Oregon data science containerization project (Borasky 2018). 1.7 Navigation If this is the first bookdown (Xie 2016) book you’ve read, here’s how to navigate the website. The controls on the upper left: there are four controls on the upper left. A “hamburger” menu: this toggles the table of contents on the left side of the page on or off. A magnifying glass: this toggles a search box on or off. A letter “A”: this lets you pick how you want the site to display. You have your choice of small or large text, a serif or sans-serif font, and a white, sepia or night theme. A pencil: this is the “Edit” button. This will take you to a GitHub edit dialog for the chapter you’re reading. If you’re a committer to the repository, you’ll be able to edit the source directly. If not, GitHub will fork a copy of the repository to your own account and you’ll be able to edit that version. Then you can make a pull request. The share buttons in the upper right hand corner. There’s one for Twitter, one for Facebook, and one that gives a menu of options, including LinkedIn. References "],
["chapter-how-to-use-this-book.html", "Chapter 2 How to use this book 2.1 Retrieve the code from GitHub 2.2 Read along, experiment as you go 2.3 Participating", " Chapter 2 How to use this book This chapter explains: Getting the code used in this book How you can contribute to the book project This book is full of examples that you can replicate on your computer. 2.1 Retrieve the code from GitHub The code to generate the book and the exercises it contains can be downloaded from this repo. 2.2 Read along, experiment as you go We have never been sure whether we’re writing an expository book or a massive tutorial. You may use it either way. The best way to learn the material we cover is to experiment. After the introductory chapters and the chapter that creates the persistent database, you can jump around and each chapter stands on its own. 2.3 Participating 2.3.1 Browsing the book If you just want to read the book and copy / paste code into your working environment, simply browse to https://smithjd.github.io/sql-pet. If you get stuck, or find things aren’t working, open an issue at https://github.com/smithjd/sql-pet/issues/new/. 2.3.2 Diving in If you want to experiment with the code in the book, run it in RStudio and interact with it, you’ll need to do two more things: Install the sqlpetr R package (Borasky et al. 2018). See https://smithjd.github.io/sqlpetr for the package documentation. Installation may take some time if it has to install or update packages not available on your computer. Clone the Git repository https://github.com/smithjd/sql-pet.git and open the project file sql-pet.Rproj in RStudio. Enjoy! References "],
["chapter-learning-goals.html", "Chapter 3 Chapter Learning Goals and Use Cases 3.1 The Book’s Challenge: goals, context and expectations 3.2 Making your way through the book 3.3 Adventure Works", " Chapter 3 Chapter Learning Goals and Use Cases This chapter sets the context for the book by: Describing our assumptions about the reader of this book: the challenges you face, your R skills, your learning goals, and context. Describing what the book offers in terms of: Problems that are addressed Learning objectives Sequence of topics, ranging from connecting to the database to exploring an issue in response to questions from an executive R packages used Describing the sample database used in the book 3.1 The Book’s Challenge: goals, context and expectations Working with the data that’s behind the enterprise firewall is challenging in a unique way. Most of us R users are accustomed to a vast learning community that shares resources, discusses methods in public, and that can help each other trouble-shoot a problem. The very necessary enterprise firewall makes all of that difficult, if not impossible. And yet enterprise database environment is very important because in so many cases that’s where the data (and possibly your paycheck) are coming from. Differences between production and data warehouse environments. We are simulating a production environment. There are many similarities. Data models are different. Performance is a bigger deal in the OLTP. Data in a organizational environment around the database. Learning to keep your DBAs happy: You are your own DBA in this simulation, so you can wreak havoc and learn from it, but you can learn to be DBA-friendly here. In the end it’s the subject-matter experts (people using the data every day) that really understand your data, but you have to work with your DBAs first. You can’t believe all the data you pull out of the database. 3.1.1 The Challenge: Investigating a question using an organization’s database Using an enterprise database to create meaningful management insights requires a combination of very different skills: Need both familiarity with the data and a focus question An iterative process where the data resource can shape your understanding of the question the question you need to answer will frame how you see the data resource You need to go back and forth between the two, asking do I understand the question? do I understand the data? A “good enough” understanding of the data resource (in the DBMS) Nobody knows everything about an entire organization’s data resources. We do, however, need to know what more we need to know and estimate what we don’t know yet. Use all available documentation and understand its limits Use your own tools and skills to examine the data resource What is missing from the database: (columns, records, cells) Why is the data missing? A “good enough” understanding of the question you seek to answer How general or specific is your question? How aligned is it with the purpose for which the database was designed and is being operated? How different are your assumptions and concerns from those of the people who enter and use the data on a day to day basis? Some cycles in this iteration between question refinement and reformulation on the one hand and data retrieval and investigation on the other feel like a waste time. That’s inevitable. Bringing R tools and skills to bear on these R is a powerful tool for data access, manipulation, modeling and presentation Different R packages and techniques are available for each of the elements involved in exploring, analyzing and reporting on enterprise behavior using the enterprise database. 3.1.2 Strategies Local, idiosyncratic optimization (entry and use of data). For example, different individuals might code a variable differently. Drifting use / bastardization of a column Turf wars and acquisitions Partial recollection / history: find the people who know where the skeletons are 3.1.3 Problems that we address in the book This book emphasizes database exploration and the R techniques that are needed. We are emphasizing a tidyverse approach. &amp; graphics to really makes sense of what we find. We can’t call on real people in the adventureworks company, obviously, but we invent some characters to illustrate the investigation process as we have experienced it in various organizational settings. 3.1.4 Signposts Practice Tips Here’s how we do it. + Conventions like always using the labs() function in ggplot + Specifying the package the first time a function is used 3.1.5 Book structure The book explores R techniques and and investigation strategies using progressively more complex queries, that lead to this scenario: There is a new Executive VP of Sales at Adventure Works. She wants an overview of sales and the sales organization’s performance at Adventure Works. Once her questions are satisfied, a monthly report is developed that can run automatically and appear in her mailbox. Early chapters demonstrate now to connect to a database and find your way around it, with a pause to discuss how to secure your credentials. Both Rstudio and R script methods are shown for the same database overview. The salesordedrheader table in the sales schema is used to demonstrate packages and functions that show what a single table contains. Then the same table is used but the investigation adopts a business perspective, demonstrating R techniques that are motivated by questions like “How sales for the Adventure Works company?” Starting with base tables, then use views (that contain knowledge about the application) More involved queries join three tables in three different schemas: salesperson, employee, and person. The relevant question might be “Who is my top salesperson? Are the 3 top salespersons older or younger?” Finally, we build a series of queries that explore the sales workflow: sales territories, sales people, top customers by product, product mixture that gives top 80% of sales. What are they producing in detail? Seasonal? Type of product, region, etc.? The book ends by demonstrating how R code can be used for standard reports from the database that are emailed to a list of recipients. 3.2 Making your way through the book After working through the code in this book, you can expect to be able to: R, SQL and PostgreSQL Run queries against PostgreSQL in an environment that simulates what is found in a enterprise setting. Understand techniques and some of the trade-offs between: queries aimed at exploration or informal investigation using dplyr (Wickham 2018); and queries that should be written in SQL, because performance is important due to the size of the database or the frequency with which a query is to be run. Understand the equivalence between dplyr and SQL queries, and how R translates one into the other. Gain familiarity with techniques that help you explore a database and verify its documentation. Gain familiarity with the standard metadata that a SQL database contains to describe its own contents. Understand some advanced SQL techniques. Gain some understanding of techniques for assessing query structure and performance. Docker related Set up a PostgreSQL database in a Docker environment. Gain familiarity with the various ways of interacting with the Docker and PostgreSQL environments Understand enough about Docker to swap databases, e.g. Sports DB for the DVD rental database used in this tutorial. Or swap the database management system (DBMS), e.g. MySQL for PostgreSQL. 3.2.1 R Packages These R packages are discussed or used in exercises: DBI dbplyr devtools downloader glue gt here knitr RPostgres skimr sqlpetr (installs with: remotes::install_github(&quot;smithjd/sqlpetr&quot;, force = TRUE, quiet = TRUE, build = TRUE, build_opts = &quot;&quot;)) tidyverse In addition, these are used to render the book: * bookdown * DiagrammeR 3.3 Adventure Works In this book we have adopted the Microsoft Adventure Works online transaction processing database for our examples. It is https://docs.microsoft.com/en-us/previous-versions/sql/sql-server-2008/ms124438(v=sql.100) See Sections 3 and 4 Journal of Information Systems Education, Vol. 26(3) Summer 2015. “Teaching Tip Active Learning via a Sample Database: The Case of Microsoft’s Adventure Works” by Michel Mitri http://jise.org/Volume26/n3/JISEv26n3p177.pdf See the AdventureWorks Data Dictionary and a sample table (employee). Here is a (link to an ERD diagram)[https://i.stack.imgur.com/LMu4W.gif] References "],
["chapter-setup-adventureworks-db.html", "Chapter 4 Create and connect to the adventureworks database in PostgreSQL 4.1 Overview 4.2 Verify that Docker is up, running, and clean up if necessary 4.3 Clean up if appropriate 4.4 Build the adventureworks Docker image 4.5 Run the adventureworks Docker Image 4.6 Connect to PostgreSQL 4.7 Adventureworks Schemas 4.8 Investigate the database using Rstudio 4.9 Cleaning up: diconnect from the database and stop Docker 4.10 Using the adventureworks container in the rest of the book", " Chapter 4 Create and connect to the adventureworks database in PostgreSQL This chapter demonstrates how to: Create and connect to the PostgreSQL adventureworks database in Docker Keep necessary credentials secret while being available to R when it executes. Leverage Rstudio features to get an overview of the database Set up the environment for subsequent chapters 4.1 Overview Docker commands can be run from a terminal (e.g., the Rstudio Terminal pane) or with a system2() command. The necessary functions to start, stop Docker containers and do other busy work are provided in the sqlpetr package. Note: The functions in the package are designed to help you focus on interacting with a dbms from R. You can ignore how they work until you are ready to delve into the details. They are all named to begin with sp_. The first time a function is called in the book, we provide a note explaining its use. Please install the sqlpetr package if not already installed: library(devtools) if (!require(sqlpetr)) { remotes::install_github( &quot;smithjd/sqlpetr&quot;, force = TRUE, build = FALSE, quiet = TRUE) } Note that when you install this package the first time, it will ask you to update the packages it uses and that may take some time. These packages are called in this Chapter: library(tidyverse) library(DBI) library(RPostgres) library(glue) require(knitr) library(dbplyr) library(sqlpetr) library(bookdown) library(here) library(connections) sleep_default &lt;- 3 theme_set(theme_light()) 4.2 Verify that Docker is up, running, and clean up if necessary The sp_check_that_docker_is_up function from the sqlpetr package checks whether Docker is up and running. If it’s not, then you need to install, launch or re-install Docker. sp_check_that_docker_is_up() ## [1] &quot;Docker is up, running these containers:&quot; ## [2] &quot;CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES&quot; ## [3] &quot;611b69c981a7 postgres:11 \\&quot;docker-entrypoint.s…\\&quot; 3 days ago Up About a minute 0.0.0.0:5432-&gt;5432/tcp adventureworks&quot; 4.3 Clean up if appropriate Force-remove the adventureworks container if it was left over (e.g., from a prior runs): sp_docker_remove_container(&quot;adventureworks&quot;) ## [1] 0 4.4 Build the adventureworks Docker image Now we set up a “realistic” database named adventureworks in Docker. NOTE: This chapter doesn’t go into the details of creating or restoring the adventureworks database. For more detail on what’s going on behind the scenes, you can examine the step-by-step code in: source('book-src/restore-adventureworks-postgres-on-docker.R') To save space here in the book, we’ve created a function in sqlpetr to build this image, called OUT OF DATE!! . Vignette Building the adventureworks Docker Image describes the build process. *Ignore the errors in the following step: source(here(&quot;book-src&quot;, &quot;restore-adventureworks-postgres-on-docker.R&quot;)) ## docker run --detach --name adventureworks --publish 5432:5432 --mount type=bind,source=&quot;/Users/jds/Documents/Library/R/r-system/sql-pet&quot;,target=/petdir postgres:11 Sys.sleep(sleep_default * 2) 4.5 Run the adventureworks Docker Image Now we can run the image in a container and connect to the database. To run the image we use an sqlpetr function called OUT OF DATE sp_pg_docker_run For the rest of the book we will assume that you have a Docker container called adventureworks that can be stopped and started. In that sense each chapter in the book is independent. sp_docker_start(&quot;adventureworks&quot;) 4.6 Connect to PostgreSQL *CHECK for sqlpetr update!Thesp_make_simple_pgfunction we called above created a container from thepostgres:11library image downloaded from Docker Hub. As part of the process, it set the password for the PostgreSQL database superuserpostgres` to the value “postgres”. For simplicity, we are using a weak password at this point and it’s shown here and in the code in plain text. That is bad practice because user credentials should not be shared in open code like that. A subsequent chapter demonstrates how to store and use credentials to access the DBMS so that they are kept private. The sp_get_postgres_connection function from the sqlpetr package gets a DBI connection string to a PostgreSQL database, waiting if it is not ready. This function connects to an instance of PostgreSQL and we assign it to a symbol, con, for subsequent use. The connctions_tab = TRUE parameter opens a connections tab that’s useful for navigating a database. Note that we are using port 5439 for PostgreSQL inside the container and published to localhost. Why? If you have PostgreSQL already running on the host or another container, it probably claimed port 5432, since that’s the default. So we need to use a different port for our PostgreSQL container. Use the DBI package to connect to the adventureworks database in PostgreSQL. Remember the settings discussion about [keeping passwords hidden][Pause for some security considerations] Sys.sleep(sleep_default) # con &lt;- connection_open( # use in an interactive session con &lt;- dbConnect( # use in other settings RPostgres::Postgres(), # without the previous and next lines, some functions fail with bigint data # so change int64 to integer bigint = &quot;integer&quot;, host = &quot;localhost&quot;, port = 5432, # this version still using 5432!!! user = &quot;postgres&quot;, password = &quot;postgres&quot;, dbname = &quot;adventureworks&quot; ) 4.7 Adventureworks Schemas Think of the Adventureworks database as a model of the Adventureworks business. The business is organized around different departments (humanresources, sales, and purchasing), business processes (production), and resources (person). Each schema is a container for the all the database objects needed to model the departments, business processes, and resources. As a data analyst, the connections tab has three of the five database objects of interest. These are schemas, tables and views. The other two database objects of interest not shown in the connetions tab are the table primary and foreign keys, PK and FK. Those database objects enforce the referential integrity of the data and the performance of the application. Let the DBA’s worry about them. The Connections tab has three icons. The node icon represents a schema. The schema helps organize the structure and design of the database. The schema contains the views, the grid with the glasses, and tables, the grids without the glasses, that are of interest to the data analyst. A table is a database object usually represents something useful to a business process. For example, a sales person may enter a new order. The first screen is typically called the sales order header screen which contains information about the customer placing the order. This information is captured in salesorderheader table. The customers ordered items are typically entered via multiple screens. These are captured in the salesorderdetail table. A view is a database object that maybe a subset of either the columns or rows of a single table. For example, the customer table has information on all the customers, but the customer view, c, shows only a single customer. Or a view may have data from a primary/driving table and joined to other tables to provide a better understanding/view of the information in the primary table. For example, the primary table typically has a primary key column, PK, and zero or more foreign key columns, FK. The PK and FK are usually an integer which is great for a computer, but not so nice us mere mortals. An extended view pulls information associated with the FK. For example a sales order view a customer foreign key, can show the actual customer name. 4.8 Investigate the database using Rstudio The Rstudio Connections tab shows that you are connected to Postgres and that the adventureworks database has a many schemas each of which has multiple tables and views in it. The drop-down icon to the left of a table lists the table’s columns. Connections tab - adventureworks Clicking on the icon to the left of a schema expands the list of tables and views in that schema. Clicking on the View or Table icon opens up Rstudio’s View pane to get a peek at the data: View of employee table The number of rows and columns shown in the View pane depends on the size of the window. 4.9 Cleaning up: diconnect from the database and stop Docker Always have R disconnect from the database when you’re done. dbDisconnect(con) # or if using the connections package, use: # connection_close(con) Stop the adventureworks container: sp_docker_stop(&quot;adventureworks&quot;) Show that the container still exists even though it’s not running sp_show_all_docker_containers() ## CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES ## 37f05f6c5c62 postgres:11 &quot;docker-entrypoint.s…&quot; 26 seconds ago Exited (0) Less than a second ago adventureworks Next time, you can just use this command to start the container: sp_docker_start(&quot;adventureworks&quot;) And once stopped, the container can be removed with: sp_check_that_docker_is_up(&quot;adventureworks&quot;) 4.10 Using the adventureworks container in the rest of the book After this point in the book, we assume that Docker is up and that we can always start up our adventureworks database with: sp_docker_start(&quot;adventureworks&quot;) "],
["chapter-dbms-login-credentials.html", "Chapter 5 Securing and using your dbms log-in credentials 5.1 Set up the adventureworks Docker container 5.2 Storing your dbms credentials 5.3 Disconnect from the database and stop Docker", " Chapter 5 Securing and using your dbms log-in credentials This chapter demonstrates how to: Keep necessary credentials secret or at least invisible Interact with PostgreSQL using your stored dbms credentials Connecting to a dbms can be very frustrating at first. In many organizations, simply getting access credentials takes time and may involve jumping through multiple hoops. In addition, a dbms is terse or deliberately inscrutable when your credetials are incorrect. That’s a security strategy, not a limitation of your understanding or of your software. When R can’t log you on to a dbms, you usually will have no information as to what went wrong. There are many different strategies for managing credentials. See Securing Credentials in RStudio’s Databases using R documentation for some alternatives to the method we adopt in this book. We provide more details about PostgreSQL Authentication in our sandbox environment in an appendix. The following packages are used in this chapter: library(tidyverse) library(DBI) library(RPostgres) require(knitr) library(sqlpetr) library(connections) sleep_default &lt;- 3 theme_set(theme_light()) 5.1 Set up the adventureworks Docker container 5.1.1 Verify that Docker is running Check that Docker is up and running: sp_check_that_docker_is_up() ## [1] &quot;Docker is up but running no containers&quot; 5.1.2 Start the Docker container: Start the adventureworks Docker container: sp_docker_start(&quot;adventureworks&quot;) 5.2 Storing your dbms credentials In previous chapters the connection string for connecting to the dbms has used default credentials specified in plain text as follows: user= 'postgres', password = 'postgres' When we call sp_get_postgres_connection below we’ll use environment variables that R obtains from reading the .Renviron file when R starts up. This approach has two benefits: that file is not uploaded to GitHub and R looks for it in your default directory every time it loads. To see whether you have already created that file, use the R Studio Files tab to look at your home directory: That file should contain lines that look like the example below. Although in this example it contains the PostgreSQL default values for the username and password, they are obviously not secret. But this approach demonstrates where you should put secrets that R needs while not risking accidental uploaded to GitHub or some other public location.. Open your .Renviron file with this command: file.edit(&quot;~/.Renviron&quot;) Or you can execute define_postgresql_params.R to create the file or you could copy / paste the following into your .Renviron file: DEFAULT_POSTGRES_PASSWORD=postgres DEFAULT_POSTGRES_USER_NAME=postgres Once that file is created, restart R, and after that R reads it every time it comes up. 5.2.1 Connect with Postgres using the Sys.getenv function Connect to the postgrSQL using the sp_get_postgres_connection function: Sys.sleep(sleep_default) # con &lt;- connection_open( # use in an interactive session con &lt;- dbConnect( # use in other settings RPostgres::Postgres(), # without the previous and next lines, some functions fail with bigint data # so change int64 to integer bigint = &quot;integer&quot;, user = Sys.getenv(&quot;DEFAULT_POSTGRES_USER_NAME&quot;), password = Sys.getenv(&quot;DEFAULT_POSTGRES_PASSWORD&quot;), host = &quot;localhost&quot;, port = 5432, dbname = &quot;adventureworks&quot;) Once the connection object has been created, you can list all of the tables in one of the schemas: dbExecute(con, &quot;set search_path to humanresources, public;&quot;) # watch for duplicates! ## [1] 0 dbListTables(con) ## [1] &quot;employee&quot; &quot;shift&quot; ## [3] &quot;employeepayhistory&quot; &quot;jobcandidate&quot; ## [5] &quot;department&quot; &quot;vemployee&quot; ## [7] &quot;vemployeedepartment&quot; &quot;vemployeedepartmenthistory&quot; ## [9] &quot;vjobcandidate&quot; &quot;vjobcandidateeducation&quot; ## [11] &quot;vjobcandidateemployment&quot; &quot;employeedepartmenthistory&quot; 5.3 Disconnect from the database and stop Docker dbDisconnect(con) # or if using the connections package, use: # connection_close(con) sp_docker_stop(&quot;adventureworks&quot;) "],
["chapter-connect-to-db-with-r-code.html", "Chapter 6 Connecting to the database with R code 6.1 Verify that Docker is up and running, and start the database 6.2 Connect to PostgreSQL 6.3 Set schema search path and list its contents 6.4 Anatomy of a dplyr connection object 6.5 Disconnect from the database and stop Docker", " Chapter 6 Connecting to the database with R code This chapter demonstrates how to: Connect to and disconnect R from the adventureworks database Use dplyr to get an overview of the database, replicating the facilities provided by RStudio These packages are called in this Chapter: library(tidyverse) library(DBI) library(RPostgres) library(glue) require(knitr) library(dbplyr) library(sqlpetr) library(bookdown) library(here) library(connections) sleep_default &lt;- 3 6.1 Verify that Docker is up and running, and start the database The sp_check_that_docker_is_up function from the sqlpetr package checks whether Docker is up and running. If it’s not, then you need to install, launch or re-install Docker. sp_check_that_docker_is_up() ## [1] &quot;Docker is up but running no containers&quot; sp_docker_start(&quot;adventureworks&quot;) 6.2 Connect to PostgreSQL *CHECK for sqlpetr update!Thesp_make_simple_pgfunction we called above created a container from thepostgres:11library image downloaded from Docker Hub. As part of the process, it set the password for the PostgreSQL database superuserpostgres` to the value “postgres”. For simplicity, we are using a weak password at this point and it’s shown here and in the code in plain text. That is bad practice because user credentials should not be shared in open code like that. A subsequent chapter demonstrates how to store and use credentials to access the DBMS so that they are kept private. The sp_get_postgres_connection function from the sqlpetr package gets a DBI connection string to a PostgreSQL database, waiting if it is not ready. This function connects to an instance of PostgreSQL and we assign it to a symbol, con, for subsequent use. The connctions_tab = TRUE parameter opens a connections tab that’s useful for navigating a database. Note that we are using port 5439 for PostgreSQL inside the container and published to localhost. Why? If you have PostgreSQL already running on the host or another container, it probably claimed port 5432, since that’s the default. So we need to use a different port for our PostgreSQL container. Use the DBI package to connect to the adventureworks database in PostgreSQL. Remember the settings discussion about [keeping passwords hidden][Pause for some security considerations] # con &lt;- connection_open( # use in an interactive session Sys.sleep(sleep_default) con &lt;- dbConnect( RPostgres::Postgres(), # without the previous and next lines, some functions fail with bigint data # so change int64 to integer bigint = &quot;integer&quot;, host = &quot;localhost&quot;, port = 5432, user = &quot;postgres&quot;, password = &quot;postgres&quot;, dbname = &quot;adventureworks&quot;) 6.3 Set schema search path and list its contents Schemas will be discussed later on because multiple schemas are the norm in an enterprise database environment, but they are a side issue at this point. So we switch the order in which PostgreSQL searches for objects with the following SQL code: dbExecute(con, &quot;set search_path to sales;&quot;) ## [1] 0 With the custom search_path, the following command shows the tables in the sales schema. In the adventureworks database, there are no tables in the public schema. dbListTables(con) ## [1] &quot;countryregioncurrency&quot; &quot;customer&quot; ## [3] &quot;currencyrate&quot; &quot;creditcard&quot; ## [5] &quot;personcreditcard&quot; &quot;specialoffer&quot; ## [7] &quot;specialofferproduct&quot; &quot;salesorderheadersalesreason&quot; ## [9] &quot;shoppingcartitem&quot; &quot;salespersonquotahistory&quot; ## [11] &quot;salesperson&quot; &quot;currency&quot; ## [13] &quot;store&quot; &quot;salesorderheader&quot; ## [15] &quot;salesorderdetail&quot; &quot;salesreason&quot; ## [17] &quot;salesterritoryhistory&quot; &quot;vindividualcustomer&quot; ## [19] &quot;vpersondemographics&quot; &quot;vsalesperson&quot; ## [21] &quot;vsalespersonsalesbyfiscalyears&quot; &quot;vsalespersonsalesbyfiscalyearsdata&quot; ## [23] &quot;vstorewithaddresses&quot; &quot;vstorewithcontacts&quot; ## [25] &quot;vstorewithdemographics&quot; &quot;salestaxrate&quot; ## [27] &quot;salesterritory&quot; Notice there are several tables that start with the letter v: they are actually views which will turn out to be important. They are clearly distinguished in the connections tab, but the naming is a matter of convention. Same for dbListFields: dbListFields(con, &quot;salesorderheader&quot;) ## [1] &quot;salesorderid&quot; &quot;revisionnumber&quot; &quot;orderdate&quot; ## [4] &quot;duedate&quot; &quot;shipdate&quot; &quot;status&quot; ## [7] &quot;onlineorderflag&quot; &quot;purchaseordernumber&quot; &quot;accountnumber&quot; ## [10] &quot;customerid&quot; &quot;salespersonid&quot; &quot;territoryid&quot; ## [13] &quot;billtoaddressid&quot; &quot;shiptoaddressid&quot; &quot;shipmethodid&quot; ## [16] &quot;creditcardid&quot; &quot;creditcardapprovalcode&quot; &quot;currencyrateid&quot; ## [19] &quot;subtotal&quot; &quot;taxamt&quot; &quot;freight&quot; ## [22] &quot;totaldue&quot; &quot;comment&quot; &quot;rowguid&quot; ## [25] &quot;modifieddate&quot; Thus with this search order, the following two produce identical results: tbl(con, in_schema(&quot;sales&quot;, &quot;salesorderheader&quot;)) %&gt;% head() ## # Source: lazy query [?? x 25] ## # Database: postgres [postgres@localhost:5432/adventureworks] ## salesorderid revisionnumber orderdate duedate ## &lt;int&gt; &lt;int&gt; &lt;dttm&gt; &lt;dttm&gt; ## 1 43659 8 2011-05-31 00:00:00 2011-06-12 00:00:00 ## 2 43660 8 2011-05-31 00:00:00 2011-06-12 00:00:00 ## 3 43661 8 2011-05-31 00:00:00 2011-06-12 00:00:00 ## 4 43662 8 2011-05-31 00:00:00 2011-06-12 00:00:00 ## 5 43663 8 2011-05-31 00:00:00 2011-06-12 00:00:00 ## 6 43664 8 2011-05-31 00:00:00 2011-06-12 00:00:00 ## # … with 21 more variables: shipdate &lt;dttm&gt;, status &lt;int&gt;, ## # onlineorderflag &lt;lgl&gt;, purchaseordernumber &lt;chr&gt;, accountnumber &lt;chr&gt;, ## # customerid &lt;int&gt;, salespersonid &lt;int&gt;, territoryid &lt;int&gt;, ## # billtoaddressid &lt;int&gt;, shiptoaddressid &lt;int&gt;, shipmethodid &lt;int&gt;, ## # creditcardid &lt;int&gt;, creditcardapprovalcode &lt;chr&gt;, currencyrateid &lt;int&gt;, ## # subtotal &lt;dbl&gt;, taxamt &lt;dbl&gt;, freight &lt;dbl&gt;, totaldue &lt;dbl&gt;, comment &lt;chr&gt;, ## # rowguid &lt;chr&gt;, modifieddate &lt;dttm&gt; tbl(con, &quot;salesorderheader&quot;) %&gt;% head() ## # Source: lazy query [?? x 25] ## # Database: postgres [postgres@localhost:5432/adventureworks] ## salesorderid revisionnumber orderdate duedate ## &lt;int&gt; &lt;int&gt; &lt;dttm&gt; &lt;dttm&gt; ## 1 43659 8 2011-05-31 00:00:00 2011-06-12 00:00:00 ## 2 43660 8 2011-05-31 00:00:00 2011-06-12 00:00:00 ## 3 43661 8 2011-05-31 00:00:00 2011-06-12 00:00:00 ## 4 43662 8 2011-05-31 00:00:00 2011-06-12 00:00:00 ## 5 43663 8 2011-05-31 00:00:00 2011-06-12 00:00:00 ## 6 43664 8 2011-05-31 00:00:00 2011-06-12 00:00:00 ## # … with 21 more variables: shipdate &lt;dttm&gt;, status &lt;int&gt;, ## # onlineorderflag &lt;lgl&gt;, purchaseordernumber &lt;chr&gt;, accountnumber &lt;chr&gt;, ## # customerid &lt;int&gt;, salespersonid &lt;int&gt;, territoryid &lt;int&gt;, ## # billtoaddressid &lt;int&gt;, shiptoaddressid &lt;int&gt;, shipmethodid &lt;int&gt;, ## # creditcardid &lt;int&gt;, creditcardapprovalcode &lt;chr&gt;, currencyrateid &lt;int&gt;, ## # subtotal &lt;dbl&gt;, taxamt &lt;dbl&gt;, freight &lt;dbl&gt;, totaldue &lt;dbl&gt;, comment &lt;chr&gt;, ## # rowguid &lt;chr&gt;, modifieddate &lt;dttm&gt; 6.4 Anatomy of a dplyr connection object As introduced in the previous chapter, the dplyr::tbl function creates an object that might look like a data frame in that when you enter it on the command line, it prints a bunch of rows from the dbms table. But it is actually a list object that dplyr uses for constructing queries and retrieving data from the DBMS. The following code illustrates these issues. The dplyr::tbl function creates the connection object that we store in an object named salesorderheader_table: salesorderheader_table &lt;- dplyr::tbl(con, in_schema(&quot;sales&quot;, &quot;salesorderheader&quot;)) %&gt;% select(-rowguid) %&gt;% rename(salesorderheader_details_updated = modifieddate) At first glance, it acts like a data frame when you print it, although it only prints 10 of the table’s 31,465 rows: salesorderheader_table ## # Source: lazy query [?? x 24] ## # Database: postgres [postgres@localhost:5432/adventureworks] ## salesorderid revisionnumber orderdate duedate ## &lt;int&gt; &lt;int&gt; &lt;dttm&gt; &lt;dttm&gt; ## 1 43659 8 2011-05-31 00:00:00 2011-06-12 00:00:00 ## 2 43660 8 2011-05-31 00:00:00 2011-06-12 00:00:00 ## 3 43661 8 2011-05-31 00:00:00 2011-06-12 00:00:00 ## 4 43662 8 2011-05-31 00:00:00 2011-06-12 00:00:00 ## 5 43663 8 2011-05-31 00:00:00 2011-06-12 00:00:00 ## 6 43664 8 2011-05-31 00:00:00 2011-06-12 00:00:00 ## 7 43665 8 2011-05-31 00:00:00 2011-06-12 00:00:00 ## 8 43666 8 2011-05-31 00:00:00 2011-06-12 00:00:00 ## 9 43667 8 2011-05-31 00:00:00 2011-06-12 00:00:00 ## 10 43668 8 2011-05-31 00:00:00 2011-06-12 00:00:00 ## # … with more rows, and 20 more variables: shipdate &lt;dttm&gt;, status &lt;int&gt;, ## # onlineorderflag &lt;lgl&gt;, purchaseordernumber &lt;chr&gt;, accountnumber &lt;chr&gt;, ## # customerid &lt;int&gt;, salespersonid &lt;int&gt;, territoryid &lt;int&gt;, ## # billtoaddressid &lt;int&gt;, shiptoaddressid &lt;int&gt;, shipmethodid &lt;int&gt;, ## # creditcardid &lt;int&gt;, creditcardapprovalcode &lt;chr&gt;, currencyrateid &lt;int&gt;, ## # subtotal &lt;dbl&gt;, taxamt &lt;dbl&gt;, freight &lt;dbl&gt;, totaldue &lt;dbl&gt;, comment &lt;chr&gt;, ## # salesorderheader_details_updated &lt;dttm&gt; However, notice that the first output line shows ??, rather than providing the number of rows in the table. Similarly, the next to last line shows: … with more rows, and 20 more variables: whereas the output for a normal tbl of this salesorderheader data would say: … with 31,455 more rows, and 20 more variables: So even though salesorderheader_table is a tbl, it’s also a tbl_PqConnection: class(salesorderheader_table) ## [1] &quot;tbl_PqConnection&quot; &quot;tbl_dbi&quot; &quot;tbl_sql&quot; &quot;tbl_lazy&quot; ## [5] &quot;tbl&quot; It is not just a normal tbl of data. We can see that from the structure of salesorderheader_table: str(salesorderheader_table, max.level = 3) ## List of 2 ## $ src:List of 2 ## ..$ con :Formal class &#39;PqConnection&#39; [package &quot;RPostgres&quot;] with 3 slots ## ..$ disco: NULL ## ..- attr(*, &quot;class&quot;)= chr [1:4] &quot;src_PqConnection&quot; &quot;src_dbi&quot; &quot;src_sql&quot; &quot;src&quot; ## $ ops:List of 4 ## ..$ name: chr &quot;select&quot; ## ..$ x :List of 2 ## .. ..$ x : &#39;ident_q&#39; chr &quot;sales.salesorderheader&quot; ## .. ..$ vars: chr [1:25] &quot;salesorderid&quot; &quot;revisionnumber&quot; &quot;orderdate&quot; &quot;duedate&quot; ... ## .. ..- attr(*, &quot;class&quot;)= chr [1:3] &quot;op_base_remote&quot; &quot;op_base&quot; &quot;op&quot; ## ..$ dots: list() ## ..$ args:List of 1 ## .. ..$ vars:List of 24 ## ..- attr(*, &quot;class&quot;)= chr [1:3] &quot;op_select&quot; &quot;op_single&quot; &quot;op&quot; ## - attr(*, &quot;class&quot;)= chr [1:5] &quot;tbl_PqConnection&quot; &quot;tbl_dbi&quot; &quot;tbl_sql&quot; &quot;tbl_lazy&quot; ... It has only two rows! The first row contains all the information in the con object, which contains information about all the tables and objects in the database. Here is a sample: salesorderheader_table$src$con@typnames$typname[387:418] ## [1] &quot;AccountNumber&quot; &quot;_AccountNumber&quot; ## [3] &quot;Flag&quot; &quot;_Flag&quot; ## [5] &quot;Name&quot; &quot;_Name&quot; ## [7] &quot;NameStyle&quot; &quot;_NameStyle&quot; ## [9] &quot;OrderNumber&quot; &quot;_OrderNumber&quot; ## [11] &quot;Phone&quot; &quot;_Phone&quot; ## [13] &quot;department&quot; &quot;_department&quot; ## [15] &quot;pg_toast_16439&quot; &quot;d&quot; ## [17] &quot;_d&quot; &quot;employee&quot; ## [19] &quot;_employee&quot; &quot;pg_toast_16450&quot; ## [21] &quot;e&quot; &quot;_e&quot; ## [23] &quot;employeedepartmenthistory&quot; &quot;_employeedepartmenthistory&quot; ## [25] &quot;edh&quot; &quot;_edh&quot; ## [27] &quot;employeepayhistory&quot; &quot;_employeepayhistory&quot; ## [29] &quot;pg_toast_16482&quot; &quot;eph&quot; ## [31] &quot;_eph&quot; &quot;jobcandidate&quot; The second row contains a list of the columns in the salesorderheader table, among other things: salesorderheader_table$ops$x$vars ## [1] &quot;salesorderid&quot; &quot;revisionnumber&quot; &quot;orderdate&quot; ## [4] &quot;duedate&quot; &quot;shipdate&quot; &quot;status&quot; ## [7] &quot;onlineorderflag&quot; &quot;purchaseordernumber&quot; &quot;accountnumber&quot; ## [10] &quot;customerid&quot; &quot;salespersonid&quot; &quot;territoryid&quot; ## [13] &quot;billtoaddressid&quot; &quot;shiptoaddressid&quot; &quot;shipmethodid&quot; ## [16] &quot;creditcardid&quot; &quot;creditcardapprovalcode&quot; &quot;currencyrateid&quot; ## [19] &quot;subtotal&quot; &quot;taxamt&quot; &quot;freight&quot; ## [22] &quot;totaldue&quot; &quot;comment&quot; &quot;rowguid&quot; ## [25] &quot;modifieddate&quot; salesorderheader_table holds information needed to get the data from the ‘salesorderheader’ table, but salesorderheader_table does not hold the data itself. In the following sections, we will examine more closely this relationship between the salesorderheader_table object and the data in the database’s ‘salesorderheader’ table. 6.5 Disconnect from the database and stop Docker dbDisconnect(con) # or if using the connections package, use: # connection_close(con) sp_docker_stop(&quot;adventureworks&quot;) "],
["chapter-dbms-queries-intro.html", "Chapter 7 Introduction to DBMS queries 7.1 Setup 7.2 Methods for downloading a single table 7.3 Translating dplyr code to SQL queries 7.4 Mixing dplyr and SQL 7.5 Examining a single table with R 7.6 Disconnect from the database and stop Docker 7.7 Additional reading", " Chapter 7 Introduction to DBMS queries This chapter demonstrates how to: Download all or part of a table from the DBMS, including different kinds of subsets See how dplyr code is translated into SQL commands and how they can be mixed Get acquainted with some useful functions and packages for investigating a single table Begin thinking about how to divide the work between your local R session and the DBMS 7.1 Setup The following packages are used in this chapter: library(tidyverse) library(DBI) library(RPostgres) library(dbplyr) require(knitr) library(bookdown) library(sqlpetr) library(skimr) library(connections) sleep_default &lt;- 3 Assume that the Docker container with PostgreSQL and the adventureworks database are ready to go. If not go back to [Chapter 6][#chapter_setup-adventureworks-db] sqlpetr::sp_docker_start(&quot;adventureworks&quot;) Sys.sleep(sleep_default) Connect to the database: # con &lt;- connection_open( # use in an interactive session con &lt;- dbConnect( # use in other settings RPostgres::Postgres(), # without the previous and next lines, some functions fail with bigint data # so change int64 to integer bigint = &quot;integer&quot;, host = &quot;localhost&quot;, user = Sys.getenv(&quot;DEFAULT_POSTGRES_USER_NAME&quot;), password = Sys.getenv(&quot;DEFAULT_POSTGRES_PASSWORD&quot;), dbname = &quot;adventureworks&quot;, port = 5432 ) 7.2 Methods for downloading a single table For the moment, assume you know something about the database and specifically what table you need to retrieve. We return to the topic of investigating the whole database later on. dbExecute(con, &quot;set search_path to sales, humanresources;&quot;) ## [1] 0 7.2.1 Read the entire table There are a few different methods of getting data from a DBMS, and we’ll explore the different ways of controlling each one of them. DBI::dbReadTable will download an entire table into an R tibble. salesorderheader_tibble &lt;- DBI::dbReadTable(con, &quot;salesorderheader&quot;) str(salesorderheader_tibble) ## &#39;data.frame&#39;: 31465 obs. of 25 variables: ## $ salesorderid : int 43659 43660 43661 43662 43663 43664 43665 43666 43667 43668 ... ## $ revisionnumber : int 8 8 8 8 8 8 8 8 8 8 ... ## $ orderdate : POSIXct, format: &quot;2011-05-31&quot; &quot;2011-05-31&quot; ... ## $ duedate : POSIXct, format: &quot;2011-06-12&quot; &quot;2011-06-12&quot; ... ## $ shipdate : POSIXct, format: &quot;2011-06-07&quot; &quot;2011-06-07&quot; ... ## $ status : int 5 5 5 5 5 5 5 5 5 5 ... ## $ onlineorderflag : logi FALSE FALSE FALSE FALSE FALSE FALSE ... ## $ purchaseordernumber : chr &quot;PO522145787&quot; &quot;PO18850127500&quot; &quot;PO18473189620&quot; &quot;PO18444174044&quot; ... ## $ accountnumber : chr &quot;10-4020-000676&quot; &quot;10-4020-000117&quot; &quot;10-4020-000442&quot; &quot;10-4020-000227&quot; ... ## $ customerid : int 29825 29672 29734 29994 29565 29898 29580 30052 29974 29614 ... ## $ salespersonid : int 279 279 282 282 276 280 283 276 277 282 ... ## $ territoryid : int 5 5 6 6 4 1 1 4 3 6 ... ## $ billtoaddressid : int 985 921 517 482 1073 876 849 1074 629 529 ... ## $ shiptoaddressid : int 985 921 517 482 1073 876 849 1074 629 529 ... ## $ shipmethodid : int 5 5 5 5 5 5 5 5 5 5 ... ## $ creditcardid : int 16281 5618 1346 10456 4322 806 15232 13349 10370 1566 ... ## $ creditcardapprovalcode: chr &quot;105041Vi84182&quot; &quot;115213Vi29411&quot; &quot;85274Vi6854&quot; &quot;125295Vi53935&quot; ... ## $ currencyrateid : int NA NA 4 4 NA NA NA NA NA 4 ... ## $ subtotal : num 20566 1294 32726 28833 419 ... ## $ taxamt : num 1971.5 124.2 3153.8 2775.2 40.3 ... ## $ freight : num 616.1 38.8 985.6 867.2 12.6 ... ## $ totaldue : num 23153 1457 36866 32475 472 ... ## $ comment : chr NA NA NA NA ... ## $ rowguid : chr &quot;79b65321-39ca-4115-9cba-8fe0903e12e6&quot; &quot;738dc42d-d03b-48a1-9822-f95a67ea7389&quot; &quot;d91b9131-18a4-4a11-bc3a-90b6f53e9d74&quot; &quot;4a1ecfc0-cc3a-4740-b028-1c50bb48711c&quot; ... ## $ modifieddate : POSIXct, format: &quot;2011-06-07&quot; &quot;2011-06-07&quot; ... That’s very simple, but if the table is very large it may not be a problem, since R is designed to keep the entire table in memory. The tables that are found in an enterprise database such as adventureworks may be large, they are most often records kept by people. That somewhat limits their size (relative to data generated by machines) and expands the possibilities for human error. Note that the first line of the str() output reports the total number of observations. Later on we’ll use this tibble to demonstrate several packages and functions, but use only the first 13 columns for simplicity. salesorderheader_tibble &lt;- salesorderheader_tibble[,1:13] 7.2.2 Create a pointer to a table that can be reused The dplyr::tbl function gives us more control over access to a table by enabling control over which columns and rows to download. It creates an object that might look like a data frame, but it’s actually a list object that dplyr uses for constructing queries and retrieving data from the DBMS. salesorderheader_table &lt;- dplyr::tbl(con, &quot;salesorderheader&quot;) class(salesorderheader_table) ## [1] &quot;tbl_PqConnection&quot; &quot;tbl_dbi&quot; &quot;tbl_sql&quot; &quot;tbl_lazy&quot; ## [5] &quot;tbl&quot; 7.2.3 Controlling the number of rows returned with collect() The collect function triggers the creation of a tibble and controls the number of rows that the DBMS sends to R. For more complex queries, the dplyr::collect() function provides a mechanism to indicate what’s processed on on the DBMS server and what’s processed by R on the local machine. The chapter on Lazy Evaluation and Execution Environment discusses this issue in detail. salesorderheader_table %&gt;% dplyr::collect(n = 3) %&gt;% dim() ## [1] 3 25 salesorderheader_table %&gt;% dplyr::collect(n = 500) %&gt;% dim() ## [1] 500 25 7.2.4 Retrieving random rows from the DBMS When the DBMS contains many rows, a sample of the data may be plenty for your purposes. Although dplyr has nice functions to sample a data frame that’s already in R (e.g., the sample_n and sample_frac functions), to get a sample from the DBMS we have to use dbGetQuery to send native SQL to the database. To peek ahead, here is one example of a query that retrieves 20 rows from a 1% sample: one_percent_sample &lt;- DBI::dbGetQuery( con, &quot;SELECT orderdate, subtotal, taxamt, freight, totaldue FROM salesorderheader TABLESAMPLE BERNOULLI(3) LIMIT 20; &quot; ) one_percent_sample ## orderdate subtotal taxamt freight totaldue ## 1 2011-06-22 699.0982 55.9279 17.4775 772.5036 ## 2 2011-06-25 3578.2700 286.2616 89.4568 3953.9884 ## 3 2011-06-29 3374.9900 269.9992 84.3748 3729.3640 ## 4 2011-06-30 3578.2700 286.2616 89.4568 3953.9884 ## 5 2011-07-01 32492.6040 3118.7048 974.5952 36585.9040 ## 6 2011-07-03 3578.2700 286.2616 89.4568 3953.9884 ## 7 2011-07-22 3578.2700 286.2616 89.4568 3953.9884 ## 8 2011-08-01 2039.9940 195.8394 61.1998 2297.0332 ## 9 2011-08-01 1362.3067 130.1463 40.6707 1533.1237 ## 10 2011-08-07 3578.2700 286.2616 89.4568 3953.9884 ## 11 2011-08-07 3578.2700 286.2616 89.4568 3953.9884 ## 12 2011-08-14 3578.2700 286.2616 89.4568 3953.9884 ## 13 2011-09-06 3578.2700 286.2616 89.4568 3953.9884 ## 14 2011-09-08 3374.9900 269.9992 84.3748 3729.3640 ## 15 2011-09-08 699.0982 55.9279 17.4775 772.5036 ## 16 2011-09-10 3578.2700 286.2616 89.4568 3953.9884 ## 17 2011-09-11 3578.2700 286.2616 89.4568 3953.9884 ## 18 2011-09-12 3578.2700 286.2616 89.4568 3953.9884 ## 19 2011-09-19 3578.2700 286.2616 89.4568 3953.9884 ## 20 2011-10-01 35651.0339 3424.4400 1070.1375 40145.6114 Exact sample of 100 records This technique depends on knowing the range of a record index, such as the businessentityid in the salesorderheader table of our adventureworks database. Start by finding the min and max values. DBI::dbListFields(con, &quot;salesorderheader&quot;) ## [1] &quot;salesorderid&quot; &quot;revisionnumber&quot; &quot;orderdate&quot; ## [4] &quot;duedate&quot; &quot;shipdate&quot; &quot;status&quot; ## [7] &quot;onlineorderflag&quot; &quot;purchaseordernumber&quot; &quot;accountnumber&quot; ## [10] &quot;customerid&quot; &quot;salespersonid&quot; &quot;territoryid&quot; ## [13] &quot;billtoaddressid&quot; &quot;shiptoaddressid&quot; &quot;shipmethodid&quot; ## [16] &quot;creditcardid&quot; &quot;creditcardapprovalcode&quot; &quot;currencyrateid&quot; ## [19] &quot;subtotal&quot; &quot;taxamt&quot; &quot;freight&quot; ## [22] &quot;totaldue&quot; &quot;comment&quot; &quot;rowguid&quot; ## [25] &quot;modifieddate&quot; salesorderheader_df &lt;- DBI::dbReadTable(con, &quot;salesorderheader&quot;) (max_id &lt;- max(salesorderheader_df$salesorderid)) ## [1] 75123 (min_id &lt;- min(salesorderheader_df$salesorderid)) ## [1] 43659 Set the random number seed and draw the sample. set.seed(123) sample_rows &lt;- sample(1:max(salesorderheader_df$salesorderid), 10) salesorderheader_table &lt;- dplyr::tbl(con, &quot;salesorderheader&quot;) Run query with the filter verb listing the randomly sampled rows to be retrieved: salesorderheader_sample &lt;- salesorderheader_table %&gt;% dplyr::filter(salesorderid %in% sample_rows) %&gt;% dplyr::collect() str(salesorderheader_sample) ## Classes &#39;tbl_df&#39;, &#39;tbl&#39; and &#39;data.frame&#39;: 7 obs. of 25 variables: ## $ salesorderid : int 45404 46435 51663 57870 62555 65161 68293 ## $ revisionnumber : int 8 8 8 8 8 8 8 ## $ orderdate : POSIXct, format: &quot;2012-01-10&quot; &quot;2012-05-06&quot; ... ## $ duedate : POSIXct, format: &quot;2012-01-22&quot; &quot;2012-05-18&quot; ... ## $ shipdate : POSIXct, format: &quot;2012-01-17&quot; &quot;2012-05-13&quot; ... ## $ status : int 5 5 5 5 5 5 5 ## $ onlineorderflag : logi TRUE TRUE TRUE TRUE TRUE FALSE ... ## $ purchaseordernumber : chr NA NA NA NA ... ## $ accountnumber : chr &quot;10-4030-011217&quot; &quot;10-4030-012251&quot; &quot;10-4030-016327&quot; &quot;10-4030-018572&quot; ... ## $ customerid : int 11217 12251 16327 18572 13483 29799 13239 ## $ salespersonid : int NA NA NA NA NA 281 NA ## $ territoryid : int 1 9 8 4 1 4 6 ## $ billtoaddressid : int 19321 24859 19265 16902 15267 997 27923 ## $ shiptoaddressid : int 19321 24859 19265 16902 15267 997 27923 ## $ shipmethodid : int 1 1 1 1 1 5 1 ## $ creditcardid : int 8241 13188 16357 1884 4409 12582 1529 ## $ creditcardapprovalcode: chr &quot;332581Vi42712&quot; &quot;635144Vi68383&quot; &quot;420152Vi84562&quot; &quot;1224478Vi9772&quot; ... ## $ currencyrateid : int NA 4121 NA NA NA NA 11581 ## $ subtotal : num 3578 3375 2466 14 57 ... ## $ taxamt : num 286.26 270 197.31 1.12 4.56 ... ## $ freight : num 89.457 84.375 61.658 0.349 1.424 ... ## $ totaldue : num 3954 3729.4 2725.3 15.4 63 ... ## $ comment : chr NA NA NA NA ... ## $ rowguid : chr &quot;358f91b2-dadd-4014-8d4f-7f9736cb664e&quot; &quot;eb312409-fcd5-4bac-bd3b-16d4bd7889db&quot; &quot;ddc60552-af98-4166-9249-d09d424d8430&quot; &quot;fe46e631-47b9-4e14-9da5-1e4a4a135364&quot; ... ## $ modifieddate : POSIXct, format: &quot;2012-01-17&quot; &quot;2012-05-13&quot; ... 7.2.5 Sub-setting variables A table in the DBMS may not only have many more rows than you want, but also many more columns. The select command controls which columns are retrieved. salesorderheader_table %&gt;% dplyr::select(orderdate, subtotal, taxamt, freight, totaldue) %&gt;% head() ## # Source: lazy query [?? x 5] ## # Database: postgres [postgres@localhost:5432/adventureworks] ## orderdate subtotal taxamt freight totaldue ## &lt;dttm&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 2011-05-31 00:00:00 20566. 1972. 616. 23153. ## 2 2011-05-31 00:00:00 1294. 124. 38.8 1457. ## 3 2011-05-31 00:00:00 32726. 3154. 986. 36866. ## 4 2011-05-31 00:00:00 28833. 2775. 867. 32475. ## 5 2011-05-31 00:00:00 419. 40.3 12.6 472. ## 6 2011-05-31 00:00:00 24433. 2345. 733. 27510. That’s exactly equivalent to submitting the following SQL commands directly: DBI::dbGetQuery( con, &#39;SELECT &quot;orderdate&quot;, &quot;subtotal&quot;, &quot;taxamt&quot;, &quot;freight&quot;, &quot;totaldue&quot; FROM &quot;salesorderheader&quot; LIMIT 6&#39;) ## orderdate subtotal taxamt freight totaldue ## 1 2011-05-31 20565.6206 1971.5149 616.0984 23153.2339 ## 2 2011-05-31 1294.2529 124.2483 38.8276 1457.3288 ## 3 2011-05-31 32726.4786 3153.7696 985.5530 36865.8012 ## 4 2011-05-31 28832.5289 2775.1646 867.2389 32474.9324 ## 5 2011-05-31 419.4589 40.2681 12.5838 472.3108 ## 6 2011-05-31 24432.6088 2344.9921 732.8100 27510.4109 We won’t discuss dplyr methods for sub-setting variables, deriving new ones, or sub-setting rows based on the values found in the table, because they are covered well in other places, including: Comprehensive reference: https://dplyr.tidyverse.org/ Good tutorial: https://suzan.rbind.io/tags/dplyr/ In practice we find that, renaming variables is often quite important because the names in an SQL database might not meet your needs as an analyst. In “the wild”, you will find names that are ambiguous or overly specified, with spaces in them, and other problems that will make them difficult to use in R. It is good practice to do whatever renaming you are going to do in a predictable place like at the top of your code. The names in the adventureworks database are simple and clear, but if they were not, you might rename them for subsequent use in this way: tbl(con, &quot;salesorderheader&quot;) %&gt;% dplyr::rename(order_date = orderdate, sub_total_amount = subtotal, tax_amount = taxamt, freight_amount = freight, total_due_amount = totaldue) %&gt;% dplyr::select(order_date, sub_total_amount, tax_amount, freight_amount, total_due_amount ) %&gt;% show_query() ## &lt;SQL&gt; ## SELECT &quot;orderdate&quot; AS &quot;order_date&quot;, &quot;subtotal&quot; AS &quot;sub_total_amount&quot;, &quot;taxamt&quot; AS &quot;tax_amount&quot;, &quot;freight&quot; AS &quot;freight_amount&quot;, &quot;totaldue&quot; AS &quot;total_due_amount&quot; ## FROM &quot;salesorderheader&quot; That’s equivalent to the following SQL code: DBI::dbGetQuery( con, &#39;SELECT &quot;orderdate&quot; AS &quot;order_date&quot;, &quot;subtotal&quot; AS &quot;sub_total_amount&quot;, &quot;taxamt&quot; AS &quot;tax_amount&quot;, &quot;freight&quot; AS &quot;freight_amount&quot;, &quot;totaldue&quot; AS &quot;total_due_amount&quot; FROM &quot;salesorderheader&quot;&#39; ) %&gt;% head() ## order_date sub_total_amount tax_amount freight_amount total_due_amount ## 1 2011-05-31 20565.6206 1971.5149 616.0984 23153.2339 ## 2 2011-05-31 1294.2529 124.2483 38.8276 1457.3288 ## 3 2011-05-31 32726.4786 3153.7696 985.5530 36865.8012 ## 4 2011-05-31 28832.5289 2775.1646 867.2389 32474.9324 ## 5 2011-05-31 419.4589 40.2681 12.5838 472.3108 ## 6 2011-05-31 24432.6088 2344.9921 732.8100 27510.4109 The one difference is that the SQL code returns a regular data frame and the dplyr code returns a tibble. Notice that the seconds are grayed out in the tibble display. 7.3 Translating dplyr code to SQL queries Where did the translations we’ve shown above come from? The show_query function shows how dplyr is translating your query to the dialect of the target DBMS. The show_query() function shows you what dplyr is sending to the DBMS. It might be handy for inspecting what dplyr is doing or for showing your code to someone who is more SQL- than R-literate. In general we have used the function extensively in writing this book but in the final product we will not use it unless there is something in the SQL or the translation process that needs to be explained. salesorderheader_table %&gt;% dplyr::tally() %&gt;% dplyr::show_query() ## &lt;SQL&gt; ## SELECT COUNT(*) AS &quot;n&quot; ## FROM &quot;salesorderheader&quot; Here is an extensive discussion of how dplyr code is translated into SQL: https://dbplyr.tidyverse.org/articles/sql-translation.html If you prefer to use SQL directly, rather than dplyr, you can submit SQL code to the DBMS through the DBI::dbGetQuery function: DBI::dbGetQuery( con, &#39;SELECT COUNT(*) AS &quot;n&quot; FROM &quot;salesorderheader&quot; &#39; ) ## n ## 1 31465 When you create a report to run repeatedly, you might want to put that query into R markdown. That way you can also execute that SQL code in a chunk with the following header: {sql, connection=con, output.var = &quot;query_results&quot;} SELECT COUNT(*) AS &quot;n&quot; FROM &quot;salesorderheader&quot;; R markdown stores that query result in a tibble which can be printed by referring to it: query_results ## n ## 1 31465 7.4 Mixing dplyr and SQL When dplyr finds code that it does not know how to translate into SQL, it will simply pass it along to the DBMS. Therefore you can interleave native commands that your DBMS will understand in the middle of dplyr code. Consider this example that’s derived from (Ruiz 2019): salesorderheader_table %&gt;% dplyr::select_at(vars(subtotal, contains(&quot;date&quot;))) %&gt;% dplyr::mutate(today = now()) %&gt;% dplyr::show_query() ## &lt;SQL&gt; ## SELECT &quot;subtotal&quot;, &quot;orderdate&quot;, &quot;duedate&quot;, &quot;shipdate&quot;, &quot;modifieddate&quot;, CURRENT_TIMESTAMP AS &quot;today&quot; ## FROM &quot;salesorderheader&quot; That is native to PostgreSQL, not ANSI standard SQL. Verify that it works: salesorderheader_table %&gt;% dplyr::select_at(vars(subtotal, contains(&quot;date&quot;))) %&gt;% head() %&gt;% dplyr::mutate(today = now()) %&gt;% dplyr::collect() ## # A tibble: 6 x 6 ## subtotal orderdate duedate shipdate ## &lt;dbl&gt; &lt;dttm&gt; &lt;dttm&gt; &lt;dttm&gt; ## 1 20566. 2011-05-31 00:00:00 2011-06-12 00:00:00 2011-06-07 00:00:00 ## 2 1294. 2011-05-31 00:00:00 2011-06-12 00:00:00 2011-06-07 00:00:00 ## 3 32726. 2011-05-31 00:00:00 2011-06-12 00:00:00 2011-06-07 00:00:00 ## 4 28833. 2011-05-31 00:00:00 2011-06-12 00:00:00 2011-06-07 00:00:00 ## 5 419. 2011-05-31 00:00:00 2011-06-12 00:00:00 2011-06-07 00:00:00 ## 6 24433. 2011-05-31 00:00:00 2011-06-12 00:00:00 2011-06-07 00:00:00 ## # … with 2 more variables: modifieddate &lt;dttm&gt;, today &lt;dttm&gt; 7.5 Examining a single table with R Dealing with a large, complex database highlights the utility of specific tools in R. We include brief examples that we find to be handy: Base R structure: str Printing out some of the data: datatable, kable, and View Summary statistics: summary glimpse in the tibble package, which is included in the tidyverse skim in the skimr package 7.5.1 str - a base package workhorse str is a workhorse function that lists variables, their type and a sample of the first few variable values. str(salesorderheader_tibble) ## &#39;data.frame&#39;: 31465 obs. of 13 variables: ## $ salesorderid : int 43659 43660 43661 43662 43663 43664 43665 43666 43667 43668 ... ## $ revisionnumber : int 8 8 8 8 8 8 8 8 8 8 ... ## $ orderdate : POSIXct, format: &quot;2011-05-31&quot; &quot;2011-05-31&quot; ... ## $ duedate : POSIXct, format: &quot;2011-06-12&quot; &quot;2011-06-12&quot; ... ## $ shipdate : POSIXct, format: &quot;2011-06-07&quot; &quot;2011-06-07&quot; ... ## $ status : int 5 5 5 5 5 5 5 5 5 5 ... ## $ onlineorderflag : logi FALSE FALSE FALSE FALSE FALSE FALSE ... ## $ purchaseordernumber: chr &quot;PO522145787&quot; &quot;PO18850127500&quot; &quot;PO18473189620&quot; &quot;PO18444174044&quot; ... ## $ accountnumber : chr &quot;10-4020-000676&quot; &quot;10-4020-000117&quot; &quot;10-4020-000442&quot; &quot;10-4020-000227&quot; ... ## $ customerid : int 29825 29672 29734 29994 29565 29898 29580 30052 29974 29614 ... ## $ salespersonid : int 279 279 282 282 276 280 283 276 277 282 ... ## $ territoryid : int 5 5 6 6 4 1 1 4 3 6 ... ## $ billtoaddressid : int 985 921 517 482 1073 876 849 1074 629 529 ... 7.5.2 Always look at your data with head, View, or kable There is no substitute for looking at your data and R provides several ways to just browse it. The head function controls the number of rows that are displayed. Note that tail does not work against a database object. In every-day practice you would look at more than the default 6 rows, but here we wrap head around the data frame: sqlpetr::sp_print_df(head(salesorderheader_tibble)) 7.5.3 The summary function in base The base package’s summary function provides basic statistics that serve a unique diagnostic purpose in this context. For example, the following output shows that: * `businessentityid` is a number from 1 to 16,049. In a previous section, we ran the `str` function and saw that there are 16,044 observations in this table. Therefore, the `businessentityid` seems to be sequential from 1:16049, but there are 5 values missing from that sequence. _Exercise for the Reader_: Which 5 values from 1:16049 are missing from `businessentityid` values in the `salesorderheader` table? (_Hint_: In the chapter on SQL Joins, you will learn the functions needed to answer this question.) * The number of NA&#39;s in the `return_date` column is a good first guess as to the number of DVDs rented out or lost as of 2005-09-02 02:35:22. summary(salesorderheader_tibble) ## salesorderid revisionnumber orderdate ## Min. :43659 Min. :8.000 Min. :2011-05-31 00:00:00 ## 1st Qu.:51525 1st Qu.:8.000 1st Qu.:2013-06-20 00:00:00 ## Median :59391 Median :8.000 Median :2013-11-03 00:00:00 ## Mean :59391 Mean :8.001 Mean :2013-08-21 12:05:04 ## 3rd Qu.:67257 3rd Qu.:8.000 3rd Qu.:2014-02-28 00:00:00 ## Max. :75123 Max. :9.000 Max. :2014-06-30 00:00:00 ## ## duedate shipdate status ## Min. :2011-06-12 00:00:00 Min. :2011-06-07 00:00:00 Min. :5 ## 1st Qu.:2013-07-02 00:00:00 1st Qu.:2013-06-27 00:00:00 1st Qu.:5 ## Median :2013-11-15 00:00:00 Median :2013-11-10 00:00:00 Median :5 ## Mean :2013-09-02 12:05:41 Mean :2013-08-28 12:06:06 Mean :5 ## 3rd Qu.:2014-03-13 00:00:00 3rd Qu.:2014-03-08 00:00:00 3rd Qu.:5 ## Max. :2014-07-12 00:00:00 Max. :2014-07-07 00:00:00 Max. :5 ## ## onlineorderflag purchaseordernumber accountnumber customerid ## Mode :logical Length:31465 Length:31465 Min. :11000 ## FALSE:3806 Class :character Class :character 1st Qu.:14432 ## TRUE :27659 Mode :character Mode :character Median :19452 ## Mean :20170 ## 3rd Qu.:25994 ## Max. :30118 ## ## salespersonid territoryid billtoaddressid ## Min. :274.0 Min. : 1.000 Min. : 405 ## 1st Qu.:277.0 1st Qu.: 4.000 1st Qu.:14080 ## Median :279.0 Median : 6.000 Median :19449 ## Mean :280.6 Mean : 6.091 Mean :18263 ## 3rd Qu.:284.0 3rd Qu.: 9.000 3rd Qu.:24678 ## Max. :290.0 Max. :10.000 Max. :29883 ## NA&#39;s :27659 So the summary function is surprisingly useful as we first start to look at the table contents. 7.5.4 The glimpse function in the tibble package The tibble package’s glimpse function is a more compact version of str: tibble::glimpse(salesorderheader_tibble) ## Observations: 31,465 ## Variables: 13 ## $ salesorderid &lt;int&gt; 43659, 43660, 43661, 43662, 43663, 43664, 43665, … ## $ revisionnumber &lt;int&gt; 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8… ## $ orderdate &lt;dttm&gt; 2011-05-31, 2011-05-31, 2011-05-31, 2011-05-31, … ## $ duedate &lt;dttm&gt; 2011-06-12, 2011-06-12, 2011-06-12, 2011-06-12, … ## $ shipdate &lt;dttm&gt; 2011-06-07, 2011-06-07, 2011-06-07, 2011-06-07, … ## $ status &lt;int&gt; 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5… ## $ onlineorderflag &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, … ## $ purchaseordernumber &lt;chr&gt; &quot;PO522145787&quot;, &quot;PO18850127500&quot;, &quot;PO18473189620&quot;, … ## $ accountnumber &lt;chr&gt; &quot;10-4020-000676&quot;, &quot;10-4020-000117&quot;, &quot;10-4020-0004… ## $ customerid &lt;int&gt; 29825, 29672, 29734, 29994, 29565, 29898, 29580, … ## $ salespersonid &lt;int&gt; 279, 279, 282, 282, 276, 280, 283, 276, 277, 282,… ## $ territoryid &lt;int&gt; 5, 5, 6, 6, 4, 1, 1, 4, 3, 6, 1, 3, 1, 6, 2, 6, 3… ## $ billtoaddressid &lt;int&gt; 985, 921, 517, 482, 1073, 876, 849, 1074, 629, 52… 7.5.5 The skim function in the skimr package The skimr package has several functions that make it easy to examine an unknown data frame and assess what it contains. It is also extensible. skimr::skim(salesorderheader_tibble) Table 7.1: Data summary Name salesorderheader_tibble Number of rows 31465 Number of columns 13 _______________________ Column type frequency: character 2 logical 1 numeric 7 POSIXct 3 ________________________ Group variables None Variable type: character skim_variable n_missing complete_rate min max empty n_unique whitespace purchaseordernumber 27659 0.12 10 13 0 3806 0 accountnumber 0 1.00 14 14 0 19119 0 Variable type: logical skim_variable n_missing complete_rate mean count onlineorderflag 0 1 0.88 TRU: 27659, FAL: 3806 Variable type: numeric skim_variable n_missing complete_rate mean sd p0 p25 p50 p75 p100 hist salesorderid 0 1.00 59391.00 9083.31 43659 51525 59391 67257 75123 ▇▇▇▇▇ revisionnumber 0 1.00 8.00 0.03 8 8 8 8 9 ▇▁▁▁▁ status 0 1.00 5.00 0.00 5 5 5 5 5 ▁▁▇▁▁ customerid 0 1.00 20170.18 6261.73 11000 14432 19452 25994 30118 ▇▆▅▅▇ salespersonid 27659 0.12 280.61 4.85 274 277 279 284 290 ▇▅▅▂▃ territoryid 0 1.00 6.09 2.96 1 4 6 9 10 ▃▅▃▅▇ billtoaddressid 0 1.00 18263.15 8210.07 405 14080 19449 24678 29883 ▃▁▇▇▇ Variable type: POSIXct skim_variable n_missing complete_rate min max median n_unique orderdate 0 1 2011-05-31 2014-06-30 2013-11-03 1124 duedate 0 1 2011-06-12 2014-07-12 2013-11-15 1124 shipdate 0 1 2011-06-07 2014-07-07 2013-11-10 1124 skimr::skim_to_wide(salesorderheader_tibble) #skimr doesn&#39;t like certain kinds of columns ## Warning: &#39;skimr::skim_to_wide&#39; is deprecated. ## Use &#39;skim()&#39; instead. ## See help(&quot;Deprecated&quot;) Table 7.1: Data summary Name .data Number of rows 31465 Number of columns 13 _______________________ Column type frequency: character 2 logical 1 numeric 7 POSIXct 3 ________________________ Group variables None Variable type: character skim_variable n_missing complete_rate min max empty n_unique whitespace purchaseordernumber 27659 0.12 10 13 0 3806 0 accountnumber 0 1.00 14 14 0 19119 0 Variable type: logical skim_variable n_missing complete_rate mean count onlineorderflag 0 1 0.88 TRU: 27659, FAL: 3806 Variable type: numeric skim_variable n_missing complete_rate mean sd p0 p25 p50 p75 p100 hist salesorderid 0 1.00 59391.00 9083.31 43659 51525 59391 67257 75123 ▇▇▇▇▇ revisionnumber 0 1.00 8.00 0.03 8 8 8 8 9 ▇▁▁▁▁ status 0 1.00 5.00 0.00 5 5 5 5 5 ▁▁▇▁▁ customerid 0 1.00 20170.18 6261.73 11000 14432 19452 25994 30118 ▇▆▅▅▇ salespersonid 27659 0.12 280.61 4.85 274 277 279 284 290 ▇▅▅▂▃ territoryid 0 1.00 6.09 2.96 1 4 6 9 10 ▃▅▃▅▇ billtoaddressid 0 1.00 18263.15 8210.07 405 14080 19449 24678 29883 ▃▁▇▇▇ Variable type: POSIXct skim_variable n_missing complete_rate min max median n_unique orderdate 0 1 2011-05-31 2014-06-30 2013-11-03 1124 duedate 0 1 2011-06-12 2014-07-12 2013-11-15 1124 shipdate 0 1 2011-06-07 2014-07-07 2013-11-10 1124 7.6 Disconnect from the database and stop Docker dbDisconnect(con) # or if using the connections package, use: # connection_close(con) sp_docker_stop(&quot;adventureworks&quot;) 7.7 Additional reading (Wickham 2018) (Baumer 2018) References "],
["chapter-exploring-a-single-table.html", "Chapter 8 Asking Business Questions From a Single Table 8.1 Setup our standard working environment 8.2 A word on naming 8.3 The overall AdventureWorks sales picture 8.4 Annual sales 8.5 Monthly Sales 8.6 The effect of online sales 8.7 Impact of order type on monthly sales 8.8 Detect and diagnose the day of the month problem 8.9 Correcting the order date for Sales Reps 8.10 Disconnect from the database and stop Docker", " Chapter 8 Asking Business Questions From a Single Table This chapter explores: Issues that come up when investigating a single table from a business perspective Show the multiple data anomalies found in a single AdventureWorks table (salesorderheader) The interplay between “data questions” and “business questions” The previous chapter has demonstrated some of the automated techniques for showing what’s in a table using some standard R functions and packages. Now we demonstrate a step-by-step process of making sense of what’s in one table with more of a business perspective. We illustrate the kind of detective work that’s often involved as we investigate the organizational meaning of the data in a table. We’ll investigate the salesorderheader table in the sales schema in this example to understand the sales profile of the “AdventureWorks” business. We show that there are quite a few interpretation issues even when we are examining just 3 out of the 25 columns in one table. For this kind of detective work we are seeking to understand the following elements separately and as they interact with each other: What data is stored in the database How information is represented How the data is entered at a day-to-day level to represent business activities How the business itself is changing over time 8.1 Setup our standard working environment Use these libraries: library(tidyverse) library(DBI) library(RPostgres) library(connections) library(glue) require(knitr) library(dbplyr) library(sqlpetr) library(bookdown) library(here) library(lubridate) library(gt) library(scales) library(patchwork) theme_set(theme_light()) Connect to adventureworks. In an interactive session we prefer to use connections::connection_open instead of dbConnect sp_docker_start(&quot;adventureworks&quot;) Sys.sleep(sleep_default) con &lt;- dbConnect( RPostgres::Postgres(), # without the previous and next lines, some functions fail with bigint data # so change int64 to integer bigint = &quot;integer&quot;, host = &quot;localhost&quot;, port = 5432, user = &quot;postgres&quot;, password = &quot;postgres&quot;, dbname = &quot;adventureworks&quot;) Some queries generate big integers, so we need to include RPostgres::Postgres() and bigint = &quot;integer&quot; in the connections statement because some functions in the tidyverse object to the bigint datatype. 8.2 A word on naming You will find that many tables will have columns with the same name in an enterprise database. For example, in the AdventureWorks database, almost all tables have columns named rowguid and modifieddate and there are many other examples of names that are reused throughout the database. Duplicate columns are best renamed or deliberately dropped. The meaning of a column depends on the table that contains it, so as you pull a column out of a table, when renaming it the collumns provenance should be reflected in the new name. Naming columns carefully (whether retrieved from the database or calculated) will pay off, especially as our queries become more complex. Using soh as an abbreviation of sales order header to tag columns or statistics that are derived from the salesorderheader table, as we do in this book, is one example of an intentional naming strategy: it reminds us of the original source of the data. You, future you, and your collaborators will appreciate the effort no matter what naming convention you adopt. And a naming convention when rigidly applied can yield some long and ugly names. In the following example soh appears in different positions in the column name but it is easy to guess at a glance that the data comes from the salesorderheader table. Naming derived tables is just as important as naming derived columns. 8.3 The overall AdventureWorks sales picture We begin by looking at Sales on a yearly basis, then consider monthly sales. We discover that half way through the period represented in the database, the business appears to begin selling online, which has very different characteristics than sales by Sales Reps. We then look at the details of how Sales Rep sales are recorded in the system and discover a data anomaly that we can correct. 8.4 Annual sales On an annual basis, are sales dollars trending up, down or flat? We begin with annual revenue and number of orders. annual_sales &lt;- tbl(con, in_schema(&quot;sales&quot;, &quot;salesorderheader&quot;)) %&gt;% mutate(year = substr(as.character(orderdate), 1, 4)) %&gt;% group_by(year) %&gt;% summarize( min_soh_orderdate = min(orderdate, na.rm = TRUE), max_soh_orderdate = max(orderdate, na.rm = TRUE), total_soh_dollars = round(sum(subtotal, na.rm = TRUE), 2), avg_total_soh_dollars = round(mean(subtotal, na.rm = TRUE), 2), soh_count = n() ) %&gt;% arrange(year) %&gt;% select( year, min_soh_orderdate, max_soh_orderdate, total_soh_dollars, avg_total_soh_dollars, soh_count ) %&gt;% collect() Note that all of this query is running on the server since the collect() statement is at the very end. annual_sales %&gt;% str() ## Classes &#39;tbl_df&#39;, &#39;tbl&#39; and &#39;data.frame&#39;: 4 obs. of 6 variables: ## $ year : chr &quot;2011&quot; &quot;2012&quot; &quot;2013&quot; &quot;2014&quot; ## $ min_soh_orderdate : POSIXct, format: &quot;2011-05-31&quot; &quot;2012-01-01&quot; ... ## $ max_soh_orderdate : POSIXct, format: &quot;2011-12-31&quot; &quot;2012-12-31&quot; ... ## $ total_soh_dollars : num 12641672 33524301 43622479 20057929 ## $ avg_total_soh_dollars: num 7867 8563 3076 1705 ## $ soh_count : int 1607 3915 14182 11761 We hang on to some date information for later use in plot titles. min_soh_dt &lt;- min(annual_sales$min_soh_orderdate) max_soh_dt &lt;- max(annual_sales$max_soh_orderdate) 8.4.1 Annual summary of sales, number of transactions and average sale tot_sales &lt;- ggplot(data = annual_sales, aes(x = year, y = total_soh_dollars/100000)) + geom_col() + geom_text(aes(label = round(as.numeric(total_soh_dollars/100000), digits = 0)), vjust = 1.5, color = &quot;white&quot;) + scale_y_continuous(labels = scales::dollar_format()) + labs( title = &quot;Total Sales per Year - Millions&quot;, x = NULL, y = &quot;Sales $M&quot; ) Both 2011 and 2014 turn out to be are shorter time spans than the other two years, making comparison interpretation difficult. Still, it’s clear that 2013 was the best year for annual sales dollars. Comparing the number of orders per year has roughly the same overall pattern (2013 ranks highest, etc.) but the proportions between the years are quite different. Although 2013 was the best year in terms of total number of orders, there were many more in 2014 compared with 2012. That suggests looking at the average dollars per sale for each year. 8.4.2 Average dollars per sale (tot_sales + num_orders) / avg_sale Figure 8.1: AdventureWorks sales performance That’s a big drop between average sale of more than $7,000 in the first two years down to the $3,000 range in the last two. There has been a remarkable change in this business. At the same time the total number of orders shot up from less than 4,000 a year to more than 14,000. Why are the number of orders increasing, but the average dollar amount of a sale is dropping? Perhaps monthly monthly sales has the answer. We adapt the first query to group by month and year. 8.5 Monthly Sales Our next iteration drills down from annual sales dollars to monthly sales dollars. For that we download the orderdate as a date, rather than a character variable for the year. R handles the conversion from the PostgreSQL date-time to an R date-time. We then convert it to a simple date with a lubridate function. The following query uses the postgreSQL function date_trunc, which is equivalent to lubridate’s round_date function in R. If you want to push as much of the processing as possible onto the database server and thus possibly deal with smaller datasets in R, interleaving postgreSQL functions into your dplyr code will help. monthly_sales &lt;- tbl(con, in_schema(&quot;sales&quot;, &quot;salesorderheader&quot;)) %&gt;% select(orderdate, subtotal) %&gt;% mutate( orderdate = date_trunc(&#39;month&#39;, orderdate) ) %&gt;% group_by(orderdate) %&gt;% summarize( total_soh_dollars = round(sum(subtotal, na.rm = TRUE), 2), avg_total_soh_dollars = round(mean(subtotal, na.rm = TRUE), 2), soh_count = n() ) %&gt;% show_query() %&gt;% collect() ## &lt;SQL&gt; ## SELECT &quot;orderdate&quot;, ROUND((SUM(&quot;subtotal&quot;)) :: numeric, 2) AS &quot;total_soh_dollars&quot;, ROUND((AVG(&quot;subtotal&quot;)) :: numeric, 2) AS &quot;avg_total_soh_dollars&quot;, COUNT(*) AS &quot;soh_count&quot; ## FROM (SELECT date_trunc(&#39;month&#39;, &quot;orderdate&quot;) AS &quot;orderdate&quot;, &quot;subtotal&quot; ## FROM sales.salesorderheader) &quot;dbplyr_004&quot; ## GROUP BY &quot;orderdate&quot; Note that date_trunc('month', orderdate) gets passed through exactly “as is.” In many cases we don’t really care whether our queries are executed by R or by the SQL server, but if we do care we need to substitute the postgreSQL equivalent for the R functions we might ordinarily use. In those cases we have to check whether functions from R packages like lubridate and the equivalent postgreSQL functions are exactly alike. Often they are subtly different: in the previous query the postgreSQL function produces a POSIXct column, not a Date so we need to tack on a mutate function once the data is on the R side as shown here: monthly_sales &lt;- monthly_sales %&gt;% mutate(orderdate = as.Date(orderdate)) Next let’s plot the monthly sales data: ggplot(data = monthly_sales, aes(x = orderdate, y = total_soh_dollars)) + geom_col() + scale_y_continuous(labels = dollar) + theme(plot.title = element_text(hjust = 0.5)) + labs( title = glue(&quot;Sales by Month\\n&quot;, {format(min_soh_dt, &quot;%B %d, %Y&quot;)} , &quot; to &quot;, {format(max_soh_dt, &quot;%B %d, %Y&quot;)}), x = &quot;Month&quot;, y = &quot;Sales Dollars&quot; ) Figure 8.2: Total Monthly Sales That graph doesn’t show how the business might have changed, but it is remarkable how much variation there is from one month to another – particularly in 2012 and 2014. 8.5.1 Check lagged monthly data Because of the month-over-month sales variation. We’ll use dplyr::lag to help find the delta and later visualize just how much month-to-month difference there is. monthly_sales &lt;- arrange(monthly_sales, orderdate) monthly_sales_lagged &lt;- monthly_sales %&gt;% mutate(monthly_sales_change = (dplyr::lag(total_soh_dollars)) - total_soh_dollars) monthly_sales_lagged[is.na(monthly_sales_lagged)] = 0 median(monthly_sales_lagged$monthly_sales_change, na.rm = TRUE) ## [1] -221690.505 (sum_lags &lt;- summary(monthly_sales_lagged$monthly_sales_change)) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## -5879806.05 -1172995.19 -221690.51 11968.42 1159252.70 5420357.17 The average month over month change in sales looks OK ($ 11,968) although the Median is negative: $ 11,968. There is a very wide spread in our month-over-month sales data between the lower and upper quartile. We can plot the variation as follows: ggplot(monthly_sales_lagged, aes(x = orderdate, y = monthly_sales_change)) + scale_x_date(date_breaks = &quot;year&quot;, date_labels = &quot;%Y&quot;, date_minor_breaks = &quot;3 months&quot;) + geom_line() + # geom_point() + scale_y_continuous(limits = c(-6000000,5500000), labels = scales::dollar_format()) + theme(plot.title = element_text(hjust = .5)) + labs( title = glue( &quot;Monthly Sales Change \\n&quot;, &quot;Between &quot;, {format(min_soh_dt, &quot;%B %d, %Y&quot;)} , &quot; and &quot;, {format(max_soh_dt, &quot;%B %d, %Y&quot;)} ), x = &quot;Month&quot;, y = &quot;Dollar Change&quot; ) Figure 8.3: Monthly Sales Change It looks like the big change in the business occurred in the summer of 2013 when the number of orders jumped but the dollar volume just continued to bump along. 8.5.2 Comparing dollars and orders to a base year To look at dollars and the number of orders together, we compare the monthly data to the yearly average for 2011. baseline_month &lt;- &quot;2011-07-01&quot; start_month &lt;- monthly_sales %&gt;% filter(orderdate == as.Date(baseline_month)) Express monthly data relative to 2011-07-01, 2044600, 8851.08, 231 monthly_sales_base_year_normalized_to_2011 &lt;- monthly_sales %&gt;% mutate( dollars = (100 * total_soh_dollars) / start_month$total_soh_dollars, number_of_orders = (100 * soh_count) / start_month$soh_count ) %&gt;% ungroup() monthly_sales_base_year_normalized_to_2011 &lt;- monthly_sales_base_year_normalized_to_2011 %&gt;% select(orderdate, dollars, `# of orders` = number_of_orders) %&gt;% pivot_longer(-orderdate, names_to = &quot;relative_to_2011_average&quot;, values_to = &quot;amount&quot; ) monthly_sales_base_year_normalized_to_2011 %&gt;% ggplot(aes(orderdate, amount, color = relative_to_2011_average)) + geom_line() + geom_hline(yintercept = 100) + scale_x_date(date_labels = &quot;%Y-%m&quot;, date_breaks = &quot;6 months&quot;) + labs( title = glue( &quot;Adventureworks Normalized Monthly Sales\\n&quot;, &quot;Number of Sales Orders and Dollar Totals\\n&quot;, {format(min_soh_dt, &quot;%B %d, %Y&quot;)} , &quot; to &quot;, {format(max_soh_dt, &quot;%B %d, %Y&quot;)}), x = &quot;Date&quot;, y = &quot;&quot;, color = glue(baseline_month, &quot; values = 100&quot;) ) + theme(legend.position = c(.3,.75)) Figure 8.4: Adventureworks Normalized Monthly Sales 8.6 The effect of online sales We suspect that the business has changed a lot with the advent of online orders so we check the impact of onlineorderflag on annual sales. The onlineorderflag indicates which sales channel accounted for the sale, Sales Reps or Online. 8.6.1 Add onlineorderflag to our annual sales query annual_sales_w_channel &lt;- tbl(con, in_schema(&quot;sales&quot;, &quot;salesorderheader&quot;)) %&gt;% select(orderdate, subtotal, onlineorderflag) %&gt;% collect() %&gt;% mutate( orderdate = date(orderdate), orderdate = round_date(orderdate, &quot;month&quot;), onlineorderflag = if_else(onlineorderflag == FALSE, &quot;Sales Rep&quot;, &quot;Online&quot; ), onlineorderflag = as.factor(onlineorderflag) ) %&gt;% group_by(orderdate, onlineorderflag) %&gt;% summarize( min_soh_orderdate = min(orderdate, na.rm = TRUE), max_soh_orderdate = max(orderdate, na.rm = TRUE), total_soh_dollars = round(sum(subtotal, na.rm = TRUE), 2), avg_total_soh_dollars = round(mean(subtotal, na.rm = TRUE), 2), soh_count = n() ) %&gt;% select( orderdate, onlineorderflag, min_soh_orderdate, max_soh_orderdate, total_soh_dollars, avg_total_soh_dollars, soh_count ) Note that we are creating a factor and doing most of the calculations on the R side, not on the DBMS side. 8.6.2 Annual Sales comparison Start by looking at total sales. ggplot(data = annual_sales_w_channel, aes(x = orderdate, y = total_soh_dollars)) + geom_col() + scale_y_continuous(labels = scales::dollar_format()) + facet_wrap(&quot;onlineorderflag&quot;) + labs( title = &quot;AdventureWorks Monthly Sales&quot;, caption = glue( &quot;Between &quot;, {format(min_soh_dt, &quot;%B %d, %Y&quot;)} , &quot; - &quot;, {format(max_soh_dt, &quot;%B %d, %Y&quot;)}), subtitle = &quot;Comparing Online and Sales Rep sales channels&quot;, x = &quot;Year&quot;, y = &quot;Sales $&quot; ) (#fig:Calculate annual sales dollars )Sales Channel Breakdown It looks like there are two businesses represented in the AdventureWorks database that have very different growth profiles. 8.6.3 Order volume comparison ggplot(data = annual_sales_w_channel, aes(x = orderdate, y = as.numeric(soh_count))) + geom_col() + facet_wrap(&quot;onlineorderflag&quot;) + labs( title = &quot;AdventureWorks Monthly orders&quot;, caption = glue( &quot;Between &quot;, {format(min_soh_dt, &quot;%B %d, %Y&quot;)} , &quot; - &quot;, {format(max_soh_dt, &quot;%B %d, %Y&quot;)}), subtitle = &quot;Comparing Online and Sales Rep sales channels&quot;, x = &quot;Year&quot;, y = &quot;Total number of orders&quot; ) Figure 8.5: AdventureWorks Monthly Orders by Channel Comparing Online and Sales Rep sales, the difference in the number of orders is even more striking than the difference between annual sales. 8.6.4 Comparing average order size: Sales Reps to Online orders ggplot(data = annual_sales_w_channel, aes(x = orderdate, y = avg_total_soh_dollars)) + geom_col() + facet_wrap(&quot;onlineorderflag&quot;) + scale_y_continuous(labels = scales::dollar_format()) + labs( title = &quot;AdventureWorks Average Dollars per Sale&quot;, x = glue( &quot;Year - between &quot;, {format(min_soh_dt, &quot;%B %d, %Y&quot;)} , &quot; - &quot;, {format(max_soh_dt, &quot;%B %d, %Y&quot;)}), y = &quot;Average sale amount&quot; ) Figure 8.6: Average dollar per Sale comparison 8.7 Impact of order type on monthly sales To dig into the difference between Sales Rep and Online sales we can look at monthly data. 8.7.1 Retrieve monthly sales with the onlineorderflag This query puts the collect statement earlier than the previous queries. monthly_sales_w_channel &lt;- tbl(con, in_schema(&quot;sales&quot;, &quot;salesorderheader&quot;)) %&gt;% select(orderdate, subtotal, onlineorderflag) %&gt;% collect() %&gt;% # From here on we&#39;re in R mutate( orderdate = date(orderdate), orderdate = floor_date(orderdate, unit = &quot;month&quot;), onlineorderflag = if_else(onlineorderflag == FALSE, &quot;Sales Rep&quot;, &quot;Online&quot;) ) %&gt;% group_by(orderdate, onlineorderflag) %&gt;% summarize( min_soh_orderdate = min(orderdate, na.rm = TRUE), max_soh_orderdate = max(orderdate, na.rm = TRUE), total_soh_dollars = round(sum(subtotal, na.rm = TRUE), 2), avg_total_soh_dollars = round(mean(subtotal, na.rm = TRUE), 2), soh_count = n() ) %&gt;% ungroup() monthly_sales_w_channel %&gt;% rename(`Sales Channel` = onlineorderflag) %&gt;% group_by(`Sales Channel`) %&gt;% summarize( unique_dates = n(), start_date = min(min_soh_orderdate), end_date = max(max_soh_orderdate), total_sales = round(sum(total_soh_dollars)), days_span = end_date - start_date ) %&gt;% gt() html { font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, 'Helvetica Neue', 'Fira Sans', 'Droid Sans', Arial, sans-serif; } #pzveloiefp .gt_table { display: table; border-collapse: collapse; margin-left: auto; /* table.margin.left */ margin-right: auto; /* table.margin.right */ color: #333333; font-size: 16px; /* table.font.size */ background-color: #FFFFFF; /* table.background.color */ width: auto; /* table.width */ border-top-style: solid; /* table.border.top.style */ border-top-width: 2px; /* table.border.top.width */ border-top-color: #A8A8A8; /* table.border.top.color */ border-bottom-style: solid; /* table.border.bottom.style */ border-bottom-width: 2px; /* table.border.bottom.width */ border-bottom-color: #A8A8A8; /* table.border.bottom.color */ } #pzveloiefp .gt_heading { background-color: #FFFFFF; /* heading.background.color */ border-bottom-color: #FFFFFF; /* table.background.color */ border-left-style: hidden; /* heading.border.lr.style */ border-left-width: 1px; /* heading.border.lr.width */ border-left-color: #D3D3D3; /* heading.border.lr.color */ border-right-style: hidden; /* heading.border.lr.style */ border-right-width: 1px; /* heading.border.lr.width */ border-right-color: #D3D3D3; /* heading.border.lr.color */ } #pzveloiefp .gt_title { color: #333333; font-size: 125%; /* heading.title.font.size */ font-weight: initial; /* heading.title.font.weight */ padding-top: 4px; /* heading.top.padding - not yet used */ padding-bottom: 4px; border-bottom-color: #FFFFFF; /* table.background.color */ border-bottom-width: 0; } #pzveloiefp .gt_subtitle { color: #333333; font-size: 85%; /* heading.subtitle.font.size */ font-weight: initial; /* heading.subtitle.font.weight */ padding-top: 0; padding-bottom: 4px; /* heading.bottom.padding - not yet used */ border-top-color: #FFFFFF; /* table.background.color */ border-top-width: 0; } #pzveloiefp .gt_bottom_border { border-bottom-style: solid; /* heading.border.bottom.style */ border-bottom-width: 2px; /* heading.border.bottom.width */ border-bottom-color: #D3D3D3; /* heading.border.bottom.color */ } #pzveloiefp .gt_column_spanner { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; padding-top: 4px; padding-bottom: 4px; } #pzveloiefp .gt_col_headings { border-top-style: solid; /* column_labels.border.top.style */ border-top-width: 2px; /* column_labels.border.top.width */ border-top-color: #D3D3D3; /* column_labels.border.top.color */ border-bottom-style: solid; /* column_labels.border.bottom.style */ border-bottom-width: 2px; /* column_labels.border.bottom.width */ border-bottom-color: #D3D3D3; /* column_labels.border.bottom.color */ border-left-style: none; /* column_labels.border.lr.style */ border-left-width: 1px; /* column_labels.border.lr.width */ border-left-color: #D3D3D3; /* column_labels.border.lr.color */ border-right-style: none; /* column_labels.border.lr.style */ border-right-width: 1px; /* column_labels.border.lr.width */ border-right-color: #D3D3D3; /* column_labels.border.lr.color */ } #pzveloiefp .gt_col_heading { color: #333333; background-color: #FFFFFF; /* column_labels.background.color */ font-size: 100%; /* column_labels.font.size */ font-weight: initial; /* column_labels.font.weight */ text-transform: inherit; /* column_labels.text_transform */ vertical-align: middle; padding: 5px; margin: 10px; overflow-x: hidden; } #pzveloiefp .gt_sep_right { border-right: 5px solid #FFFFFF; } #pzveloiefp .gt_group_heading { padding: 8px; /* row_group.padding */ color: #333333; background-color: #FFFFFF; /* row_group.background.color */ font-size: 100%; /* row_group.font.size */ font-weight: initial; /* row_group.font.weight */ text-transform: inherit; /* row_group.text_transform */ border-top-style: solid; /* row_group.border.top.style */ border-top-width: 2px; /* row_group.border.top.width */ border-top-color: #D3D3D3; /* row_group.border.top.color */ border-bottom-style: solid; /* row_group.border.bottom.style */ border-bottom-width: 2px; /* row_group.border.bottom.width */ border-bottom-color: #D3D3D3; /* row_group.border.bottom.color */ border-left-style: none; /* row_group.border.left.style */ border-left-width: 1px; /* row_group.border.left.width */ border-left-color: #D3D3D3; /* row_group.border.left.color */ border-right-style: none; /* row_group.border.right.style */ border-right-width: 1px; /* row_group.border.right.width */ border-right-color: #D3D3D3; /* row_group.border.right.color */ vertical-align: middle; } #pzveloiefp .gt_empty_group_heading { padding: 0.5px; color: #333333; background-color: #FFFFFF; /* row_group.background.color */ font-size: 100%; /* row_group.font.size */ font-weight: initial; /* row_group.font.weight */ border-top-style: solid; /* row_group.border.top.style */ border-top-width: 2px; /* row_group.border.top.width */ border-top-color: #D3D3D3; /* row_group.border.top.color */ border-bottom-style: solid; /* row_group.border.bottom.style */ border-bottom-width: 2px; /* row_group.border.bottom.width */ border-bottom-color: #D3D3D3; /* row_group.border.bottom.color */ vertical-align: middle; } #pzveloiefp .gt_striped { background-color: #8080800D; /* row.striping.background_color */ } #pzveloiefp .gt_from_md > :first-child { margin-top: 0; } #pzveloiefp .gt_from_md > :last-child { margin-bottom: 0; } #pzveloiefp .gt_row { padding-top: 8px; /* data_row.padding */ padding-bottom: 8px; /* data_row.padding */ padding-left: 5px; padding-right: 5px; margin: 10px; border-top-style: solid; /* table_body.hlines.style */ border-top-width: 1px; /* table_body.hlines.width */ border-top-color: #D3D3D3; /* table_body.hlines.color */ border-left-style: none; /* table_body.vlines.style */ border-left-width: 1px; /* table_body.vlines.width */ border-left-color: #D3D3D3; /* table_body.vlines.color */ border-right-style: none; /* table_body.vlines.style */ border-right-width: 1px; /* table_body.vlines.width */ border-right-color: #D3D3D3; /* table_body.vlines.color */ vertical-align: middle; overflow-x: hidden; } #pzveloiefp .gt_stub { color: #333333; background-color: #FFFFFF; /* stub.background.color */ font-weight: initial; /* stub.font.weight */ text-transform: inherit; /* stub.text_transform */ border-right-style: solid; /* stub.border.style */ border-right-width: 2px; /* stub.border.width */ border-right-color: #D3D3D3; /* stub.border.color */ padding-left: 12px; } #pzveloiefp .gt_summary_row { color: #333333; background-color: #FFFFFF; /* summary_row.background.color */ text-transform: inherit; /* summary_row.text_transform */ padding-top: 8px; /* summary_row.padding */ padding-bottom: 8px; /* summary_row.padding */ padding-left: 5px; padding-right: 5px; } #pzveloiefp .gt_first_summary_row { padding-top: 8px; /* summary_row.padding */ padding-bottom: 8px; /* summary_row.padding */ padding-left: 5px; padding-right: 5px; border-top-style: solid; /* summary_row.border.style */ border-top-width: 2px; /* summary_row.border.width */ border-top-color: #D3D3D3; /* summary_row.border.color */ } #pzveloiefp .gt_grand_summary_row { color: #333333; background-color: #FFFFFF; /* grand_summary_row.background.color */ text-transform: inherit; /* grand_summary_row.text_transform */ padding-top: 8px; /* grand_summary_row.padding */ padding-bottom: 8px; /* grand_summary_row.padding */ padding-left: 5px; padding-right: 5px; } #pzveloiefp .gt_first_grand_summary_row { padding-top: 8px; /* grand_summary_row.padding */ padding-bottom: 8px; /* grand_summary_row.padding */ padding-left: 5px; padding-right: 5px; border-top-style: double; /* grand_summary_row.border.style */ border-top-width: 6px; /* grand_summary_row.border.width */ border-top-color: #D3D3D3; /* grand_summary_row.border.color */ } #pzveloiefp .gt_table_body { border-top-style: solid; /* table_body.border.top.style */ border-top-width: 2px; /* table_body.border.top.width */ border-top-color: #D3D3D3; /* table_body.border.top.color */ border-bottom-style: solid; /* table_body.border.bottom.style */ border-bottom-width: 2px; /* table_body.border.bottom.width */ border-bottom-color: #D3D3D3; /* table_body.border.bottom.color */ } #pzveloiefp .gt_footnotes { color: #333333; background-color: #FFFFFF; /* footnotes.background.color */ border-bottom-style: none; /* footnotes.border.bottom.style */ border-bottom-width: 2px; /* footnotes.border.bottom.width */ border-bottom-color: #D3D3D3; /* footnotes.border.bottom.color */ border-left-style: none; /* footnotes.border.lr.color */ border-left-width: 2px; /* footnotes.border.lr.color */ border-left-color: #D3D3D3; /* footnotes.border.lr.color */ border-right-style: none; /* footnotes.border.lr.color */ border-right-width: 2px; /* footnotes.border.lr.color */ border-right-color: #D3D3D3; /* footnotes.border.lr.color */ } #pzveloiefp .gt_footnote { margin: 0px; font-size: 90%; /* footnotes.font.size */ padding: 4px; /* footnotes.padding */ } #pzveloiefp .gt_sourcenotes { color: #333333; background-color: #FFFFFF; /* source_notes.background.color */ border-bottom-style: none; /* source_notes.border.bottom.style */ border-bottom-width: 2px; /* source_notes.border.bottom.width */ border-bottom-color: #D3D3D3; /* source_notes.border.bottom.color */ border-left-style: none; /* source_notes.border.lr.style */ border-left-width: 2px; /* source_notes.border.lr.style */ border-left-color: #D3D3D3; /* source_notes.border.lr.style */ border-right-style: none; /* source_notes.border.lr.style */ border-right-width: 2px; /* source_notes.border.lr.style */ border-right-color: #D3D3D3; /* source_notes.border.lr.style */ } #pzveloiefp .gt_sourcenote { font-size: 90%; /* source_notes.font.size */ padding: 4px; /* source_notes.padding */ } #pzveloiefp .gt_left { text-align: left; } #pzveloiefp .gt_center { text-align: center; } #pzveloiefp .gt_right { text-align: right; font-variant-numeric: tabular-nums; } #pzveloiefp .gt_font_normal { font-weight: normal; } #pzveloiefp .gt_font_bold { font-weight: bold; } #pzveloiefp .gt_font_italic { font-style: italic; } #pzveloiefp .gt_super { font-size: 65%; } #pzveloiefp .gt_footnote_marks { font-style: italic; font-size: 65%; } Sales Channel unique_dates start_date end_date total_sales days_span Online 38 2011-05-01 2014-06-01 29358677 1127 days Sales Rep 34 2011-05-01 2014-05-01 80487704 1096 days As this table shows, the Sales Rep dates don’t match the Online dates. They start on the same date, but have a different end. The Online dates include 2 months that are not included in the Sales Rep sales (which are the main sales channel by dollar volume). 8.7.2 Monthly variation compared to a trend line Jumping to the trend line comparison, we see that the big the source of variation is on the Sales Rep side. ggplot( data = monthly_sales_w_channel, aes( x = orderdate, y = total_soh_dollars ) ) + geom_line() + geom_smooth(se = FALSE) + facet_grid(&quot;onlineorderflag&quot;, scales = &quot;free&quot;) + scale_y_continuous(labels = dollar) + scale_x_date(date_breaks = &quot;year&quot;, date_labels = &quot;%Y&quot;, date_minor_breaks = &quot;3 months&quot;) + theme(plot.title = element_text(hjust = .5)) + # Center ggplot title labs( title = glue( &quot;AdventureWorks Monthly Sales Trend&quot; ), x = glue( &quot;Month - between &quot;, {format(min_soh_dt, &quot;%B %d, %Y&quot;)} , &quot; - &quot;, {format(max_soh_dt, &quot;%B %d, %Y&quot;)}), y = &quot;Sales Dollars&quot; ) Figure 8.7: Monthly Sales Trend The monthly gyrations are much larger on the Sales Rep side, amounting to differences in a million dollars compared to small monthly variations of around $25,000 for the Online orders. 8.7.3 Compare monthly lagged data by Sales Channel First consider month-to-month change. monthly_sales_w_channel_lagged_by_month &lt;- monthly_sales_w_channel %&gt;% group_by(onlineorderflag) %&gt;% mutate( lag_soh_count = lag(soh_count, 1), lag_soh_total_dollars = lag(total_soh_dollars, 1), pct_monthly_soh_dollar_change = (total_soh_dollars - lag_soh_total_dollars) / lag_soh_total_dollars * 100, pct_monthly_soh_count_change = (soh_count - lag_soh_count) / lag_soh_count * 100 ) The following table shows some wild changes in dollar amounts and number of sales from one month to the next. monthly_sales_w_channel_lagged_by_month %&gt;% filter(abs(pct_monthly_soh_count_change) &gt; 150 | abs(pct_monthly_soh_dollar_change) &gt; 150 ) %&gt;% ungroup() %&gt;% arrange(onlineorderflag, orderdate) %&gt;% mutate( total_soh_dollars = round(total_soh_dollars), lag_soh_total_dollars = round(lag_soh_total_dollars), pct_monthly_soh_dollar_change = round(pct_monthly_soh_dollar_change), pct_monthly_soh_count_change = round(pct_monthly_soh_count_change)) %&gt;% select(orderdate, onlineorderflag, total_soh_dollars, lag_soh_total_dollars, soh_count, lag_soh_count, pct_monthly_soh_dollar_change, pct_monthly_soh_count_change) %&gt;% # names() gt() %&gt;% fmt_number( columns = c(3:4), decimals = 0) %&gt;% fmt_percent( columns = c(7:8), decimals = 0) %&gt;% cols_label( onlineorderflag = &quot;Channel&quot;, total_soh_dollars = &quot;$ this Month&quot;, lag_soh_total_dollars = &quot;$ last Month&quot;, soh_count = &quot;# this Month&quot;, lag_soh_count = &quot;# last Month&quot;, pct_monthly_soh_dollar_change = &quot;$ change&quot;, pct_monthly_soh_count_change = &quot;# change&quot; ) html { font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, 'Helvetica Neue', 'Fira Sans', 'Droid Sans', Arial, sans-serif; } #dbgvzfontn .gt_table { display: table; border-collapse: collapse; margin-left: auto; /* table.margin.left */ margin-right: auto; /* table.margin.right */ color: #333333; font-size: 16px; /* table.font.size */ background-color: #FFFFFF; /* table.background.color */ width: auto; /* table.width */ border-top-style: solid; /* table.border.top.style */ border-top-width: 2px; /* table.border.top.width */ border-top-color: #A8A8A8; /* table.border.top.color */ border-bottom-style: solid; /* table.border.bottom.style */ border-bottom-width: 2px; /* table.border.bottom.width */ border-bottom-color: #A8A8A8; /* table.border.bottom.color */ } #dbgvzfontn .gt_heading { background-color: #FFFFFF; /* heading.background.color */ border-bottom-color: #FFFFFF; /* table.background.color */ border-left-style: hidden; /* heading.border.lr.style */ border-left-width: 1px; /* heading.border.lr.width */ border-left-color: #D3D3D3; /* heading.border.lr.color */ border-right-style: hidden; /* heading.border.lr.style */ border-right-width: 1px; /* heading.border.lr.width */ border-right-color: #D3D3D3; /* heading.border.lr.color */ } #dbgvzfontn .gt_title { color: #333333; font-size: 125%; /* heading.title.font.size */ font-weight: initial; /* heading.title.font.weight */ padding-top: 4px; /* heading.top.padding - not yet used */ padding-bottom: 4px; border-bottom-color: #FFFFFF; /* table.background.color */ border-bottom-width: 0; } #dbgvzfontn .gt_subtitle { color: #333333; font-size: 85%; /* heading.subtitle.font.size */ font-weight: initial; /* heading.subtitle.font.weight */ padding-top: 0; padding-bottom: 4px; /* heading.bottom.padding - not yet used */ border-top-color: #FFFFFF; /* table.background.color */ border-top-width: 0; } #dbgvzfontn .gt_bottom_border { border-bottom-style: solid; /* heading.border.bottom.style */ border-bottom-width: 2px; /* heading.border.bottom.width */ border-bottom-color: #D3D3D3; /* heading.border.bottom.color */ } #dbgvzfontn .gt_column_spanner { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; padding-top: 4px; padding-bottom: 4px; } #dbgvzfontn .gt_col_headings { border-top-style: solid; /* column_labels.border.top.style */ border-top-width: 2px; /* column_labels.border.top.width */ border-top-color: #D3D3D3; /* column_labels.border.top.color */ border-bottom-style: solid; /* column_labels.border.bottom.style */ border-bottom-width: 2px; /* column_labels.border.bottom.width */ border-bottom-color: #D3D3D3; /* column_labels.border.bottom.color */ border-left-style: none; /* column_labels.border.lr.style */ border-left-width: 1px; /* column_labels.border.lr.width */ border-left-color: #D3D3D3; /* column_labels.border.lr.color */ border-right-style: none; /* column_labels.border.lr.style */ border-right-width: 1px; /* column_labels.border.lr.width */ border-right-color: #D3D3D3; /* column_labels.border.lr.color */ } #dbgvzfontn .gt_col_heading { color: #333333; background-color: #FFFFFF; /* column_labels.background.color */ font-size: 100%; /* column_labels.font.size */ font-weight: initial; /* column_labels.font.weight */ text-transform: inherit; /* column_labels.text_transform */ vertical-align: middle; padding: 5px; margin: 10px; overflow-x: hidden; } #dbgvzfontn .gt_sep_right { border-right: 5px solid #FFFFFF; } #dbgvzfontn .gt_group_heading { padding: 8px; /* row_group.padding */ color: #333333; background-color: #FFFFFF; /* row_group.background.color */ font-size: 100%; /* row_group.font.size */ font-weight: initial; /* row_group.font.weight */ text-transform: inherit; /* row_group.text_transform */ border-top-style: solid; /* row_group.border.top.style */ border-top-width: 2px; /* row_group.border.top.width */ border-top-color: #D3D3D3; /* row_group.border.top.color */ border-bottom-style: solid; /* row_group.border.bottom.style */ border-bottom-width: 2px; /* row_group.border.bottom.width */ border-bottom-color: #D3D3D3; /* row_group.border.bottom.color */ border-left-style: none; /* row_group.border.left.style */ border-left-width: 1px; /* row_group.border.left.width */ border-left-color: #D3D3D3; /* row_group.border.left.color */ border-right-style: none; /* row_group.border.right.style */ border-right-width: 1px; /* row_group.border.right.width */ border-right-color: #D3D3D3; /* row_group.border.right.color */ vertical-align: middle; } #dbgvzfontn .gt_empty_group_heading { padding: 0.5px; color: #333333; background-color: #FFFFFF; /* row_group.background.color */ font-size: 100%; /* row_group.font.size */ font-weight: initial; /* row_group.font.weight */ border-top-style: solid; /* row_group.border.top.style */ border-top-width: 2px; /* row_group.border.top.width */ border-top-color: #D3D3D3; /* row_group.border.top.color */ border-bottom-style: solid; /* row_group.border.bottom.style */ border-bottom-width: 2px; /* row_group.border.bottom.width */ border-bottom-color: #D3D3D3; /* row_group.border.bottom.color */ vertical-align: middle; } #dbgvzfontn .gt_striped { background-color: #8080800D; /* row.striping.background_color */ } #dbgvzfontn .gt_from_md > :first-child { margin-top: 0; } #dbgvzfontn .gt_from_md > :last-child { margin-bottom: 0; } #dbgvzfontn .gt_row { padding-top: 8px; /* data_row.padding */ padding-bottom: 8px; /* data_row.padding */ padding-left: 5px; padding-right: 5px; margin: 10px; border-top-style: solid; /* table_body.hlines.style */ border-top-width: 1px; /* table_body.hlines.width */ border-top-color: #D3D3D3; /* table_body.hlines.color */ border-left-style: none; /* table_body.vlines.style */ border-left-width: 1px; /* table_body.vlines.width */ border-left-color: #D3D3D3; /* table_body.vlines.color */ border-right-style: none; /* table_body.vlines.style */ border-right-width: 1px; /* table_body.vlines.width */ border-right-color: #D3D3D3; /* table_body.vlines.color */ vertical-align: middle; overflow-x: hidden; } #dbgvzfontn .gt_stub { color: #333333; background-color: #FFFFFF; /* stub.background.color */ font-weight: initial; /* stub.font.weight */ text-transform: inherit; /* stub.text_transform */ border-right-style: solid; /* stub.border.style */ border-right-width: 2px; /* stub.border.width */ border-right-color: #D3D3D3; /* stub.border.color */ padding-left: 12px; } #dbgvzfontn .gt_summary_row { color: #333333; background-color: #FFFFFF; /* summary_row.background.color */ text-transform: inherit; /* summary_row.text_transform */ padding-top: 8px; /* summary_row.padding */ padding-bottom: 8px; /* summary_row.padding */ padding-left: 5px; padding-right: 5px; } #dbgvzfontn .gt_first_summary_row { padding-top: 8px; /* summary_row.padding */ padding-bottom: 8px; /* summary_row.padding */ padding-left: 5px; padding-right: 5px; border-top-style: solid; /* summary_row.border.style */ border-top-width: 2px; /* summary_row.border.width */ border-top-color: #D3D3D3; /* summary_row.border.color */ } #dbgvzfontn .gt_grand_summary_row { color: #333333; background-color: #FFFFFF; /* grand_summary_row.background.color */ text-transform: inherit; /* grand_summary_row.text_transform */ padding-top: 8px; /* grand_summary_row.padding */ padding-bottom: 8px; /* grand_summary_row.padding */ padding-left: 5px; padding-right: 5px; } #dbgvzfontn .gt_first_grand_summary_row { padding-top: 8px; /* grand_summary_row.padding */ padding-bottom: 8px; /* grand_summary_row.padding */ padding-left: 5px; padding-right: 5px; border-top-style: double; /* grand_summary_row.border.style */ border-top-width: 6px; /* grand_summary_row.border.width */ border-top-color: #D3D3D3; /* grand_summary_row.border.color */ } #dbgvzfontn .gt_table_body { border-top-style: solid; /* table_body.border.top.style */ border-top-width: 2px; /* table_body.border.top.width */ border-top-color: #D3D3D3; /* table_body.border.top.color */ border-bottom-style: solid; /* table_body.border.bottom.style */ border-bottom-width: 2px; /* table_body.border.bottom.width */ border-bottom-color: #D3D3D3; /* table_body.border.bottom.color */ } #dbgvzfontn .gt_footnotes { color: #333333; background-color: #FFFFFF; /* footnotes.background.color */ border-bottom-style: none; /* footnotes.border.bottom.style */ border-bottom-width: 2px; /* footnotes.border.bottom.width */ border-bottom-color: #D3D3D3; /* footnotes.border.bottom.color */ border-left-style: none; /* footnotes.border.lr.color */ border-left-width: 2px; /* footnotes.border.lr.color */ border-left-color: #D3D3D3; /* footnotes.border.lr.color */ border-right-style: none; /* footnotes.border.lr.color */ border-right-width: 2px; /* footnotes.border.lr.color */ border-right-color: #D3D3D3; /* footnotes.border.lr.color */ } #dbgvzfontn .gt_footnote { margin: 0px; font-size: 90%; /* footnotes.font.size */ padding: 4px; /* footnotes.padding */ } #dbgvzfontn .gt_sourcenotes { color: #333333; background-color: #FFFFFF; /* source_notes.background.color */ border-bottom-style: none; /* source_notes.border.bottom.style */ border-bottom-width: 2px; /* source_notes.border.bottom.width */ border-bottom-color: #D3D3D3; /* source_notes.border.bottom.color */ border-left-style: none; /* source_notes.border.lr.style */ border-left-width: 2px; /* source_notes.border.lr.style */ border-left-color: #D3D3D3; /* source_notes.border.lr.style */ border-right-style: none; /* source_notes.border.lr.style */ border-right-width: 2px; /* source_notes.border.lr.style */ border-right-color: #D3D3D3; /* source_notes.border.lr.style */ } #dbgvzfontn .gt_sourcenote { font-size: 90%; /* source_notes.font.size */ padding: 4px; /* source_notes.padding */ } #dbgvzfontn .gt_left { text-align: left; } #dbgvzfontn .gt_center { text-align: center; } #dbgvzfontn .gt_right { text-align: right; font-variant-numeric: tabular-nums; } #dbgvzfontn .gt_font_normal { font-weight: normal; } #dbgvzfontn .gt_font_bold { font-weight: bold; } #dbgvzfontn .gt_font_italic { font-style: italic; } #dbgvzfontn .gt_super { font-size: 65%; } #dbgvzfontn .gt_footnote_marks { font-style: italic; font-size: 65%; } orderdate Channel $ this Month $ last Month # this Month # last Month $ change # change 2011-06-01 Online 458,911 14,477 141 5 307,000&percnt; 272,000&percnt; 2013-07-01 Online 847,139 860,141 1564 533 &minus;200&percnt; 19,300&percnt; 2011-07-01 Sales Rep 1,538,408 489,329 75 38 21,400&percnt; 9,700&percnt; 2012-01-01 Sales Rep 3,356,069 713,117 143 40 37,100&percnt; 25,800&percnt; 2012-03-01 Sales Rep 2,269,117 882,900 85 37 15,700&percnt; 13,000&percnt; 2014-03-01 Sales Rep 5,526,352 3,231 271 3 17,096,000&percnt; 893,300&percnt; 2014-05-01 Sales Rep 3,415,479 1,285 179 2 26,573,900&percnt; 885,000&percnt; We suspect that the business has changed a lot with the advent of Online orders. 8.8 Detect and diagnose the day of the month problem There have been several indications that Sales Rep sales are recorded once a month while Online sales are recorded on a daily basis. 8.8.1 Sales Rep Orderdate Distribution Look at the dates when sales are entered for sales by Sales Reps. The following query / plot combination shows this pattern. and the exception for transactions entered on the first day of the month. tbl(con, in_schema(&quot;sales&quot;, &quot;salesorderheader&quot;)) %&gt;% filter(onlineorderflag == FALSE) %&gt;% # Drop online orders mutate(orderday = day(orderdate)) %&gt;% count(orderday, name = &quot;Orders&quot;) %&gt;% collect() %&gt;% full_join(tibble(orderday = seq(1:31))) %&gt;% mutate(orderday = as.factor(orderday)) %&gt;% ggplot(aes(orderday, Orders)) + geom_col() + coord_flip() + labs(title = &quot;The first day of the month looks odd&quot;, x = &quot;Day Number&quot;) ## Joining, by = &quot;orderday&quot; ## Warning: Removed 26 rows containing missing values (position_stack). Figure 8.8: Days of the month with Sales Rep activity recorded We can check on which months have orders entered on the first of the month. sales_rep_day_of_month_sales &lt;- tbl(con, in_schema(&quot;sales&quot;, &quot;salesorderheader&quot;)) %&gt;% filter(onlineorderflag == FALSE) %&gt;% # Drop online orders select(orderdate, subtotal) %&gt;% mutate( year = year(orderdate), month = month(orderdate), day = day(orderdate) ) %&gt;% count(year, month, day) %&gt;% collect() %&gt;% pivot_wider(names_from = day, values_from = n, names_prefix = &quot;day_&quot;, values_fill = list(day_1 = 0, day_28 = 0, day_29 = 0, day_30 = 0, day_31 = 0) ) %&gt;% as.data.frame() %&gt;% select(year, month, day_1, day_28, day_29, day_30, day_31) %&gt;% filter(day_1 &gt; 0) %&gt;% arrange(year, month) sales_rep_day_of_month_sales ## year month day_1 day_28 day_29 day_30 day_31 ## 1 2011 7 75 NA NA NA NA ## 2 2011 8 60 NA NA NA 40 ## 3 2011 10 90 NA NA NA 63 ## 4 2011 12 40 NA NA NA NA ## 5 2012 1 79 NA 64 NA NA ## 6 2014 3 91 NA NA 2 178 ## 7 2014 5 179 NA NA NA NA There are two months with multiple sales rep order days for 2011, (11/08 and 11/10), one for 2012, (1201), and two in 2014, (14/01 and 14/03). The 14/03 is the only three day sales rep order month. Are there months where there were no sales recorded for the sales reps? There are two approaches. The first is to generate a list of months between the beginning and end of history and compare that to the Sales Rep records monthly_sales_rep_sales &lt;- monthly_sales_w_channel %&gt;% filter(onlineorderflag == &quot;Sales Rep&quot;) %&gt;% mutate(orderdate = as.Date(floor_date(orderdate, &quot;month&quot;))) %&gt;% count(orderdate) str(monthly_sales_rep_sales) ## Classes &#39;tbl_df&#39;, &#39;tbl&#39; and &#39;data.frame&#39;: 34 obs. of 2 variables: ## $ orderdate: Date, format: &quot;2011-05-01&quot; &quot;2011-07-01&quot; ... ## $ n : int 1 1 1 1 1 1 1 1 1 1 ... date_list &lt;- tibble(month_date = seq.Date(floor_date(as.Date(min_soh_dt), &quot;month&quot;), floor_date(as.Date(max_soh_dt), &quot;month&quot;), by = &quot;month&quot;), date_exists = FALSE) date_list %&gt;% anti_join(monthly_sales_rep_sales, by = c(&quot;month_date&quot; = &quot;orderdate&quot;) ) ## # A tibble: 4 x 2 ## month_date date_exists ## &lt;date&gt; &lt;lgl&gt; ## 1 2011-06-01 FALSE ## 2 2011-09-01 FALSE ## 3 2011-11-01 FALSE ## 4 2014-06-01 FALSE June, September, and November are missing for 2011. June for 2014 The second approach is to use the dates found in the database for online orders. Defining “complete” may not always be as simple as generating a complete list of months. sales_order_header_online &lt;- tbl(con, in_schema(&quot;sales&quot;, &quot;salesorderheader&quot;)) %&gt;% filter(onlineorderflag == TRUE) %&gt;% mutate( orderdate = date_trunc(&#39;month&#39;, orderdate) ) %&gt;% count(orderdate, name = &quot;online_count&quot;) sales_order_header_sales_rep &lt;- tbl(con, in_schema(&quot;sales&quot;, &quot;salesorderheader&quot;)) %&gt;% filter(onlineorderflag == FALSE) %&gt;% mutate( orderdate = date_trunc(&#39;month&#39;, orderdate) ) %&gt;% count(orderdate, name = &quot;sales_rep_count&quot;) missing_dates &lt;- sales_order_header_sales_rep %&gt;% full_join(sales_order_header_online) %&gt;% show_query() %&gt;% collect() ## Joining, by = &quot;orderdate&quot; ## &lt;SQL&gt; ## SELECT COALESCE(&quot;LHS&quot;.&quot;orderdate&quot;, &quot;RHS&quot;.&quot;orderdate&quot;) AS &quot;orderdate&quot;, &quot;LHS&quot;.&quot;sales_rep_count&quot; AS &quot;sales_rep_count&quot;, &quot;RHS&quot;.&quot;online_count&quot; AS &quot;online_count&quot; ## FROM (SELECT &quot;orderdate&quot;, COUNT(*) AS &quot;sales_rep_count&quot; ## FROM (SELECT &quot;salesorderid&quot;, &quot;revisionnumber&quot;, date_trunc(&#39;month&#39;, &quot;orderdate&quot;) AS &quot;orderdate&quot;, &quot;duedate&quot;, &quot;shipdate&quot;, &quot;status&quot;, &quot;onlineorderflag&quot;, &quot;purchaseordernumber&quot;, &quot;accountnumber&quot;, &quot;customerid&quot;, &quot;salespersonid&quot;, &quot;territoryid&quot;, &quot;billtoaddressid&quot;, &quot;shiptoaddressid&quot;, &quot;shipmethodid&quot;, &quot;creditcardid&quot;, &quot;creditcardapprovalcode&quot;, &quot;currencyrateid&quot;, &quot;subtotal&quot;, &quot;taxamt&quot;, &quot;freight&quot;, &quot;totaldue&quot;, &quot;comment&quot;, &quot;rowguid&quot;, &quot;modifieddate&quot; ## FROM (SELECT * ## FROM sales.salesorderheader ## WHERE (&quot;onlineorderflag&quot; = FALSE)) &quot;dbplyr_010&quot;) &quot;dbplyr_011&quot; ## GROUP BY &quot;orderdate&quot;) &quot;LHS&quot; ## FULL JOIN (SELECT &quot;orderdate&quot;, COUNT(*) AS &quot;online_count&quot; ## FROM (SELECT &quot;salesorderid&quot;, &quot;revisionnumber&quot;, date_trunc(&#39;month&#39;, &quot;orderdate&quot;) AS &quot;orderdate&quot;, &quot;duedate&quot;, &quot;shipdate&quot;, &quot;status&quot;, &quot;onlineorderflag&quot;, &quot;purchaseordernumber&quot;, &quot;accountnumber&quot;, &quot;customerid&quot;, &quot;salespersonid&quot;, &quot;territoryid&quot;, &quot;billtoaddressid&quot;, &quot;shiptoaddressid&quot;, &quot;shipmethodid&quot;, &quot;creditcardid&quot;, &quot;creditcardapprovalcode&quot;, &quot;currencyrateid&quot;, &quot;subtotal&quot;, &quot;taxamt&quot;, &quot;freight&quot;, &quot;totaldue&quot;, &quot;comment&quot;, &quot;rowguid&quot;, &quot;modifieddate&quot; ## FROM (SELECT * ## FROM sales.salesorderheader ## WHERE (&quot;onlineorderflag&quot; = TRUE)) &quot;dbplyr_012&quot;) &quot;dbplyr_013&quot; ## GROUP BY &quot;orderdate&quot;) &quot;RHS&quot; ## ON (&quot;LHS&quot;.&quot;orderdate&quot; = &quot;RHS&quot;.&quot;orderdate&quot;) missing_dates &lt;- sales_order_header_online %&gt;% anti_join(sales_order_header_sales_rep) %&gt;% arrange(orderdate) %&gt;% collect() ## Joining, by = &quot;orderdate&quot; missing_dates ## # A tibble: 4 x 2 ## orderdate online_count ## &lt;dttm&gt; &lt;int&gt; ## 1 2011-06-01 00:00:00 141 ## 2 2011-09-01 00:00:00 157 ## 3 2011-11-01 00:00:00 230 ## 4 2014-06-01 00:00:00 939 str(missing_dates) ## Classes &#39;tbl_df&#39;, &#39;tbl&#39; and &#39;data.frame&#39;: 4 obs. of 2 variables: ## $ orderdate : POSIXct, format: &quot;2011-06-01&quot; &quot;2011-09-01&quot; ... ## $ online_count: int 141 157 230 939 And in this case they agree! discuss February issues. and stuff. look at each year sepraately as a diagnostic Use the same pivot strategy on the corrected data. difference between detective work with a graph and just print it out. “now I see what’s driving the hint.” We have xx months when we add the month before and the month after the suspicious months. We don’t know whether the problem postings have been carried forward or backward. We check for and eliminate duplicates as well. Most of the Sales Reps’ orders are entered on a single day of the month, unique days = 1. It is possible that these are monthly recurring orders that get released on a given day of the month. If that is the case, what are the Sales Reps doing the rest of the month? ** ?? The lines with multiple days, unique_days &gt; 1, have a noticeable higher number of orders, so_cnt, and associated so dollars.?? ** 8.9 Correcting the order date for Sales Reps 8.9.1 Define a date correction function in R This code does the date-correction work on the R side: monthly_sales_rep_adjusted &lt;- tbl(con, in_schema(&quot;sales&quot;, &quot;salesorderheader&quot;)) %&gt;% filter(onlineorderflag == FALSE) %&gt;% select(orderdate, subtotal, onlineorderflag) %&gt;% group_by(orderdate) %&gt;% summarize( total_soh_dollars = round(sum(subtotal, na.rm = TRUE), 2), soh_count = n() ) %&gt;% mutate( orderdate = as.Date(orderdate), day = day(orderdate) ) %&gt;% collect() %&gt;% ungroup() %&gt;% mutate( adjusted_orderdate = case_when( day == 1L ~ orderdate -1, TRUE ~ orderdate ), year_month = floor_date(adjusted_orderdate, &quot;month&quot;) ) %&gt;% group_by(year_month) %&gt;% summarize( total_soh_dollars = round(sum(total_soh_dollars, na.rm = TRUE), 2), soh_count = sum(soh_count) ) %&gt;% ungroup() Inspect: str(monthly_sales_rep_adjusted) ## Classes &#39;tbl_df&#39;, &#39;tbl&#39; and &#39;data.frame&#39;: 36 obs. of 3 variables: ## $ year_month : Date, format: &quot;2011-05-01&quot; &quot;2011-06-01&quot; ... ## $ total_soh_dollars: num 489329 1538408 1165897 844721 2324136 ... ## $ soh_count : int 38 75 60 40 90 63 40 79 64 37 ... monthly_sales_rep_adjusted %&gt;% filter(year(year_month) %in% c(2011,2014)) ## # A tibble: 12 x 3 ## year_month total_soh_dollars soh_count ## &lt;date&gt; &lt;dbl&gt; &lt;int&gt; ## 1 2011-05-01 489329. 38 ## 2 2011-06-01 1538408. 75 ## 3 2011-07-01 1165897. 60 ## 4 2011-08-01 844721 40 ## 5 2011-09-01 2324136. 90 ## 6 2011-10-01 1702945. 63 ## 7 2011-11-01 713117. 40 ## 8 2011-12-01 1900789. 79 ## 9 2014-01-01 2738752. 175 ## 10 2014-02-01 2207772. 94 ## 11 2014-03-01 3321810. 180 ## 12 2014-04-01 3416764. 181 8.9.2 Define and store a PostgreSQL function to correct the date The following code defines a function on the server side to correct the date: dbExecute( con, &quot;CREATE OR REPLACE FUNCTION so_adj_date(so_date timestamp, ONLINE_ORDER boolean) RETURNS timestamp AS $$ BEGIN IF (ONLINE_ORDER) THEN RETURN (SELECT so_date); ELSE RETURN(SELECT CASE WHEN EXTRACT(DAY FROM so_date) = 1 THEN so_date - &#39;1 day&#39;::interval ELSE so_date END ); END IF; END; $$ LANGUAGE PLPGSQL; &quot; ) ## [1] 0 8.9.3 Use the PostgreSQL function If you can do the heavy lifting on the database side, that’s good. R can do it, but it’s best for finding the issues. monthly_sales_rep_adjusted_with_psql_function &lt;- tbl(con, in_schema(&quot;sales&quot;, &quot;salesorderheader&quot;)) %&gt;% select(orderdate, subtotal, onlineorderflag) %&gt;% mutate( orderdate = as.Date(orderdate)) %&gt;% mutate(adjusted_orderdate = as.Date(so_adj_date(orderdate, onlineorderflag))) %&gt;% filter(onlineorderflag == FALSE) %&gt;% group_by(adjusted_orderdate) %&gt;% summarize( total_soh_dollars = round(sum(subtotal, na.rm = TRUE), 2), soh_count = n() ) %&gt;% collect() %&gt;% mutate( year_month = floor_date(adjusted_orderdate, &quot;month&quot;)) %&gt;% group_by(year_month) %&gt;% ungroup() %&gt;% arrange(year_month) monthly_sales_rep_adjusted_with_psql_function %&gt;% filter(year(year_month) %in% c(2011,2014)) ## # A tibble: 14 x 4 ## adjusted_orderdate total_soh_dollars soh_count year_month ## &lt;date&gt; &lt;dbl&gt; &lt;int&gt; &lt;date&gt; ## 1 2011-05-31 489329. 38 2011-05-01 ## 2 2011-06-30 1538408. 75 2011-06-01 ## 3 2011-07-31 1165897. 60 2011-07-01 ## 4 2011-08-31 844721 40 2011-08-01 ## 5 2011-09-30 2324136. 90 2011-09-01 ## 6 2011-10-31 1702945. 63 2011-10-01 ## 7 2011-11-30 713117. 40 2011-11-01 ## 8 2011-12-31 1900789. 79 2011-12-01 ## 9 2014-01-28 1565. 2 2014-01-01 ## 10 2014-01-29 2737188. 173 2014-01-01 ## 11 2014-02-28 2207772. 94 2014-02-01 ## 12 2014-03-30 7291. 2 2014-03-01 ## 13 2014-03-31 3314519. 178 2014-03-01 ## 14 2014-04-30 3416764. 181 2014-04-01 There’s one minor difference between the two: all_equal(monthly_sales_rep_adjusted, monthly_sales_rep_adjusted_with_psql_function) ## [1] &quot;Cols in y but not x: `adjusted_orderdate`. &quot; 8.9.4 Monthly Sales by Order Type with corrected dates – relative to a trend line monthly_sales_rep_as_is &lt;- monthly_sales_w_channel %&gt;% filter(onlineorderflag == &quot;Sales Rep&quot;) ggplot( data = monthly_sales_rep_adjusted, aes(x = year_month, y = soh_count) ) + geom_line(alpha = .5) + geom_smooth(se = FALSE) + geom_smooth( data = monthly_sales_rep_as_is, aes( orderdate, soh_count ), color = &quot;red&quot;, alpha = .5, se = FALSE ) + theme(plot.title = element_text(hjust = .5)) + # Center ggplot title labs( title = glue( &quot;Number of Sales per month using corrected dates\\n&quot;, &quot;Counting Sales Order Header records&quot; ), x = paste0(&quot;Monthly - between &quot;, min_soh_dt, &quot; - &quot;, max_soh_dt), y = &quot;Number of Sales Recorded&quot; ) monthly_sales_rep_as_is &lt;- monthly_sales_w_channel %&gt;% filter(onlineorderflag == &quot;Sales Rep&quot;) %&gt;% mutate(orderdate = as.Date(floor_date(orderdate, unit = &quot;month&quot;))) %&gt;% group_by(orderdate) %&gt;% summarize( total_soh_dollars = round(sum(total_soh_dollars, na.rm = TRUE), 2), soh_count = sum(soh_count) ) monthly_sales_rep_as_is %&gt;% filter(year(orderdate) %in% c(2011,2014)) ## # A tibble: 10 x 3 ## orderdate total_soh_dollars soh_count ## &lt;date&gt; &lt;dbl&gt; &lt;int&gt; ## 1 2011-05-01 489329. 38 ## 2 2011-07-01 1538408. 75 ## 3 2011-08-01 2010618. 100 ## 4 2011-10-01 4027080. 153 ## 5 2011-12-01 713117. 40 ## 6 2014-01-01 2738752. 175 ## 7 2014-02-01 3231. 3 ## 8 2014-03-01 5526352. 271 ## 9 2014-04-01 1285. 2 ## 10 2014-05-01 3415479. 179 ggplot( data = monthly_sales_rep_adjusted, aes(x = year_month, y = soh_count) ) + geom_line(alpha = .5 , color = &quot;green&quot;) + geom_point(alpha = .5 , color = &quot;green&quot;) + geom_point( data = monthly_sales_rep_as_is, aes( orderdate, soh_count), color = &quot;red&quot;, alpha = .5) + theme(plot.title = element_text(hjust = .5)) + # Center ggplot title annotate(geom = &quot;text&quot;, y = 250, x = as.Date(&quot;2011-06-01&quot;), label = &quot;Orange dots: original data\\nGreen dots: corrected data\\nBrown dots: unchanged&quot;, hjust = 0) + labs( title = glue( &quot;Number of Sales per Month&quot; ), subtitle = &quot;Original and corrected amounts&quot;, x = paste0(&quot;Monthly - between &quot;, min_soh_dt, &quot; - &quot;, max_soh_dt), y = &quot;Number of Sales Recorded&quot; ) Figure 8.9: Comparing monthly_sales_rep_adjusted and monthly_sales_rep_as_is mon_sales &lt;- monthly_sales_rep_adjusted %&gt;% rename(orderdate = year_month) sales_original_and_adjusted &lt;- bind_rows(mon_sales, monthly_sales_rep_as_is, .id = &quot;date_kind&quot;) Sales still seem to gyrate! We have found that sales rep sales data is often very strange. 8.10 Disconnect from the database and stop Docker dbDisconnect(con) # when running interactively use: connection_close(con) ## Warning in connection_release(conn@ptr): Already disconnected sp_docker_stop(&quot;adventureworks&quot;) "],
["chapter-lazy-evaluation-queries.html", "Chapter 9 Lazy Evaluation and Lazy Queries 9.1 Setup 9.2 R is lazy and comes with guardrails 9.3 Lazy evaluation and lazy queries 9.3.3 Source: lazy query [?? x 4] 9.3.3 Database: postgres 9.3.3 [postgres@localhost:5432/adventureworks] 9.4 Other resources", " Chapter 9 Lazy Evaluation and Lazy Queries This chapter: Reviews lazy loading, lazy evaluation and lazy query execution Demonstrates how dplyr code gets executed (and how R determines what is translated to SQL and what is processed locally by R) Offers some further resources on lazy loading, evaluation, execution, etc. 9.1 Setup The following packages are used in this chapter: library(tidyverse) library(DBI) library(RPostgres) library(dbplyr) require(knitr) library(bookdown) library(sqlpetr) library(connections) sleep_default &lt;- 3 Start your adventureworks container: sqlpetr::sp_docker_start(&quot;adventureworks&quot;) Sys.sleep(sleep_default) Connect to the database: # con &lt;- connection_open( # use in an interactive session con &lt;- dbConnect( # use in other settings RPostgres::Postgres(), # without the previous and next lines, some functions fail with bigint data # so change int64 to integer bigint = &quot;integer&quot;, user = Sys.getenv(&quot;DEFAULT_POSTGRES_USER_NAME&quot;), password = Sys.getenv(&quot;DEFAULT_POSTGRES_PASSWORD&quot;), dbname = &quot;adventureworks&quot;, host = &quot;localhost&quot;, port = 5432) 9.2 R is lazy and comes with guardrails By design, R is both a language and an interactive development environment (IDE). As a language, R tries to be as efficient as possible. As an IDE, R creates some guardrails to make it easy and safe to work with your data. For example getOption(&quot;max.print&quot;) prevents R from printing more rows of data than you want to handle in an interactive session, with a default of 99999 lines, which may or may not suit you. On the other hand SQL is a “Structured Query Language (SQL): a standard computer language for relational database management and data manipulation.”.1 SQL has various database-specific Interactive Development Environments (IDEs), such as pgAdmin for PostgreSQL. Roger Peng explains in R Programming for Data Science that: R has maintained the original S philosophy, which is that it provides a language that is both useful for interactive work, but contains a powerful programming language for developing new tools. This is complicated when R interacts with SQL. In a vignette for dbplyr Hadley Wickham explains: The most important difference between ordinary data frames and remote database queries is that your R code is translated into SQL and executed in the database on the remote server, not in R on your local machine. When working with databases, dplyr tries to be as lazy as possible: It never pulls data into R unless you explicitly ask for it. It delays doing any work until the last possible moment: it collects together everything you want to do and then sends it to the database in one step. Exactly when, which, and how much data is returned from the dbms is the topic of this chapter. Exactly how the data is represented in the dbms and then translated to a data frame is discussed in the DBI specification. Eventually, if you are interacting with a dbms from R you will need to understand the differences between lazy loading, lazy evaluation, and lazy queries. 9.2.1 Lazy loading “Lazy loading is always used for code in packages but is optional (selected by the package maintainer) for datasets in packages.”2 Lazy loading means that the code for a particular function doesn’t actually get loaded into memory until the last minute – when it’s actually being used. 9.2.2 Lazy evaluation Essentially “Lazy evaluation is a programming strategy that allows a symbol to be evaluated only when needed.”3 That means that lazy evaluation is about symbols such as function arguments4 when they are evaluated. Tidy evaluation complicates lazy evaluation.5 9.2.3 Lazy Queries “When you create a &quot;lazy&quot; query, you’re creating a pointer to a set of conditions on the database, but the query isn’t actually run and the data isn’t actually loaded until you call &quot;next&quot; or some similar method to actually fetch the data and load it into an object.”6 9.3 Lazy evaluation and lazy queries When does a lazy query trigger data retrieval? It depends on a lot of factors, as we explore below: 9.3.1 Create a black box query for experimentation Define the three tables discussed in the previous chapter to build a black box query: sales_person_table &lt;- tbl(con, in_schema(&quot;sales&quot;, &quot;salesperson&quot;)) %&gt;% select(-rowguid) %&gt;% rename(sale_info_updated = modifieddate) employee_table &lt;- tbl(con, in_schema(&quot;humanresources&quot;, &quot;employee&quot;)) %&gt;% select(-modifieddate, -rowguid) person_table &lt;- tbl(con, in_schema(&quot;person&quot;, &quot;person&quot;)) %&gt;% select(-modifieddate, -rowguid) Here is a typical string of dplyr verbs strung together with the magrittr %&gt;% pipe command that will be used to tease out the several different behaviors that a lazy query has when passed to different R functions. This query joins three connection objects into a query we’ll call Q: Q &lt;- sales_person_table %&gt;% dplyr::left_join(employee_table, by = c(&quot;businessentityid&quot; = &quot;businessentityid&quot;)) %&gt;% dplyr::left_join(person_table , by = c(&quot;businessentityid&quot; = &quot;businessentityid&quot;)) %&gt;% dplyr::select(firstname, lastname, salesytd, birthdate) The str function gives us a hint at how R is collecting information that can be used to construct and execute a query later on: str(Q, max.level = 2) ## List of 2 ## $ src:List of 2 ## ..$ con :Formal class &#39;PqConnection&#39; [package &quot;RPostgres&quot;] with 3 slots ## ..$ disco: NULL ## ..- attr(*, &quot;class&quot;)= chr [1:4] &quot;src_PqConnection&quot; &quot;src_dbi&quot; &quot;src_sql&quot; &quot;src&quot; ## $ ops:List of 4 ## ..$ name: chr &quot;select&quot; ## ..$ x :List of 4 ## .. ..- attr(*, &quot;class&quot;)= chr [1:3] &quot;op_join&quot; &quot;op_double&quot; &quot;op&quot; ## ..$ dots: list() ## ..$ args:List of 1 ## ..- attr(*, &quot;class&quot;)= chr [1:3] &quot;op_select&quot; &quot;op_single&quot; &quot;op&quot; ## - attr(*, &quot;class&quot;)= chr [1:5] &quot;tbl_PqConnection&quot; &quot;tbl_dbi&quot; &quot;tbl_sql&quot; &quot;tbl_lazy&quot; ... 9.3.2 Experiment overview Think of Q as a black box for the moment. The following examples will show how Q is interpreted differently by different functions. It’s important to remember in the following discussion that the “and then” operator (%&gt;%) actually wraps the subsequent code inside the preceding code so that Q %&gt;% print() is equivalent to print(Q). Notation A single green check indicates that some rows are returned. Two green checks indicate that all the rows are returned. The red X indicates that no rows are returned. R code Result Q %&gt;% print() Prints x rows; same as just entering Q Q %&gt;% dplyr::as_tibble() Forces Q to be a tibble Q %&gt;% head() Prints the first 6 rows Q %&gt;% tail() Error: tail() is not supported by sql sources Q %&gt;% length() Counts the rows in Q Q %&gt;% str() Shows the top 3 levels of the object Q Q %&gt;% nrow() Attempts to determine the number of rows Q %&gt;% dplyr::tally() Counts all the rows – on the dbms side Q %&gt;% dplyr::collect(n = 20) Prints 20 rows Q %&gt;% dplyr::collect(n = 20) %&gt;% head() Prints 6 rows Q %&gt;% ggplot Plots a barchart Q %&gt;% dplyr::show_query() Translates the lazy query object into SQL The next chapter will discuss how to build queries and how to explore intermediate steps. But first, the following subsections provide a more detailed discussion of each row in the preceding table. 9.3.3 Q %&gt;% print() Remember that Q %&gt;% print() is equivalent to print(Q) and the same as just entering Q on the command line. We use the magrittr pipe operator here, because chaining functions highlights how the same object behaves differently in each use. Q %&gt;% print() ## # Source: lazy query [?? x 4] ## # Database: postgres [postgres@localhost:5432/adventureworks] ## firstname lastname salesytd birthdate ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;date&gt; ## 1 Stephen Jiang 559698. 1951-10-17 ## 2 Michael Blythe 3763178. 1968-12-25 ## 3 Linda Mitchell 4251369. 1980-02-27 ## 4 Jillian Carson 3189418. 1962-08-29 ## 5 Garrett Vargas 1453719. 1975-02-04 ## 6 Tsvi Reiter 2315186. 1974-01-18 ## 7 Pamela Ansman-Wolfe 1352577. 1974-12-06 ## 8 Shu Ito 2458536. 1968-03-09 ## 9 José Saraiva 2604541. 1963-12-11 ## 10 David Campbell 1573013. 1974-02-11 ## # … with more rows R retrieves 10 observations and 3 columns. In its role as IDE, R has provided nicely formatted output that is similar to what it prints for a tibble, with descriptive information about the dataset and each column: 9.3.3 Source: lazy query [?? x 4] 9.3.3 Database: postgres 9.3.3 [postgres@localhost:5432/adventureworks] firstname lastname salesytd birthdate R has not determined how many rows are left to retrieve as it shows with [?? x 4] and ... with more rows in the data summary. 9.3.4 Q %&gt;% dplyr::as_tibble() In contrast to print(), the as_tibble() function causes R to download the whole table, using tibble’s default of displaying only the first 10 rows. Q %&gt;% dplyr::as_tibble() ## # A tibble: 17 x 4 ## firstname lastname salesytd birthdate ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;date&gt; ## 1 Stephen Jiang 559698. 1951-10-17 ## 2 Michael Blythe 3763178. 1968-12-25 ## 3 Linda Mitchell 4251369. 1980-02-27 ## 4 Jillian Carson 3189418. 1962-08-29 ## 5 Garrett Vargas 1453719. 1975-02-04 ## 6 Tsvi Reiter 2315186. 1974-01-18 ## 7 Pamela Ansman-Wolfe 1352577. 1974-12-06 ## 8 Shu Ito 2458536. 1968-03-09 ## 9 José Saraiva 2604541. 1963-12-11 ## 10 David Campbell 1573013. 1974-02-11 ## 11 Tete Mensa-Annan 1576562. 1978-01-05 ## 12 Syed Abbas 172524. 1975-01-11 ## 13 Lynn Tsoflias 1421811. 1977-02-14 ## 14 Amy Alberts 519906. 1957-09-20 ## 15 Rachel Valdez 1827067. 1975-07-09 ## 16 Jae Pak 4116871. 1968-03-17 ## 17 Ranjit Varkey Chudukatil 3121616. 1975-09-30 9.3.5 Q %&gt;% head() The head() function is very similar to print but has a different “max.print” value. Q %&gt;% head() ## # Source: lazy query [?? x 4] ## # Database: postgres [postgres@localhost:5432/adventureworks] ## firstname lastname salesytd birthdate ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;date&gt; ## 1 Stephen Jiang 559698. 1951-10-17 ## 2 Michael Blythe 3763178. 1968-12-25 ## 3 Linda Mitchell 4251369. 1980-02-27 ## 4 Jillian Carson 3189418. 1962-08-29 ## 5 Garrett Vargas 1453719. 1975-02-04 ## 6 Tsvi Reiter 2315186. 1974-01-18 9.3.6 Q %&gt;% tail() Produces an error, because Q does not hold all of the data, so it is not possible to list the last few items from the table: try( Q %&gt;% tail(), silent = FALSE, outFile = stdout() ) ## Error : tail() is not supported by sql sources 9.3.7 Q %&gt;% length() Because the Q object is relatively complex, using str() on it prints many lines. You can glimpse what’s going on with length(): Q %&gt;% length() ## [1] 2 9.3.8 Q %&gt;% str() Looking inside shows some of what’s going on (three levels deep): Q %&gt;% str(max.level = 3) ## List of 2 ## $ src:List of 2 ## ..$ con :Formal class &#39;PqConnection&#39; [package &quot;RPostgres&quot;] with 3 slots ## ..$ disco: NULL ## ..- attr(*, &quot;class&quot;)= chr [1:4] &quot;src_PqConnection&quot; &quot;src_dbi&quot; &quot;src_sql&quot; &quot;src&quot; ## $ ops:List of 4 ## ..$ name: chr &quot;select&quot; ## ..$ x :List of 4 ## .. ..$ name: chr &quot;join&quot; ## .. ..$ x :List of 2 ## .. .. ..- attr(*, &quot;class&quot;)= chr [1:5] &quot;tbl_PqConnection&quot; &quot;tbl_dbi&quot; &quot;tbl_sql&quot; &quot;tbl_lazy&quot; ... ## .. ..$ y :List of 2 ## .. .. ..- attr(*, &quot;class&quot;)= chr [1:5] &quot;tbl_PqConnection&quot; &quot;tbl_dbi&quot; &quot;tbl_sql&quot; &quot;tbl_lazy&quot; ... ## .. ..$ args:List of 4 ## .. ..- attr(*, &quot;class&quot;)= chr [1:3] &quot;op_join&quot; &quot;op_double&quot; &quot;op&quot; ## ..$ dots: list() ## ..$ args:List of 1 ## .. ..$ vars:List of 4 ## ..- attr(*, &quot;class&quot;)= chr [1:3] &quot;op_select&quot; &quot;op_single&quot; &quot;op&quot; ## - attr(*, &quot;class&quot;)= chr [1:5] &quot;tbl_PqConnection&quot; &quot;tbl_dbi&quot; &quot;tbl_sql&quot; &quot;tbl_lazy&quot; ... 9.3.9 Q %&gt;% nrow() Notice the difference between nrow() and tally(). The nrow functions returns NA and does not execute a query: Q %&gt;% nrow() ## [1] NA 9.3.10 Q %&gt;% dplyr::tally() The tally function actually counts all the rows. Q %&gt;% dplyr::tally() ## # Source: lazy query [?? x 1] ## # Database: postgres [postgres@localhost:5432/adventureworks] ## n ## &lt;int&gt; ## 1 17 The nrow() function knows that Q is a list. On the other hand, the tally() function tells SQL to go count all the rows. Notice that Q results in 1,000 rows – the same number of rows as film. 9.3.11 Q %&gt;% dplyr::collect() The dplyr::collect function triggers a call to the DBI:dbFetch() function behind the scenes, which forces R to download a specified number of rows: Q %&gt;% dplyr::collect(n = 20) ## # A tibble: 17 x 4 ## firstname lastname salesytd birthdate ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;date&gt; ## 1 Stephen Jiang 559698. 1951-10-17 ## 2 Michael Blythe 3763178. 1968-12-25 ## 3 Linda Mitchell 4251369. 1980-02-27 ## 4 Jillian Carson 3189418. 1962-08-29 ## 5 Garrett Vargas 1453719. 1975-02-04 ## 6 Tsvi Reiter 2315186. 1974-01-18 ## 7 Pamela Ansman-Wolfe 1352577. 1974-12-06 ## 8 Shu Ito 2458536. 1968-03-09 ## 9 José Saraiva 2604541. 1963-12-11 ## 10 David Campbell 1573013. 1974-02-11 ## 11 Tete Mensa-Annan 1576562. 1978-01-05 ## 12 Syed Abbas 172524. 1975-01-11 ## 13 Lynn Tsoflias 1421811. 1977-02-14 ## 14 Amy Alberts 519906. 1957-09-20 ## 15 Rachel Valdez 1827067. 1975-07-09 ## 16 Jae Pak 4116871. 1968-03-17 ## 17 Ranjit Varkey Chudukatil 3121616. 1975-09-30 Q %&gt;% dplyr::collect(n = 20) %&gt;% head() ## # A tibble: 6 x 4 ## firstname lastname salesytd birthdate ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;date&gt; ## 1 Stephen Jiang 559698. 1951-10-17 ## 2 Michael Blythe 3763178. 1968-12-25 ## 3 Linda Mitchell 4251369. 1980-02-27 ## 4 Jillian Carson 3189418. 1962-08-29 ## 5 Garrett Vargas 1453719. 1975-02-04 ## 6 Tsvi Reiter 2315186. 1974-01-18 The dplyr::collect function triggers the creation of a tibble and controls the number of rows that the DBMS sends to R. Notice that head only prints 6 of the 20 rows that R has retrieved. If you do not provide a value for the n argument, all of the rows will be retrieved into your R workspace. 9.3.12 Q %&gt;% ggplot Passing the Q object to ggplot executes the query and plots the result. Q %&gt;% ggplot2::ggplot(aes(birthdate, salesytd)) + geom_point() * Rewrite previous query and this comment with adventureworks in mind. Comment on the plot… 9.3.13 Q %&gt;% dplyr::show_query() Q %&gt;% dplyr::show_query() ## &lt;SQL&gt; ## SELECT &quot;firstname&quot;, &quot;lastname&quot;, &quot;salesytd&quot;, &quot;birthdate&quot; ## FROM (SELECT &quot;LHS&quot;.&quot;businessentityid&quot; AS &quot;businessentityid&quot;, &quot;LHS&quot;.&quot;territoryid&quot; AS &quot;territoryid&quot;, &quot;LHS&quot;.&quot;salesquota&quot; AS &quot;salesquota&quot;, &quot;LHS&quot;.&quot;bonus&quot; AS &quot;bonus&quot;, &quot;LHS&quot;.&quot;commissionpct&quot; AS &quot;commissionpct&quot;, &quot;LHS&quot;.&quot;salesytd&quot; AS &quot;salesytd&quot;, &quot;LHS&quot;.&quot;saleslastyear&quot; AS &quot;saleslastyear&quot;, &quot;LHS&quot;.&quot;sale_info_updated&quot; AS &quot;sale_info_updated&quot;, &quot;LHS&quot;.&quot;nationalidnumber&quot; AS &quot;nationalidnumber&quot;, &quot;LHS&quot;.&quot;loginid&quot; AS &quot;loginid&quot;, &quot;LHS&quot;.&quot;jobtitle&quot; AS &quot;jobtitle&quot;, &quot;LHS&quot;.&quot;birthdate&quot; AS &quot;birthdate&quot;, &quot;LHS&quot;.&quot;maritalstatus&quot; AS &quot;maritalstatus&quot;, &quot;LHS&quot;.&quot;gender&quot; AS &quot;gender&quot;, &quot;LHS&quot;.&quot;hiredate&quot; AS &quot;hiredate&quot;, &quot;LHS&quot;.&quot;salariedflag&quot; AS &quot;salariedflag&quot;, &quot;LHS&quot;.&quot;vacationhours&quot; AS &quot;vacationhours&quot;, &quot;LHS&quot;.&quot;sickleavehours&quot; AS &quot;sickleavehours&quot;, &quot;LHS&quot;.&quot;currentflag&quot; AS &quot;currentflag&quot;, &quot;LHS&quot;.&quot;organizationnode&quot; AS &quot;organizationnode&quot;, &quot;RHS&quot;.&quot;persontype&quot; AS &quot;persontype&quot;, &quot;RHS&quot;.&quot;namestyle&quot; AS &quot;namestyle&quot;, &quot;RHS&quot;.&quot;title&quot; AS &quot;title&quot;, &quot;RHS&quot;.&quot;firstname&quot; AS &quot;firstname&quot;, &quot;RHS&quot;.&quot;middlename&quot; AS &quot;middlename&quot;, &quot;RHS&quot;.&quot;lastname&quot; AS &quot;lastname&quot;, &quot;RHS&quot;.&quot;suffix&quot; AS &quot;suffix&quot;, &quot;RHS&quot;.&quot;emailpromotion&quot; AS &quot;emailpromotion&quot;, &quot;RHS&quot;.&quot;additionalcontactinfo&quot; AS &quot;additionalcontactinfo&quot;, &quot;RHS&quot;.&quot;demographics&quot; AS &quot;demographics&quot; ## FROM (SELECT &quot;LHS&quot;.&quot;businessentityid&quot; AS &quot;businessentityid&quot;, &quot;LHS&quot;.&quot;territoryid&quot; AS &quot;territoryid&quot;, &quot;LHS&quot;.&quot;salesquota&quot; AS &quot;salesquota&quot;, &quot;LHS&quot;.&quot;bonus&quot; AS &quot;bonus&quot;, &quot;LHS&quot;.&quot;commissionpct&quot; AS &quot;commissionpct&quot;, &quot;LHS&quot;.&quot;salesytd&quot; AS &quot;salesytd&quot;, &quot;LHS&quot;.&quot;saleslastyear&quot; AS &quot;saleslastyear&quot;, &quot;LHS&quot;.&quot;sale_info_updated&quot; AS &quot;sale_info_updated&quot;, &quot;RHS&quot;.&quot;nationalidnumber&quot; AS &quot;nationalidnumber&quot;, &quot;RHS&quot;.&quot;loginid&quot; AS &quot;loginid&quot;, &quot;RHS&quot;.&quot;jobtitle&quot; AS &quot;jobtitle&quot;, &quot;RHS&quot;.&quot;birthdate&quot; AS &quot;birthdate&quot;, &quot;RHS&quot;.&quot;maritalstatus&quot; AS &quot;maritalstatus&quot;, &quot;RHS&quot;.&quot;gender&quot; AS &quot;gender&quot;, &quot;RHS&quot;.&quot;hiredate&quot; AS &quot;hiredate&quot;, &quot;RHS&quot;.&quot;salariedflag&quot; AS &quot;salariedflag&quot;, &quot;RHS&quot;.&quot;vacationhours&quot; AS &quot;vacationhours&quot;, &quot;RHS&quot;.&quot;sickleavehours&quot; AS &quot;sickleavehours&quot;, &quot;RHS&quot;.&quot;currentflag&quot; AS &quot;currentflag&quot;, &quot;RHS&quot;.&quot;organizationnode&quot; AS &quot;organizationnode&quot; ## FROM (SELECT &quot;businessentityid&quot;, &quot;territoryid&quot;, &quot;salesquota&quot;, &quot;bonus&quot;, &quot;commissionpct&quot;, &quot;salesytd&quot;, &quot;saleslastyear&quot;, &quot;modifieddate&quot; AS &quot;sale_info_updated&quot; ## FROM sales.salesperson) &quot;LHS&quot; ## LEFT JOIN (SELECT &quot;businessentityid&quot;, &quot;nationalidnumber&quot;, &quot;loginid&quot;, &quot;jobtitle&quot;, &quot;birthdate&quot;, &quot;maritalstatus&quot;, &quot;gender&quot;, &quot;hiredate&quot;, &quot;salariedflag&quot;, &quot;vacationhours&quot;, &quot;sickleavehours&quot;, &quot;currentflag&quot;, &quot;organizationnode&quot; ## FROM humanresources.employee) &quot;RHS&quot; ## ON (&quot;LHS&quot;.&quot;businessentityid&quot; = &quot;RHS&quot;.&quot;businessentityid&quot;) ## ) &quot;LHS&quot; ## LEFT JOIN (SELECT &quot;businessentityid&quot;, &quot;persontype&quot;, &quot;namestyle&quot;, &quot;title&quot;, &quot;firstname&quot;, &quot;middlename&quot;, &quot;lastname&quot;, &quot;suffix&quot;, &quot;emailpromotion&quot;, &quot;additionalcontactinfo&quot;, &quot;demographics&quot; ## FROM person.person) &quot;RHS&quot; ## ON (&quot;LHS&quot;.&quot;businessentityid&quot; = &quot;RHS&quot;.&quot;businessentityid&quot;) ## ) &quot;dbplyr_009&quot; Hand-written SQL code to do the same job will probably look a lot nicer and could be more efficient, but functionally dplyr does the job. ## Disconnect from the database and stop Docker dbDisconnect(con) # or if using the connections package, use: # connection_close(con) sp_docker_stop(&quot;adventureworks&quot;) 9.4 Other resources Benjamin S. Baumer. 2017. A Grammar for Reproducible and Painless Extract-Transform-Load Operations on Medium Data. https://arxiv.org/abs/1708.07073 dplyr Reference documentation: Remote tables. https://dplyr.tidyverse.org/reference/index.html#section-remote-tables Data Carpentry. SQL Databases and R. https://datacarpentry.org/R-ecology-lesson/05-r-and-databases.html https://www.techopedia.com/definition/1245/structured-query-language-sql↩ https://cran.r-project.org/doc/manuals/r-release/R-ints.html#Lazy-loading↩ https://colinfay.me/lazyeval/↩ http://adv-r.had.co.nz/Functions.html#function-arguments↩ https://colinfay.me/tidyeval-1/↩ https://www.quora.com/What-is-a-lazy-query↩ "],
["chapter-lazy-evaluation-and-timing.html", "Chapter 10 Lazy Evaluation and Execution Environment 10.1 Setup 10.2 Disconnect from the database and stop Docker 10.3 Other resources", " Chapter 10 Lazy Evaluation and Execution Environment This chapter: Builds on the lazy loading discussion in the previous chapter Demonstrates how the use of the dplyr::collect() creates a boundary between code that is sent to a dbms and code that is executed locally 10.1 Setup The following packages are used in this chapter: library(tidyverse) library(DBI) library(RPostgres) library(dbplyr) require(knitr) library(bookdown) library(sqlpetr) sleep_default &lt;- 3 If you have not yet set up the Docker container with PostgreSQL and the dvdrental database, go back to [those instructions][Build the pet-sql Docker Image] to configure your environment. Otherwise, start your adventureworks container: sqlpetr::sp_docker_start(&quot;adventureworks&quot;) Sys.sleep(sleep_default) Connect to the database: con &lt;- dbConnect( RPostgres::Postgres(), # without the previous and next lines, some functions fail with bigint data # so change int64 to integer bigint = &quot;integer&quot;, host = &quot;localhost&quot;, user = Sys.getenv(&quot;DEFAULT_POSTGRES_USER_NAME&quot;), password = Sys.getenv(&quot;DEFAULT_POSTGRES_PASSWORD&quot;), dbname = &quot;adventureworks&quot;, port = 5432) Here is a simple string of dplyr verbs similar to the query used to illustrate issues in the last chapter: Note that in the previous example we follow this book’s convention of creating a connection object to each table and fully qualifying function names (e.g., specifying the package). In practice, it’s possible and convenient to use more abbreviated notation. Q &lt;- tbl(con, in_schema(&quot;sales&quot;, &quot;salesperson&quot;)) %&gt;% left_join(tbl(con, in_schema(&quot;humanresources&quot;, &quot;employee&quot;)), by = c(&quot;businessentityid&quot; = &quot;businessentityid&quot;)) %&gt;% select(birthdate, saleslastyear) Q ## # Source: lazy query [?? x 2] ## # Database: postgres [postgres@localhost:5432/adventureworks] ## birthdate saleslastyear ## &lt;date&gt; &lt;dbl&gt; ## 1 1951-10-17 0 ## 2 1968-12-25 1750406. ## 3 1980-02-27 1439156. ## 4 1962-08-29 1997186. ## 5 1975-02-04 1620277. ## 6 1974-01-18 1849641. ## 7 1974-12-06 1927059. ## 8 1968-03-09 2073506. ## 9 1963-12-11 2038235. ## 10 1974-02-11 1371635. ## # … with more rows 10.1.1 Experiment overview Think of Q as a black box for the moment. The following examples will show how Q is interpreted differently by different functions. It’s important to remember in the following discussion that the “and then” operator (%&gt;%) actually wraps the subsequent code inside the preceding code so that Q %&gt;% print() is equivalent to print(Q). Notation Symbol Explanation A single green check indicates that some rows are returned. Two green checks indicate that all the rows are returned. The red X indicates that no rows are returned. R code Result Time-based, execution environment issues Qc &lt;- Q %&gt;% count(saleslastyear, sort = TRUE) Extends the lazy query object The next chapter will discuss how to build queries and how to explore intermediate steps. But first, the following subsections provide a more detailed discussion of each row in the preceding table. 10.1.2 Time-based, execution environment issues Remember that if the expression is assigned to an object, it is not executed. If an expression is entered on the command line or appears in your script by itself, a print() function is implied. These two are different: Q %&gt;% sum(saleslastyear) Q_query &lt;- Q %&gt;% sum(saleslastyear) This behavior is the basis of a useful debugging and development process where queries are built up incrementally. 10.1.3 Q %&gt;% more dplyr Because the following statement implies a print() function at the end, we can run it repeatedly, adding dplyr expressions, and only get 10 rows back. Every time we add a dplyr expression to a chain, R will rewrite the SQL code. For example: As we understand more about the data, we simply add dplyr expressions to pinpoint what we are looking for: Q %&gt;% filter(saleslastyear &gt; 40) %&gt;% arrange(desc(saleslastyear)) ## # Source: lazy query [?? x 2] ## # Database: postgres [postgres@localhost:5432/adventureworks] ## # Ordered by: desc(saleslastyear) ## birthdate saleslastyear ## &lt;date&gt; &lt;dbl&gt; ## 1 1975-09-30 2396540. ## 2 1977-02-14 2278549. ## 3 1968-03-09 2073506. ## 4 1963-12-11 2038235. ## 5 1962-08-29 1997186. ## 6 1974-12-06 1927059. ## 7 1974-01-18 1849641. ## 8 1968-12-25 1750406. ## 9 1968-03-17 1635823. ## 10 1975-02-04 1620277. ## # … with more rows Q %&gt;% summarize(total_sales = sum(saleslastyear, na.rm = TRUE), sales_persons_count = n()) ## # Source: lazy query [?? x 2] ## # Database: postgres [postgres@localhost:5432/adventureworks] ## total_sales sales_persons_count ## &lt;dbl&gt; &lt;int&gt; ## 1 23685964. 17 When all the accumulated dplyr verbs are executed, they are submitted to the dbms and the number of rows that are returned follow the same rules as discussed above. ### Interspersing SQL and dplyr Q %&gt;% # mutate(birthdate = date(birthdate)) %&gt;% show_query() ## &lt;SQL&gt; ## SELECT &quot;birthdate&quot;, &quot;saleslastyear&quot; ## FROM (SELECT &quot;LHS&quot;.&quot;businessentityid&quot; AS &quot;businessentityid&quot;, &quot;LHS&quot;.&quot;territoryid&quot; AS &quot;territoryid&quot;, &quot;LHS&quot;.&quot;salesquota&quot; AS &quot;salesquota&quot;, &quot;LHS&quot;.&quot;bonus&quot; AS &quot;bonus&quot;, &quot;LHS&quot;.&quot;commissionpct&quot; AS &quot;commissionpct&quot;, &quot;LHS&quot;.&quot;salesytd&quot; AS &quot;salesytd&quot;, &quot;LHS&quot;.&quot;saleslastyear&quot; AS &quot;saleslastyear&quot;, &quot;LHS&quot;.&quot;rowguid&quot; AS &quot;rowguid.x&quot;, &quot;LHS&quot;.&quot;modifieddate&quot; AS &quot;modifieddate.x&quot;, &quot;RHS&quot;.&quot;nationalidnumber&quot; AS &quot;nationalidnumber&quot;, &quot;RHS&quot;.&quot;loginid&quot; AS &quot;loginid&quot;, &quot;RHS&quot;.&quot;jobtitle&quot; AS &quot;jobtitle&quot;, &quot;RHS&quot;.&quot;birthdate&quot; AS &quot;birthdate&quot;, &quot;RHS&quot;.&quot;maritalstatus&quot; AS &quot;maritalstatus&quot;, &quot;RHS&quot;.&quot;gender&quot; AS &quot;gender&quot;, &quot;RHS&quot;.&quot;hiredate&quot; AS &quot;hiredate&quot;, &quot;RHS&quot;.&quot;salariedflag&quot; AS &quot;salariedflag&quot;, &quot;RHS&quot;.&quot;vacationhours&quot; AS &quot;vacationhours&quot;, &quot;RHS&quot;.&quot;sickleavehours&quot; AS &quot;sickleavehours&quot;, &quot;RHS&quot;.&quot;currentflag&quot; AS &quot;currentflag&quot;, &quot;RHS&quot;.&quot;rowguid&quot; AS &quot;rowguid.y&quot;, &quot;RHS&quot;.&quot;modifieddate&quot; AS &quot;modifieddate.y&quot;, &quot;RHS&quot;.&quot;organizationnode&quot; AS &quot;organizationnode&quot; ## FROM sales.salesperson AS &quot;LHS&quot; ## LEFT JOIN humanresources.employee AS &quot;RHS&quot; ## ON (&quot;LHS&quot;.&quot;businessentityid&quot; = &quot;RHS&quot;.&quot;businessentityid&quot;) ## ) &quot;dbplyr_006&quot; # Need to come up with a different example illustrating where # the `collect` statement goes. # sales_person_table %&gt;% # mutate(birthdate = date(birthdate)) # # try(sales_person_table %&gt;% # mutate(birthdate = lubridate::date(birthdate)) # ) # # sales_person_table %&gt;% collect() %&gt;% # mutate(birthdate = lubridate::date(birthdate)) This may not be relevant in the context where it turns out that dates in adventureworks come through as date! The idea is to show how functions are interpreted BEFORE sending to the SQL translator. to_char &lt;- function(date, fmt) {return(fmt)} # sales_person_table %&gt;% # mutate(birthdate = to_char(birthdate, &quot;YYYY-MM&quot;)) %&gt;% # show_query() # # sales_person_table %&gt;% # mutate(birthdate = to_char(birthdate, &quot;YYYY-MM&quot;)) 10.1.4 Many handy R functions can’t be translated to SQL It just so happens that PostgreSQL has a date function that does the same thing as the date function in the lubridate package. In the following code the date function is executed by PostreSQL. # sales_person_table %&gt;% mutate(birthdate = date(birthdate)) If we specify that we want to use the lubridate version (or any number of other R functions) they are passed to the dbms unless we explicitly tell dplyr to stop translating and bring the results back to the R environment for local processing. try(sales_person_table %&gt;% collect() %&gt;% mutate(birthdate = lubridate::date(birthdate))) ## Error in eval(lhs, parent, parent) : ## object &#39;sales_person_table&#39; not found 10.1.5 Further lazy execution examples See more examples of lazy execution here. 10.2 Disconnect from the database and stop Docker dbDisconnect(con) sp_docker_stop(&quot;adventureworks&quot;) 10.3 Other resources Benjamin S. Baumer. 2017. A Grammar for Reproducible and Painless Extract-Transform-Load Operations on Medium Data. https://arxiv.org/abs/1708.07073 dplyr Reference documentation: Remote tables. https://dplyr.tidyverse.org/reference/index.html#section-remote-tables Data Carpentry. SQL Databases and R. https://datacarpentry.org/R-ecology-lesson/05-r-and-databases.html "],
["chapter-leveraging-database-views.html", "Chapter 11 Leveraging Database Views 11.1 Setup our standard working environment 11.2 The role of database views 11.3 Unpacking the elements of a view in the Tidyverse 11.4 Compare the official view and the dplyr output 11.5 Revise the view to summarize by quarter not fiscal year 11.6 Clean up and close down", " Chapter 11 Leveraging Database Views This chapter demonstrates how to: Understand database views and their uses Unpack a database view to see what it’s doing Reproduce a database view with dplyr code Write an alternative to a view that provides more details Create a database view either for personal use or for submittal to your enterprise DBA 11.1 Setup our standard working environment Use these libraries: library(tidyverse) library(DBI) library(RPostgres) library(connections) library(glue) require(knitr) library(dbplyr) library(sqlpetr) library(bookdown) library(lubridate) library(gt) Connect to adventureworks: sp_docker_start(&quot;adventureworks&quot;) Sys.sleep(sleep_default) # con &lt;- connection_open( # use in an interactive session con &lt;- dbConnect( # use in other settings RPostgres::Postgres(), # without the previous and next lines, some functions fail with bigint data # so change int64 to integer bigint = &quot;integer&quot;, host = &quot;localhost&quot;, port = 5432, user = &quot;postgres&quot;, password = &quot;postgres&quot;, dbname = &quot;adventureworks&quot; ) dbExecute(con, &quot;set search_path to sales;&quot;) # so that `dbListFields()` works ## [1] 0 11.2 The role of database views A database view is an SQL query that is stored in the database. Most views are used for data retrieval, since they usually denormalize the tables involved. Because they are standardized and well-understood, they can save you a lot of work and document a query that can serve as a model to build on. 11.2.1 Why database views are useful Database views are useful for many reasons. Authoritative: database views are typically written by the business application vendor or DBA, so they contain authoritative knowledge about the structure and intended use of the database. Performance: views are designed to gather data in an efficient way, using all the indexes in an efficient sequence and doing as much work on the database server as possible. Abstraction: views are abstractions or simplifications of complex queries that provide customary (useful) aggregations. Common examples would be monthly totals or aggregation of activity tied to one individual. Reuse: a view puts commonly used code in one place where it can be used for many purposes by many people. If there is a change or a problem found in a view, it only needs to be fixed in one place, rather than having to change many places downstream. Security: a view can give selective access to someone who does not have access to underlying tables or columns. Provenance: views standardize data provenance. For example, the AdventureWorks database all of them are named in a consistent way that suggests the underlying tables that they query. And they all start with a v. The bottom line is that views can save you a lot of work. 11.2.2 Rely on – and be critical of – views Because they represent a commonly used view of the database, it might seem like a view is always right. Even though they are conventional and authorized, they may still need verification or auditing, especially when used for a purpose other than the original intent. They can guide you toward what you need from the database but they could also mislead because they are easy to use and available. People may forget why a specific view exists and who is using it. Therefore any given view might be a forgotten vestige. part of a production data pipeline or a priceless nugget of insight. Who knows? Consider the view’s owner, schema, whether it’s a materialized index view or not, if it has a trigger and what the likely intention was behind the view. 11.3 Unpacking the elements of a view in the Tidyverse Since a view is in some ways just like an ordinary table, we can use familiar tools in the same way as they are used on a database table. For example, the simplest way of getting a list of columns in a view is the same as it is for a regular table: dbListFields(con, &quot;vsalespersonsalesbyfiscalyearsdata&quot;) ## [1] &quot;salespersonid&quot; &quot;fullname&quot; &quot;jobtitle&quot; &quot;salesterritory&quot; ## [5] &quot;salestotal&quot; &quot;fiscalyear&quot; 11.3.1 Use a view just like any other table From a retrieval perspective a database view is just like any other table. Using a view to retrieve data from the database will be completely standard across all flavors of SQL. v_salesperson_sales_by_fiscal_years_data &lt;- tbl(con, in_schema(&quot;sales&quot;,&quot;vsalespersonsalesbyfiscalyearsdata&quot;)) %&gt;% collect() str(v_salesperson_sales_by_fiscal_years_data) ## Classes &#39;tbl_df&#39;, &#39;tbl&#39; and &#39;data.frame&#39;: 48 obs. of 6 variables: ## $ salespersonid : int 275 275 275 275 276 276 276 276 277 277 ... ## $ fullname : chr &quot;Michael G Blythe&quot; &quot;Michael G Blythe&quot; &quot;Michael G Blythe&quot; &quot;Michael G Blythe&quot; ... ## $ jobtitle : chr &quot;Sales Representative&quot; &quot;Sales Representative&quot; &quot;Sales Representative&quot; &quot;Sales Representative&quot; ... ## $ salesterritory: chr &quot;Northeast&quot; &quot;Northeast&quot; &quot;Northeast&quot; &quot;Northeast&quot; ... ## $ salestotal : num 63763 2399593 3765459 3065088 5476 ... ## $ fiscalyear : num 2011 2012 2013 2014 2011 ... As we will see, our sample view, vsalespersonsalesbyfiscalyearsdata joins 5 different tables. We can assume that subsetting or calculation on any of the columns in the component tables will happen behind the scenes, on the database side, and done correctly. For example, the following query filters on a column that exists in only one of the view’s component tables. tbl(con, in_schema(&quot;sales&quot;,&quot;vsalespersonsalesbyfiscalyearsdata&quot;)) %&gt;% count(salesterritory, fiscalyear) %&gt;% collect() %&gt;% # ---- pull data here ---- # pivot_wider(names_from = fiscalyear, values_from = n, names_prefix = &quot;FY_&quot;) ## # A tibble: 10 x 5 ## # Groups: salesterritory [10] ## salesterritory FY_2014 FY_2011 FY_2013 FY_2012 ## &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 Southwest 2 2 2 2 ## 2 Northeast 1 1 1 1 ## 3 Southeast 1 1 1 1 ## 4 France 1 NA 1 1 ## 5 Canada 2 2 2 2 ## 6 United Kingdom 1 NA 1 1 ## 7 Northwest 3 2 3 2 ## 8 Central 1 1 1 1 ## 9 Australia 1 NA 1 NA ## 10 Germany 1 NA 1 NA Although finding out what a view does behind the scenes requires that you use functions that are not standard, doing so has several general purposes: It is satisfying to know what’s going on behind the scenes. Specific elements or components of a view might be worth plagiarizing or incorporating in our queries. It is necessary to understand the mechanics of a view if we are going to build on what it does or intend to extend or modify it. 11.3.2 SQL source code Functions for inspecting a view itself are not part of the ANSI standard, so they will be database-specific. Here is the code to retrieve a PostgreSQL view (using the pg_get_viewdef function): view_definition &lt;- dbGetQuery(con, &quot;select pg_get_viewdef(&#39;sales.vsalespersonsalesbyfiscalyearsdata&#39;, true)&quot;) The PostgreSQL pg_get_viewdef function returns a data frame with one column named pg_get_viewdef and one row. To properly view its contents, the \\n character strings need to be turned into new-lines. cat(unlist(view_definition$pg_get_viewdef)) ## SELECT granular.salespersonid, ## granular.fullname, ## granular.jobtitle, ## granular.salesterritory, ## sum(granular.subtotal) AS salestotal, ## granular.fiscalyear ## FROM ( SELECT soh.salespersonid, ## ((p.firstname::text || &#39; &#39;::text) || COALESCE(p.middlename::text || &#39; &#39;::text, &#39;&#39;::text)) || p.lastname::text AS fullname, ## e.jobtitle, ## st.name AS salesterritory, ## soh.subtotal, ## date_part(&#39;year&#39;::text, soh.orderdate + &#39;6 mons&#39;::interval) AS fiscalyear ## FROM salesperson sp ## JOIN salesorderheader soh ON sp.businessentityid = soh.salespersonid ## JOIN salesterritory st ON sp.territoryid = st.territoryid ## JOIN humanresources.employee e ON soh.salespersonid = e.businessentityid ## JOIN person.person p ON p.businessentityid = sp.businessentityid) granular ## GROUP BY granular.salespersonid, granular.fullname, granular.jobtitle, granular.salesterritory, granular.fiscalyear; Even if you don’t intend to become completely fluent in SQL, it’s useful to study as much of it as possible. Studying the SQL in a view is particularly useful to: Test your understanding of the database structure, elements, and usage Extend what’s already been done to extract useful data from the database 11.3.3 The ERD as context for SQL code A database Entity Relationship Diagram (ERD) is very helpful in making sense of the SQL in a view. The ERD for AdventureWorks is here. If a published ERD is not available, a tool like the PostgreSQL pg_modeler is capable of generating an ERD (or at least describing the portion of the database that is visible to you). 11.3.4 Selecting relevant tables and columns Before bginning to write code, it can be helpful to actually mark up the ERD to identify the specific tables that are involved in the view you are going to reproduce. Define each table that is involved and identify the columns that will be needed from that table. The sales.vsalespersonsalesbyfiscalyearsdata view joins data from five different tables: sales_order_header sales_territory sales_person employee person For each of the tables in the view, we select the columns that appear in the sales.vsalespersonsalesbyfiscalyearsdata. Selecting columns in this way prevents joins that dbplyr would make automatically based on common column names, such as rowguid and ModifiedDate columns, which appear in almost all AdventureWorks tables. In the following code we follow the convention that any column that we change or create on the fly uses a snake case naming convention. sales_order_header &lt;- tbl(con, in_schema(&quot;sales&quot;, &quot;salesorderheader&quot;)) %&gt;% select(orderdate, salespersonid, subtotal) sales_territory &lt;- tbl(con, in_schema(&quot;sales&quot;, &quot;salesterritory&quot;)) %&gt;% select(territoryid, territory_name = name) sales_person &lt;- tbl(con, in_schema(&quot;sales&quot;, &quot;salesperson&quot;)) %&gt;% select(businessentityid, territoryid) employee &lt;- tbl(con, in_schema(&quot;humanresources&quot;, &quot;employee&quot;)) %&gt;% select(businessentityid, jobtitle) In addition to selecting rows as shown in the previous statements, mutate and other functions help us replicate code in the view such as: ((p.firstname::text || &#39; &#39;::text) || COALESCE(p.middlename::text || &#39; &#39;::text, &#39;&#39;::text)) || p.lastname::text AS fullname The following dplyr code pastes the first, middle and last names together to make full_name: person &lt;- tbl(con, in_schema(&quot;person&quot;, &quot;person&quot;)) %&gt;% mutate(full_name = paste(firstname, middlename, lastname)) %&gt;% select(businessentityid, full_name) Double-check on the names that are defined in each tbl object. The following function will show the names of columns in the tables we’ve defined: getnames &lt;- function(table) { {table} %&gt;% collect(n = 5) %&gt;% # ---- pull data here ---- # names() } Verify the names selected: getnames(sales_order_header) ## [1] &quot;orderdate&quot; &quot;salespersonid&quot; &quot;subtotal&quot; getnames(sales_territory) ## [1] &quot;territoryid&quot; &quot;territory_name&quot; getnames(sales_person) ## [1] &quot;businessentityid&quot; &quot;territoryid&quot; getnames(employee) ## [1] &quot;businessentityid&quot; &quot;jobtitle&quot; getnames(person) ## [1] &quot;businessentityid&quot; &quot;full_name&quot; 11.3.5 Join the tables together First, join and download all of the data pertaining to a person. Notice that since each of these 4 tables contain businessentityid, dplyr will join them all on that common column automatically. And since we know that all of these tables are small, we don’t mind a query that joins and downloads all the data. salesperson_info &lt;- sales_person %&gt;% left_join(employee) %&gt;% left_join(person) %&gt;% left_join(sales_territory) %&gt;% collect() ## Joining, by = &quot;businessentityid&quot; ## Joining, by = &quot;businessentityid&quot; ## Joining, by = &quot;territoryid&quot; str(salesperson_info) ## Classes &#39;tbl_df&#39;, &#39;tbl&#39; and &#39;data.frame&#39;: 17 obs. of 5 variables: ## $ businessentityid: int 274 275 276 277 278 279 280 281 282 283 ... ## $ territoryid : int NA 2 4 3 6 5 1 4 6 1 ... ## $ jobtitle : chr &quot;North American Sales Manager&quot; &quot;Sales Representative&quot; &quot;Sales Representative&quot; &quot;Sales Representative&quot; ... ## $ full_name : chr &quot;Stephen Y Jiang&quot; &quot;Michael G Blythe&quot; &quot;Linda C Mitchell&quot; &quot;Jillian Carson&quot; ... ## $ territory_name : chr NA &quot;Northeast&quot; &quot;Southwest&quot; &quot;Central&quot; ... The one part of the view that we haven’t replicated is: date_part('year'::text, soh.orderdate + '6 mons'::interval) AS fiscalyear The lubridate package makes it very easy to convert orderdate to fiscal_year. Doing that same conversion without lubridate (e.g., only dplyr and ANSI-STANDARD SQL) is harder. Therefore we just pull the data from the server after the left_join and do the rest of the job on the R side. Note that this query doesn’t correct the problematic entry dates that we explored in the chapter on Asking Business Questions From a Single Table. That will collapse many rows into a much smaller table. We know from our previous investigation that Sales Rep into sales are recorded more or less once a month. Therefore most of the crunching in this query happens on the database server side. sales_data_fiscal_year &lt;- sales_person %&gt;% left_join(sales_order_header, by = c(&quot;businessentityid&quot; = &quot;salespersonid&quot;)) %&gt;% group_by(businessentityid, orderdate) %&gt;% summarize(sales_total = sum(subtotal, na.rm = TRUE)) %&gt;% mutate( orderdate = as.Date(orderdate), day = day(orderdate) ) %&gt;% collect() %&gt;% # ---- pull data here ---- # mutate( fiscal_year = year(orderdate %m+% months(6)) ) %&gt;% ungroup() %&gt;% group_by(businessentityid, fiscal_year) %&gt;% summarize(sales_total = sum(sales_total, na.rm = FALSE)) %&gt;% ungroup() Put the two parts together: sales_data_fiscal_year and person_info to yield the final query. salesperson_sales_by_fiscal_years_dplyr &lt;- sales_data_fiscal_year %&gt;% left_join(salesperson_info) %&gt;% filter(!is.na(territoryid)) ## Joining, by = &quot;businessentityid&quot; Notice that we’re dropping the Sales Managers who appear in the salesperson_info data frame because they don’t have a territoryid. 11.4 Compare the official view and the dplyr output Use pivot_wider to make it easier to compare the native view to our dplyr replicate. salesperson_sales_by_fiscal_years_dplyr %&gt;% select(-jobtitle, -businessentityid, -territoryid) %&gt;% pivot_wider(names_from = fiscal_year, values_from = sales_total, values_fill = list(sales_total = 0)) %&gt;% arrange(territory_name, full_name) %&gt;% filter(territory_name == &quot;Canada&quot;) ## # A tibble: 2 x 6 ## full_name territory_name `2011` `2012` `2013` `2014` ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Garrett R Vargas Canada 9109. 1254087. 1179531. 1166720. ## 2 José Edvaldo Saraiva Canada 106252. 2171995. 1388793. 2259378. v_salesperson_sales_by_fiscal_years_data %&gt;% select(-jobtitle, -salespersonid) %&gt;% pivot_wider(names_from = fiscalyear, values_from = salestotal, values_fill = list(salestotal = 0)) %&gt;% arrange(salesterritory, fullname) %&gt;% filter(salesterritory == &quot;Canada&quot;) ## # A tibble: 2 x 6 ## fullname salesterritory `2011` `2012` `2013` `2014` ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Garrett R Vargas Canada 9109. 1254087. 1179531. 1166720. ## 2 José Edvaldo Saraiva Canada 106252. 2171995. 1388793. 2259378. The yearly totals match exactly. The column names don’t match up, because we are using snake case convention for derived elements. 11.5 Revise the view to summarize by quarter not fiscal year To summarize sales data by SAles Rep and quarter requires the %m+% infix operator from lubridate. The interleaved comments in the query below has hints that explain it. The totals in this revised query are off by a rounding error from the totals shown above in the fiscal year summaries. tbl(con, in_schema(&quot;sales&quot;, &quot;salesorderheader&quot;)) %&gt;% group_by(salespersonid, orderdate) %&gt;% summarize(subtotal = sum(subtotal, na.rm = TRUE), digits = 0) %&gt;% collect() %&gt;% # ---- pull data here ---- # # Adding 6 months to orderdate requires a lubridate function mutate(orderdate = as.Date(orderdate) %m+% months(6), year = year(orderdate), quarter = quarter(orderdate)) %&gt;% ungroup() %&gt;% group_by(salespersonid, year, quarter) %&gt;% summarize(subtotal = round(sum(subtotal, na.rm = TRUE), digits = 0)) %&gt;% ungroup() %&gt;% # Join with the person information previously gathered left_join(salesperson_info, by = c(&quot;salespersonid&quot; = &quot;businessentityid&quot;)) %&gt;% filter(territory_name == &quot;Canada&quot;) %&gt;% # Pivot to make it easier to see what&#39;s going on pivot_wider(names_from = quarter, values_from = subtotal, values_fill = list(Q1 = 0, Q2 = 0, Q3 = 0, Q4 = 0), names_prefix = &quot;Q&quot;, id_cols = full_name:year) %&gt;% select(`Name` = full_name, year, Q1, Q2, Q3, Q4) %&gt;% mutate(`Year Total` = Q1 + Q2 + Q3 + Q4) %&gt;% head(., n = 10) %&gt;% gt() %&gt;% fmt_number(use_seps = TRUE, decimals = 0, columns = vars(Q1,Q2, Q3, Q4, `Year Total`)) html { font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, 'Helvetica Neue', 'Fira Sans', 'Droid Sans', Arial, sans-serif; } #pzveloiefp .gt_table { display: table; border-collapse: collapse; margin-left: auto; /* table.margin.left */ margin-right: auto; /* table.margin.right */ color: #333333; font-size: 16px; /* table.font.size */ background-color: #FFFFFF; /* table.background.color */ width: auto; /* table.width */ border-top-style: solid; /* table.border.top.style */ border-top-width: 2px; /* table.border.top.width */ border-top-color: #A8A8A8; /* table.border.top.color */ border-bottom-style: solid; /* table.border.bottom.style */ border-bottom-width: 2px; /* table.border.bottom.width */ border-bottom-color: #A8A8A8; /* table.border.bottom.color */ } #pzveloiefp .gt_heading { background-color: #FFFFFF; /* heading.background.color */ border-bottom-color: #FFFFFF; /* table.background.color */ border-left-style: hidden; /* heading.border.lr.style */ border-left-width: 1px; /* heading.border.lr.width */ border-left-color: #D3D3D3; /* heading.border.lr.color */ border-right-style: hidden; /* heading.border.lr.style */ border-right-width: 1px; /* heading.border.lr.width */ border-right-color: #D3D3D3; /* heading.border.lr.color */ } #pzveloiefp .gt_title { color: #333333; font-size: 125%; /* heading.title.font.size */ font-weight: initial; /* heading.title.font.weight */ padding-top: 4px; /* heading.top.padding - not yet used */ padding-bottom: 4px; border-bottom-color: #FFFFFF; /* table.background.color */ border-bottom-width: 0; } #pzveloiefp .gt_subtitle { color: #333333; font-size: 85%; /* heading.subtitle.font.size */ font-weight: initial; /* heading.subtitle.font.weight */ padding-top: 0; padding-bottom: 4px; /* heading.bottom.padding - not yet used */ border-top-color: #FFFFFF; /* table.background.color */ border-top-width: 0; } #pzveloiefp .gt_bottom_border { border-bottom-style: solid; /* heading.border.bottom.style */ border-bottom-width: 2px; /* heading.border.bottom.width */ border-bottom-color: #D3D3D3; /* heading.border.bottom.color */ } #pzveloiefp .gt_column_spanner { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; padding-top: 4px; padding-bottom: 4px; } #pzveloiefp .gt_col_headings { border-top-style: solid; /* column_labels.border.top.style */ border-top-width: 2px; /* column_labels.border.top.width */ border-top-color: #D3D3D3; /* column_labels.border.top.color */ border-bottom-style: solid; /* column_labels.border.bottom.style */ border-bottom-width: 2px; /* column_labels.border.bottom.width */ border-bottom-color: #D3D3D3; /* column_labels.border.bottom.color */ border-left-style: none; /* column_labels.border.lr.style */ border-left-width: 1px; /* column_labels.border.lr.width */ border-left-color: #D3D3D3; /* column_labels.border.lr.color */ border-right-style: none; /* column_labels.border.lr.style */ border-right-width: 1px; /* column_labels.border.lr.width */ border-right-color: #D3D3D3; /* column_labels.border.lr.color */ } #pzveloiefp .gt_col_heading { color: #333333; background-color: #FFFFFF; /* column_labels.background.color */ font-size: 100%; /* column_labels.font.size */ font-weight: initial; /* column_labels.font.weight */ text-transform: inherit; /* column_labels.text_transform */ vertical-align: middle; padding: 5px; margin: 10px; overflow-x: hidden; } #pzveloiefp .gt_sep_right { border-right: 5px solid #FFFFFF; } #pzveloiefp .gt_group_heading { padding: 8px; /* row_group.padding */ color: #333333; background-color: #FFFFFF; /* row_group.background.color */ font-size: 100%; /* row_group.font.size */ font-weight: initial; /* row_group.font.weight */ text-transform: inherit; /* row_group.text_transform */ border-top-style: solid; /* row_group.border.top.style */ border-top-width: 2px; /* row_group.border.top.width */ border-top-color: #D3D3D3; /* row_group.border.top.color */ border-bottom-style: solid; /* row_group.border.bottom.style */ border-bottom-width: 2px; /* row_group.border.bottom.width */ border-bottom-color: #D3D3D3; /* row_group.border.bottom.color */ border-left-style: none; /* row_group.border.left.style */ border-left-width: 1px; /* row_group.border.left.width */ border-left-color: #D3D3D3; /* row_group.border.left.color */ border-right-style: none; /* row_group.border.right.style */ border-right-width: 1px; /* row_group.border.right.width */ border-right-color: #D3D3D3; /* row_group.border.right.color */ vertical-align: middle; } #pzveloiefp .gt_empty_group_heading { padding: 0.5px; color: #333333; background-color: #FFFFFF; /* row_group.background.color */ font-size: 100%; /* row_group.font.size */ font-weight: initial; /* row_group.font.weight */ border-top-style: solid; /* row_group.border.top.style */ border-top-width: 2px; /* row_group.border.top.width */ border-top-color: #D3D3D3; /* row_group.border.top.color */ border-bottom-style: solid; /* row_group.border.bottom.style */ border-bottom-width: 2px; /* row_group.border.bottom.width */ border-bottom-color: #D3D3D3; /* row_group.border.bottom.color */ vertical-align: middle; } #pzveloiefp .gt_striped { background-color: #8080800D; /* row.striping.background_color */ } #pzveloiefp .gt_from_md > :first-child { margin-top: 0; } #pzveloiefp .gt_from_md > :last-child { margin-bottom: 0; } #pzveloiefp .gt_row { padding-top: 8px; /* data_row.padding */ padding-bottom: 8px; /* data_row.padding */ padding-left: 5px; padding-right: 5px; margin: 10px; border-top-style: solid; /* table_body.hlines.style */ border-top-width: 1px; /* table_body.hlines.width */ border-top-color: #D3D3D3; /* table_body.hlines.color */ border-left-style: none; /* table_body.vlines.style */ border-left-width: 1px; /* table_body.vlines.width */ border-left-color: #D3D3D3; /* table_body.vlines.color */ border-right-style: none; /* table_body.vlines.style */ border-right-width: 1px; /* table_body.vlines.width */ border-right-color: #D3D3D3; /* table_body.vlines.color */ vertical-align: middle; overflow-x: hidden; } #pzveloiefp .gt_stub { color: #333333; background-color: #FFFFFF; /* stub.background.color */ font-weight: initial; /* stub.font.weight */ text-transform: inherit; /* stub.text_transform */ border-right-style: solid; /* stub.border.style */ border-right-width: 2px; /* stub.border.width */ border-right-color: #D3D3D3; /* stub.border.color */ padding-left: 12px; } #pzveloiefp .gt_summary_row { color: #333333; background-color: #FFFFFF; /* summary_row.background.color */ text-transform: inherit; /* summary_row.text_transform */ padding-top: 8px; /* summary_row.padding */ padding-bottom: 8px; /* summary_row.padding */ padding-left: 5px; padding-right: 5px; } #pzveloiefp .gt_first_summary_row { padding-top: 8px; /* summary_row.padding */ padding-bottom: 8px; /* summary_row.padding */ padding-left: 5px; padding-right: 5px; border-top-style: solid; /* summary_row.border.style */ border-top-width: 2px; /* summary_row.border.width */ border-top-color: #D3D3D3; /* summary_row.border.color */ } #pzveloiefp .gt_grand_summary_row { color: #333333; background-color: #FFFFFF; /* grand_summary_row.background.color */ text-transform: inherit; /* grand_summary_row.text_transform */ padding-top: 8px; /* grand_summary_row.padding */ padding-bottom: 8px; /* grand_summary_row.padding */ padding-left: 5px; padding-right: 5px; } #pzveloiefp .gt_first_grand_summary_row { padding-top: 8px; /* grand_summary_row.padding */ padding-bottom: 8px; /* grand_summary_row.padding */ padding-left: 5px; padding-right: 5px; border-top-style: double; /* grand_summary_row.border.style */ border-top-width: 6px; /* grand_summary_row.border.width */ border-top-color: #D3D3D3; /* grand_summary_row.border.color */ } #pzveloiefp .gt_table_body { border-top-style: solid; /* table_body.border.top.style */ border-top-width: 2px; /* table_body.border.top.width */ border-top-color: #D3D3D3; /* table_body.border.top.color */ border-bottom-style: solid; /* table_body.border.bottom.style */ border-bottom-width: 2px; /* table_body.border.bottom.width */ border-bottom-color: #D3D3D3; /* table_body.border.bottom.color */ } #pzveloiefp .gt_footnotes { color: #333333; background-color: #FFFFFF; /* footnotes.background.color */ border-bottom-style: none; /* footnotes.border.bottom.style */ border-bottom-width: 2px; /* footnotes.border.bottom.width */ border-bottom-color: #D3D3D3; /* footnotes.border.bottom.color */ border-left-style: none; /* footnotes.border.lr.color */ border-left-width: 2px; /* footnotes.border.lr.color */ border-left-color: #D3D3D3; /* footnotes.border.lr.color */ border-right-style: none; /* footnotes.border.lr.color */ border-right-width: 2px; /* footnotes.border.lr.color */ border-right-color: #D3D3D3; /* footnotes.border.lr.color */ } #pzveloiefp .gt_footnote { margin: 0px; font-size: 90%; /* footnotes.font.size */ padding: 4px; /* footnotes.padding */ } #pzveloiefp .gt_sourcenotes { color: #333333; background-color: #FFFFFF; /* source_notes.background.color */ border-bottom-style: none; /* source_notes.border.bottom.style */ border-bottom-width: 2px; /* source_notes.border.bottom.width */ border-bottom-color: #D3D3D3; /* source_notes.border.bottom.color */ border-left-style: none; /* source_notes.border.lr.style */ border-left-width: 2px; /* source_notes.border.lr.style */ border-left-color: #D3D3D3; /* source_notes.border.lr.style */ border-right-style: none; /* source_notes.border.lr.style */ border-right-width: 2px; /* source_notes.border.lr.style */ border-right-color: #D3D3D3; /* source_notes.border.lr.style */ } #pzveloiefp .gt_sourcenote { font-size: 90%; /* source_notes.font.size */ padding: 4px; /* source_notes.padding */ } #pzveloiefp .gt_left { text-align: left; } #pzveloiefp .gt_center { text-align: center; } #pzveloiefp .gt_right { text-align: right; font-variant-numeric: tabular-nums; } #pzveloiefp .gt_font_normal { font-weight: normal; } #pzveloiefp .gt_font_bold { font-weight: bold; } #pzveloiefp .gt_font_italic { font-style: italic; } #pzveloiefp .gt_super { font-size: 65%; } #pzveloiefp .gt_footnote_marks { font-style: italic; font-size: 65%; } Name year Q1 Q2 Q3 Q4 Year Total Garrett R Vargas 2011 NA NA NA 9,109 NA Garrett R Vargas 2012 233,696 257,287 410,518 352,587 1,254,088 Garrett R Vargas 2013 316,818 203,647 291,333 367,732 1,179,530 Garrett R Vargas 2014 393,788 336,984 290,536 145,413 1,166,721 José Edvaldo Saraiva 2011 NA NA NA 106,252 NA José Edvaldo Saraiva 2012 521,794 546,962 795,861 307,379 2,171,996 José Edvaldo Saraiva 2013 408,415 324,062 231,991 424,326 1,388,794 José Edvaldo Saraiva 2014 748,430 466,137 618,832 425,979 2,259,378 11.6 Clean up and close down connection_close(con) # Use in an interactive setting # dbDisconnect(con) # Use in non-interactive setting "],
["chapter-postgresql-metadata.html", "Chapter 12 Getting metadata about and from PostgreSQL 12.1 Views trick parked here for the time being 12.2 Database contents and structure 12.3 What columns do those tables contain? 12.4 Characterizing how things are named 12.5 Database keys 12.6 Creating your own data dictionary 12.7 Save your work!", " Chapter 12 Getting metadata about and from PostgreSQL This chapter demonstrates: What kind of data about the database is contained in a dbms Several methods for obtaining metadata from the dbms The following packages are used in this chapter: library(tidyverse) library(DBI) library(RPostgres) library(glue) library(here) require(knitr) library(dbplyr) library(sqlpetr) Assume that the Docker container with PostgreSQL and the dvdrental database are ready to go. sp_docker_start(&quot;adventureworks&quot;) Connect to the database: con &lt;- sqlpetr::sp_get_postgres_connection( user = Sys.getenv(&quot;DEFAULT_POSTGRES_USER_NAME&quot;), password = Sys.getenv(&quot;DEFAULT_POSTGRES_PASSWORD&quot;), dbname = &quot;adventureworks&quot;, port = 5432, seconds_to_test = 20, connection_tab = TRUE ) 12.1 Views trick parked here for the time being 12.1.1 Explore the vsalelsperson and vsalespersonsalesbyfiscalyearsdata views The following trick goes later in the book, where it’s used to prove the finding that to make sense of othe data you need to cat(unlist(dbGetQuery(con, &quot;select pg_get_viewdef(&#39;sales.vsalesperson&#39;, true)&quot;))) ## SELECT s.businessentityid, ## p.title, ## p.firstname, ## p.middlename, ## p.lastname, ## p.suffix, ## e.jobtitle, ## pp.phonenumber, ## pnt.name AS phonenumbertype, ## ea.emailaddress, ## p.emailpromotion, ## a.addressline1, ## a.addressline2, ## a.city, ## sp.name AS stateprovincename, ## a.postalcode, ## cr.name AS countryregionname, ## st.name AS territoryname, ## st.&quot;group&quot; AS territorygroup, ## s.salesquota, ## s.salesytd, ## s.saleslastyear ## FROM sales.salesperson s ## JOIN humanresources.employee e ON e.businessentityid = s.businessentityid ## JOIN person.person p ON p.businessentityid = s.businessentityid ## JOIN person.businessentityaddress bea ON bea.businessentityid = s.businessentityid ## JOIN person.address a ON a.addressid = bea.addressid ## JOIN person.stateprovince sp ON sp.stateprovinceid = a.stateprovinceid ## JOIN person.countryregion cr ON cr.countryregioncode::text = sp.countryregioncode::text ## LEFT JOIN sales.salesterritory st ON st.territoryid = s.territoryid ## LEFT JOIN person.emailaddress ea ON ea.businessentityid = p.businessentityid ## LEFT JOIN person.personphone pp ON pp.businessentityid = p.businessentityid ## LEFT JOIN person.phonenumbertype pnt ON pnt.phonenumbertypeid = pp.phonenumbertypeid; ## pg_get_viewdef ## 1 SELECT granular.salespersonid,\\n granular.fullname,\\n granular.jobtitle,\\n granular.salesterritory,\\n sum(granular.subtotal) AS salestotal,\\n granular.fiscalyear\\n FROM ( SELECT soh.salespersonid,\\n ((p.firstname::text || &#39; &#39;::text) || COALESCE(p.middlename::text || &#39; &#39;::text, &#39;&#39;::text)) || p.lastname::text AS fullname,\\n e.jobtitle,\\n st.name AS salesterritory,\\n soh.subtotal,\\n date_part(&#39;year&#39;::text, soh.orderdate + &#39;6 mons&#39;::interval) AS fiscalyear\\n FROM sales.salesperson sp\\n JOIN sales.salesorderheader soh ON sp.businessentityid = soh.salespersonid\\n JOIN sales.salesterritory st ON sp.territoryid = st.territoryid\\n JOIN humanresources.employee e ON soh.salespersonid = e.businessentityid\\n JOIN person.person p ON p.businessentityid = sp.businessentityid) granular\\n GROUP BY granular.salespersonid, granular.fullname, granular.jobtitle, granular.salesterritory, granular.fiscalyear; 12.2 Database contents and structure After just looking at the data you seek, it might be worthwhile stepping back and looking at the big picture. 12.2.1 Database structure For large or complex databases you need to use both the available documentation for your database (e.g., the dvdrental database) and the other empirical tools that are available. For example it’s worth learning to interpret the symbols in an Entity Relationship Diagram: The information_schema is a trove of information about the database. Its format is more or less consistent across the different SQL implementations that are available. Here we explore some of what’s available using several different methods. PostgreSQL stores a lot of metadata. 12.2.2 Contents of the information_schema For this chapter R needs the dbplyr package to access alternate schemas. A schema is an object that contains one or more tables. Most often there will be a default schema, but to access the metadata, you need to explicitly specify which schema contains the data you want. 12.2.3 What tables are in the database? The simplest way to get a list of tables is with … NO LONGER WORKS: schema_list &lt;- tbl(con, in_schema(&quot;information_schema&quot;, &quot;schemata&quot;)) %&gt;% select(catalog_name, schema_name, schema_owner) %&gt;% collect() sp_print_df(head(schema_list)) ### Digging into the information_schema We usually need more detail than just a list of tables. Most SQL databases have an information_schema that has a standard structure to describe and control the database. The information_schema is in a different schema from the default, so to connect to the tables table in the information_schema we connect to the database in a different way: table_info_schema_table &lt;- tbl(con, dbplyr::in_schema(&quot;information_schema&quot;, &quot;tables&quot;)) The information_schema is large and complex and contains 343 tables. So it’s easy to get lost in it. This query retrieves a list of the tables in the database that includes additional detail, not just the name of the table. table_info &lt;- table_info_schema_table %&gt;% # filter(table_schema == &quot;public&quot;) %&gt;% select(table_catalog, table_schema, table_name, table_type) %&gt;% arrange(table_type, table_name) %&gt;% collect() sp_print_df(head(table_info)) In this context table_catalog is synonymous with database. Notice that VIEWS are composites made up of one or more BASE TABLES. The SQL world has its own terminology. For example rs is shorthand for result set. That’s equivalent to using df for a data frame. The following SQL query returns the same information as the previous dplyr code. rs &lt;- dbGetQuery( con, &quot;select table_catalog, table_schema, table_name, table_type from information_schema.tables where table_schema not in (&#39;pg_catalog&#39;,&#39;information_schema&#39;) order by table_type, table_name ;&quot; ) sp_print_df(head(rs)) 12.3 What columns do those tables contain? Of course, the DBI package has a dbListFields function that provides the simplest way to get the minimum, a list of column names: # DBI::dbListFields(con, &quot;rental&quot;) But the information_schema has a lot more useful information that we can use. columns_info_schema_table &lt;- tbl(con, dbplyr::in_schema(&quot;information_schema&quot;, &quot;columns&quot;)) Since the information_schema contains 2961 columns, we are narrowing our focus to just one table. This query retrieves more information about the rental table: columns_info_schema_info &lt;- columns_info_schema_table %&gt;% # filter(table_schema == &quot;public&quot;) %&gt;% select( table_catalog, table_schema, table_name, column_name, data_type, ordinal_position, character_maximum_length, column_default, numeric_precision, numeric_precision_radix ) %&gt;% collect(n = Inf) %&gt;% mutate(data_type = case_when( data_type == &quot;character varying&quot; ~ paste0(data_type, &quot; (&quot;, character_maximum_length, &quot;)&quot;), data_type == &quot;real&quot; ~ paste0(data_type, &quot; (&quot;, numeric_precision, &quot;,&quot;, numeric_precision_radix, &quot;)&quot;), TRUE ~ data_type )) %&gt;% # filter(table_name == &quot;rental&quot;) %&gt;% select(-table_schema, -numeric_precision, -numeric_precision_radix) glimpse(columns_info_schema_info) ## Observations: 2,961 ## Variables: 7 ## $ table_catalog &lt;chr&gt; &quot;adventureworks&quot;, &quot;adventureworks&quot;, &quot;adventu… ## $ table_name &lt;chr&gt; &quot;pg_proc&quot;, &quot;pg_proc&quot;, &quot;pg_proc&quot;, &quot;pg_proc&quot;, … ## $ column_name &lt;chr&gt; &quot;proname&quot;, &quot;pronamespace&quot;, &quot;proowner&quot;, &quot;prol… ## $ data_type &lt;chr&gt; &quot;name&quot;, &quot;oid&quot;, &quot;oid&quot;, &quot;oid&quot;, &quot;real (24,2)&quot;, … ## $ ordinal_position &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 1… ## $ character_maximum_length &lt;int&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, … ## $ column_default &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, … sp_print_df(head(columns_info_schema_info)) 12.3.1 What is the difference between a VIEW and a BASE TABLE? The BASE TABLE has the underlying data in the database table_info_schema_table %&gt;% filter( table_type == &quot;BASE TABLE&quot;) %&gt;% # filter(table_schema == &quot;public&quot; &amp; table_type == &quot;BASE TABLE&quot;) %&gt;% select(table_name, table_type) %&gt;% left_join(columns_info_schema_table, by = c(&quot;table_name&quot; = &quot;table_name&quot;)) %&gt;% select( table_type, table_name, column_name, data_type, ordinal_position, column_default ) %&gt;% collect(n = Inf) %&gt;% filter(str_detect(table_name, &quot;cust&quot;)) %&gt;% head() %&gt;% sp_print_df() Probably should explore how the VIEW is made up of data from BASE TABLEs. table_info_schema_table %&gt;% filter( table_type == &quot;VIEW&quot;) %&gt;% # filter(table_schema == &quot;public&quot; &amp; table_type == &quot;VIEW&quot;) %&gt;% select(table_name, table_type) %&gt;% left_join(columns_info_schema_table, by = c(&quot;table_name&quot; = &quot;table_name&quot;)) %&gt;% select( table_type, table_name, column_name, data_type, ordinal_position, column_default ) %&gt;% collect(n = Inf) %&gt;% filter(str_detect(table_name, &quot;cust&quot;)) %&gt;% head() %&gt;% sp_print_df() 12.3.2 What data types are found in the database? columns_info_schema_info %&gt;% count(data_type) %&gt;% head() %&gt;% sp_print_df() 12.4 Characterizing how things are named Names are the handle for accessing the data. Tables and columns may or may not be named consistently or in a way that makes sense to you. You should look at these names as data. 12.4.1 Counting columns and name reuse Pull out some rough-and-ready but useful statistics about your database. Since we are in SQL-land we talk about variables as columns. this is wrong! public_tables &lt;- columns_info_schema_table %&gt;% # filter(str_detect(table_name, &quot;pg_&quot;) == FALSE) %&gt;% # filter(table_schema == &quot;public&quot;) %&gt;% collect() public_tables %&gt;% count(table_name, sort = TRUE) %&gt;% head(n = 15) %&gt;% sp_print_df() How many column names are shared across tables (or duplicated)? public_tables %&gt;% count(column_name, sort = TRUE) %&gt;% filter(n &gt; 1) %&gt;% head() ## # A tibble: 6 x 2 ## column_name n ## &lt;chr&gt; &lt;int&gt; ## 1 modifieddate 140 ## 2 rowguid 61 ## 3 id 60 ## 4 name 59 ## 5 businessentityid 49 ## 6 productid 32 How many column names are unique? public_tables %&gt;% count(column_name) %&gt;% filter(n == 1) %&gt;% count() %&gt;% head() ## # A tibble: 1 x 1 ## n ## &lt;int&gt; ## 1 882 12.5 Database keys 12.5.1 Direct SQL How do we use this output? Could it be generated by dplyr? rs &lt;- dbGetQuery( con, &quot; --SELECT conrelid::regclass as table_from select table_catalog||&#39;.&#39;||table_schema||&#39;.&#39;||table_name table_name , conname, pg_catalog.pg_get_constraintdef(r.oid, true) as condef FROM information_schema.columns c,pg_catalog.pg_constraint r WHERE 1 = 1 --r.conrelid = &#39;16485&#39; AND r.contype in (&#39;f&#39;,&#39;p&#39;) ORDER BY 1 ;&quot; ) glimpse(rs) ## Observations: 467,838 ## Variables: 3 ## $ table_name &lt;chr&gt; &quot;adventureworks.hr.d&quot;, &quot;adventureworks.hr.d&quot;, &quot;adventurewo… ## $ conname &lt;chr&gt; &quot;FK_SalesOrderDetail_SpecialOfferProduct_SpecialOfferIDPro… ## $ condef &lt;chr&gt; &quot;FOREIGN KEY (specialofferid, productid) REFERENCES sales.… sp_print_df(head(rs)) The following is more compact and looks more useful. What is the difference between the two? rs &lt;- dbGetQuery( con, &quot;select conrelid::regclass as table_from ,c.conname ,pg_get_constraintdef(c.oid) from pg_constraint c join pg_namespace n on n.oid = c.connamespace where c.contype in (&#39;f&#39;,&#39;p&#39;) and n.nspname = &#39;public&#39; order by conrelid::regclass::text, contype DESC; &quot; ) glimpse(rs) ## Observations: 0 ## Variables: 3 ## $ table_from &lt;chr&gt; ## $ conname &lt;chr&gt; ## $ pg_get_constraintdef &lt;chr&gt; sp_print_df(head(rs)) dim(rs)[1] ## [1] 0 12.5.2 Database keys with dplyr This query shows the primary and foreign keys in the database. tables &lt;- tbl(con, dbplyr::in_schema(&quot;information_schema&quot;, &quot;tables&quot;)) table_constraints &lt;- tbl(con, dbplyr::in_schema(&quot;information_schema&quot;, &quot;table_constraints&quot;)) key_column_usage &lt;- tbl(con, dbplyr::in_schema(&quot;information_schema&quot;, &quot;key_column_usage&quot;)) referential_constraints &lt;- tbl(con, dbplyr::in_schema(&quot;information_schema&quot;, &quot;referential_constraints&quot;)) constraint_column_usage &lt;- tbl(con, dbplyr::in_schema(&quot;information_schema&quot;, &quot;constraint_column_usage&quot;)) keys &lt;- tables %&gt;% left_join(table_constraints, by = c( &quot;table_catalog&quot; = &quot;table_catalog&quot;, &quot;table_schema&quot; = &quot;table_schema&quot;, &quot;table_name&quot; = &quot;table_name&quot; )) %&gt;% # table_constraints %&gt;% filter(constraint_type %in% c(&quot;FOREIGN KEY&quot;, &quot;PRIMARY KEY&quot;)) %&gt;% left_join(key_column_usage, by = c( &quot;table_catalog&quot; = &quot;table_catalog&quot;, &quot;constraint_catalog&quot; = &quot;constraint_catalog&quot;, &quot;constraint_schema&quot; = &quot;constraint_schema&quot;, &quot;table_name&quot; = &quot;table_name&quot;, &quot;table_schema&quot; = &quot;table_schema&quot;, &quot;constraint_name&quot; = &quot;constraint_name&quot; ) ) %&gt;% # left_join(constraint_column_usage) %&gt;% # does this table add anything useful? select(table_name, table_type, constraint_name, constraint_type, column_name, ordinal_position) %&gt;% arrange(table_name) %&gt;% collect() glimpse(keys) ## Observations: 190 ## Variables: 6 ## $ table_name &lt;chr&gt; &quot;address&quot;, &quot;address&quot;, &quot;addresstype&quot;, &quot;billofmaterial… ## $ table_type &lt;chr&gt; &quot;BASE TABLE&quot;, &quot;BASE TABLE&quot;, &quot;BASE TABLE&quot;, &quot;BASE TABL… ## $ constraint_name &lt;chr&gt; &quot;FK_Address_StateProvince_StateProvinceID&quot;, &quot;PK_Addr… ## $ constraint_type &lt;chr&gt; &quot;FOREIGN KEY&quot;, &quot;PRIMARY KEY&quot;, &quot;PRIMARY KEY&quot;, &quot;FOREIG… ## $ column_name &lt;chr&gt; &quot;stateprovinceid&quot;, &quot;addressid&quot;, &quot;addresstypeid&quot;, &quot;co… ## $ ordinal_position &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 2, 1, 1, 1, 2… sp_print_df(head(keys)) What do we learn from the following query? How is it useful? rs &lt;- dbGetQuery( con, &quot;SELECT r.*, pg_catalog.pg_get_constraintdef(r.oid, true) as condef FROM pg_catalog.pg_constraint r WHERE 1=1 --r.conrelid = &#39;16485&#39; AND r.contype = &#39;f&#39; ORDER BY 1; &quot; ) head(rs) ## conname connamespace contype condeferrable condeferred ## 1 cardinal_number_domain_check 12771 c FALSE FALSE ## 2 yes_or_no_check 12771 c FALSE FALSE ## 3 CK_Employee_BirthDate 16386 c FALSE FALSE ## 4 CK_Employee_Gender 16386 c FALSE FALSE ## 5 CK_Employee_HireDate 16386 c FALSE FALSE ## 6 CK_Employee_MaritalStatus 16386 c FALSE FALSE ## convalidated conrelid contypid conindid conparentid confrelid confupdtype ## 1 TRUE 0 12785 0 0 0 ## 2 TRUE 0 12797 0 0 0 ## 3 TRUE 16450 0 0 0 0 ## 4 TRUE 16450 0 0 0 0 ## 5 TRUE 16450 0 0 0 0 ## 6 TRUE 16450 0 0 0 0 ## confdeltype confmatchtype conislocal coninhcount connoinherit conkey confkey ## 1 TRUE 0 FALSE &lt;NA&gt; &lt;NA&gt; ## 2 TRUE 0 FALSE &lt;NA&gt; &lt;NA&gt; ## 3 TRUE 0 FALSE {5} &lt;NA&gt; ## 4 TRUE 0 FALSE {7} &lt;NA&gt; ## 5 TRUE 0 FALSE {8} &lt;NA&gt; ## 6 TRUE 0 FALSE {6} &lt;NA&gt; ## conpfeqop conppeqop conffeqop conexclop ## 1 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 2 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 3 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 4 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 5 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 6 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## conbin ## 1 {OPEXPR :opno 525 :opfuncid 150 :opresulttype 16 :opretset false :opcollid 0 :inputcollid 0 :args ({COERCETODOMAINVALUE :typeId 23 :typeMod -1 :collation 0 :location 195} {CONST :consttype 23 :consttypmod -1 :constcollid 0 :constlen 4 :constbyval true :constisnull false :location 204 :constvalue 4 [ 0 0 0 0 0 0 0 0 ]}) :location 201} ## 2 {SCALARARRAYOPEXPR :opno 98 :opfuncid 67 :useOr true :inputcollid 100 :args ({RELABELTYPE :arg {COERCETODOMAINVALUE :typeId 1043 :typeMod 7 :collation 100 :location 121} :resulttype 25 :resulttypmod -1 :resultcollid 100 :relabelformat 2 :location -1} {ARRAYCOERCEEXPR :arg {ARRAY :array_typeid 1015 :array_collid 100 :element_typeid 1043 :elements ({CONST :consttype 1043 :consttypmod -1 :constcollid 100 :constlen -1 :constbyval false :constisnull false :location 131 :constvalue 7 [ 28 0 0 0 89 69 83 ]} {CONST :consttype 1043 :consttypmod -1 :constcollid 100 :constlen -1 :constbyval false :constisnull false :location 138 :constvalue 6 [ 24 0 0 0 78 79 ]}) :multidims false :location -1} :elemexpr {RELABELTYPE :arg {CASETESTEXPR :typeId 1043 :typeMod -1 :collation 0} :resulttype 25 :resulttypmod -1 :resultcollid 100 :relabelformat 2 :location -1} :resulttype 1009 :resulttypmod -1 :resultcollid 100 :coerceformat 2 :location -1}) :location 127} ## 3 {BOOLEXPR :boolop and :args ({OPEXPR :opno 1098 :opfuncid 1090 :opresulttype 16 :opretset false :opcollid 0 :inputcollid 0 :args ({VAR :varno 1 :varattno 5 :vartype 1082 :vartypmod -1 :varcollid 0 :varlevelsup 0 :varnoold 1 :varoattno 5 :location 804} {CONST :consttype 1082 :consttypmod -1 :constcollid 0 :constlen 4 :constbyval true :constisnull false :location 817 :constvalue 4 [ 33 -100 -1 -1 -1 -1 -1 -1 ]}) :location 814} {OPEXPR :opno 2359 :opfuncid 2352 :opresulttype 16 :opretset false :opcollid 0 :inputcollid 0 :args ({VAR :varno 1 :varattno 5 :vartype 1082 :vartypmod -1 :varcollid 0 :varlevelsup 0 :varnoold 1 :varoattno 5 :location 842} {OPEXPR :opno 1329 :opfuncid 1190 :opresulttype 1184 :opretset false :opcollid 0 :inputcollid 0 :args ({FUNCEXPR :funcid 1299 :funcresulttype 1184 :funcretset false :funcvariadic false :funcformat 0 :funccollid 0 :inputcollid 0 :args &lt;&gt; :location 856} {CONST :consttype 1186 :consttypmod -1 :constcollid 0 :constlen 16 :constbyval false :constisnull false :location 864 :constvalue 16 [ 0 0 0 0 0 0 0 0 0 0 0 0 -40 0 0 0 ]}) :location 862}) :location 852}) :location 837} ## 4 {SCALARARRAYOPEXPR :opno 98 :opfuncid 67 :useOr true :inputcollid 100 :args ({FUNCEXPR :funcid 871 :funcresulttype 25 :funcretset false :funcvariadic false :funcformat 0 :funccollid 100 :inputcollid 100 :args ({FUNCEXPR :funcid 401 :funcresulttype 25 :funcretset false :funcvariadic false :funcformat 1 :funccollid 100 :inputcollid 100 :args ({VAR :varno 1 :varattno 7 :vartype 1042 :vartypmod 5 :varcollid 100 :varlevelsup 0 :varnoold 1 :varoattno 7 :location 941}) :location 948}) :location 934} {ARRAY :array_typeid 1009 :array_collid 100 :element_typeid 25 :elements ({CONST :consttype 25 :consttypmod -1 :constcollid 100 :constlen -1 :constbyval false :constisnull false :location 969 :constvalue 5 [ 20 0 0 0 77 ]} {CONST :consttype 25 :consttypmod -1 :constcollid 100 :constlen -1 :constbyval false :constisnull false :location 980 :constvalue 5 [ 20 0 0 0 70 ]}) :multidims false :location 963}) :location 956} ## 5 {BOOLEXPR :boolop and :args ({OPEXPR :opno 1098 :opfuncid 1090 :opresulttype 16 :opretset false :opcollid 0 :inputcollid 0 :args ({VAR :varno 1 :varattno 8 :vartype 1082 :vartypmod -1 :varcollid 0 :varlevelsup 0 :varnoold 1 :varoattno 8 :location 1042} {CONST :consttype 1082 :consttypmod -1 :constcollid 0 :constlen 4 :constbyval true :constisnull false :location 1054 :constvalue 4 [ 1 -5 -1 -1 -1 -1 -1 -1 ]}) :location 1051} {OPEXPR :opno 2359 :opfuncid 2352 :opresulttype 16 :opretset false :opcollid 0 :inputcollid 0 :args ({VAR :varno 1 :varattno 8 :vartype 1082 :vartypmod -1 :varcollid 0 :varlevelsup 0 :varnoold 1 :varoattno 8 :location 1079} {OPEXPR :opno 1327 :opfuncid 1189 :opresulttype 1184 :opretset false :opcollid 0 :inputcollid 0 :args ({FUNCEXPR :funcid 1299 :funcresulttype 1184 :funcretset false :funcvariadic false :funcformat 0 :funccollid 0 :inputcollid 0 :args &lt;&gt; :location 1092} {CONST :consttype 1186 :consttypmod -1 :constcollid 0 :constlen 16 :constbyval false :constisnull false :location 1100 :constvalue 16 [ 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 ]}) :location 1098}) :location 1088}) :location 1074} ## 6 {SCALARARRAYOPEXPR :opno 98 :opfuncid 67 :useOr true :inputcollid 100 :args ({FUNCEXPR :funcid 871 :funcresulttype 25 :funcretset false :funcvariadic false :funcformat 0 :funccollid 100 :inputcollid 100 :args ({FUNCEXPR :funcid 401 :funcresulttype 25 :funcretset false :funcvariadic false :funcformat 1 :funccollid 100 :inputcollid 100 :args ({VAR :varno 1 :varattno 6 :vartype 1042 :vartypmod 5 :varcollid 100 :varlevelsup 0 :varnoold 1 :varoattno 6 :location 1181}) :location 1195}) :location 1174} {ARRAY :array_typeid 1009 :array_collid 100 :element_typeid 25 :elements ({CONST :consttype 25 :consttypmod -1 :constcollid 100 :constlen -1 :constbyval false :constisnull false :location 1216 :constvalue 5 [ 20 0 0 0 77 ]} {CONST :consttype 25 :consttypmod -1 :constcollid 100 :constlen -1 :constbyval false :constisnull false :location 1227 :constvalue 5 [ 20 0 0 0 83 ]}) :multidims false :location 1210}) :location 1203} ## consrc ## 1 (VALUE &gt;= 0) ## 2 ((VALUE)::text = ANY ((ARRAY[&#39;YES&#39;::character varying, &#39;NO&#39;::character varying])::text[])) ## 3 ((birthdate &gt;= &#39;1930-01-01&#39;::date) AND (birthdate &lt;= (now() - &#39;18 years&#39;::interval))) ## 4 (upper((gender)::text) = ANY (ARRAY[&#39;M&#39;::text, &#39;F&#39;::text])) ## 5 ((hiredate &gt;= &#39;1996-07-01&#39;::date) AND (hiredate &lt;= (now() + &#39;1 day&#39;::interval))) ## 6 (upper((maritalstatus)::text) = ANY (ARRAY[&#39;M&#39;::text, &#39;S&#39;::text])) ## condef ## 1 CHECK (VALUE &gt;= 0) ## 2 CHECK (VALUE::text = ANY (ARRAY[&#39;YES&#39;::character varying, &#39;NO&#39;::character varying]::text[])) ## 3 CHECK (birthdate &gt;= &#39;1930-01-01&#39;::date AND birthdate &lt;= (now() - &#39;18 years&#39;::interval)) ## 4 CHECK (upper(gender::text) = ANY (ARRAY[&#39;M&#39;::text, &#39;F&#39;::text])) ## 5 CHECK (hiredate &gt;= &#39;1996-07-01&#39;::date AND hiredate &lt;= (now() + &#39;1 day&#39;::interval)) ## 6 CHECK (upper(maritalstatus::text) = ANY (ARRAY[&#39;M&#39;::text, &#39;S&#39;::text])) 12.6 Creating your own data dictionary If you are going to work with a database for an extended period it can be useful to create your own data dictionary. This can take the form of keeping detaild notes as well as extracting metadata from the dbms. Here is an illustration of the idea. This probably doens’t work anymore # some_tables &lt;- c(&quot;rental&quot;, &quot;city&quot;, &quot;store&quot;) # # all_meta &lt;- map_df(some_tables, sp_get_dbms_data_dictionary, con = con) # # all_meta # # glimpse(all_meta) # # sp_print_df(head(all_meta)) 12.7 Save your work! The work you do to understand the structure and contents of a database can be useful for others (including future-you). So at the end of a session, you might look at all the data frames you want to save. Consider saving them in a form where you can add notes at the appropriate level (as in a Google Doc representing table or columns that you annotate over time). ls() ## [1] &quot;columns_info_schema_info&quot; &quot;columns_info_schema_table&quot; ## [3] &quot;con&quot; &quot;constraint_column_usage&quot; ## [5] &quot;cranex&quot; &quot;key_column_usage&quot; ## [7] &quot;keys&quot; &quot;public_tables&quot; ## [9] &quot;referential_constraints&quot; &quot;rs&quot; ## [11] &quot;schema_list&quot; &quot;table_constraints&quot; ## [13] &quot;table_info&quot; &quot;table_info_schema_table&quot; ## [15] &quot;tables&quot; ``` ## Cleaning up Always have R disconnect from the database when you’re done and stop the Adventureworks Container dbDisconnect(con) sp_docker_stop(&quot;adventureworks&quot;) "],
["appendix-background-basic-concepts.html", "A Background and Basic Concepts A.1 The big picture: R and the Docker / PostgreSQL playground on your machine A.2 Your computer and its operating system A.3 R A.4 Our sqlpetr package A.5 Docker A.6 ‘Normal’ and ‘normalized’ data A.7 SQL Language A.8 Enterprise DBMS", " A Background and Basic Concepts This Appendix describes: The overall structure of our Docker-based PostgreSQL sandbox Basic concepts around each of the elements that make up our sandbox: tidy data, pipes, Docker, PostgreSQL, data representation, and our petsqlr package. A.1 The big picture: R and the Docker / PostgreSQL playground on your machine Here is an overview of how R and Docker fit on your operating system in this book’s sandbox: R and Docker You run R from RStudio to set up Docker, launch PostgreSQL inside it and then send queries directly to PostgreSQL from R. (We provide more details about our sandbox environment in the chapter on mapping your environment. A.2 Your computer and its operating system The playground that we construct in this book is designed so that some of the mysteries of accessing a corporate database are more visible – it’s all happening on your computer. The challenge, however, is that we know very little about your computer and its operating system. In the workshops we’ve given about this book, the details of individual computers have turned out to be diverse and difficult to pin down in advance. So there can be many issues, but not many basic concepts that we can highlight in advance. A.3 R We assume a general familiarity with R and RStudio. RStudio’s Big Data workshop at the 2019 RStudio has an abundance of introductory material (Ruiz 2019). This book is Tidyverse-oriented, so we assume familiarity with the pipe operator, tidy data (Wickham 2014), dplyr, and techniques for tidying data (Wickham 2018). R connects to a database by means of a series of packages that work together. The following diagram from a big data workshop at the 2019 RStudio conference shows the big picture. The biggest difference in terms of retrieval strategies is between writing dplyr and native SQL code. Dplyr generates SQL-92 standard code; whereas you can write SQL code that leverages the specific language features of your DBMS when you write SQL code yourself. Rstudio’s DBMS architecture - slide # 33 A.4 Our sqlpetr package The sqlpetr package is the companion R package for this database tutorial. It has two classes of functions: Functions to install the dependencies needed to build the book and perform the operations covered in the tutorial, and Utilities for dealing with Docker and the PostgreSQL Docker image we use. sqlpetr has a pkgdown site at https://smithjd.github.io/sqlpetr/. A.5 Docker Docker and the DevOps tools surrounding it have fostered a revolution in the way services are delivered over the internet. In this book, we’re piggybacking on a small piece of that revolution, Docker on the desktop. A.5.1 Virtual machines and hypervisors A virtual machine is a machine that is running purely as software hosted by another real machine. To the user, a virtual machine looks just like a real one. But it has no processors, memory or I/O devices of its own - all of those are supplied and managed by the host. A virtual machine can run any operating system that will run on the host’s hardware. A Linux host can run a Windows virtual machine and vice versa. A hypervisor is the component of the host system software that manages virtual machines, usually called guests. Linux systems have a native hypervisor called Kernel Virtual Machine (kvm). And laptop, desktop and server processors from Intel and Advanced Micro Devices (AMD) have hardware that makes this hypervisor more efficient. Windows servers and Windows 10 Pro have a hypervisor called Hyper-V. Like kvm, Hyper-V can take advantage of the hardware in Intel and AMD processors. On Macintosh, there is a Hypervisor Framework (https://developer.apple.com/documentation/hypervisor) and other tools build on that. If this book is about Docker, why do we care about virtual machines and hypervisors? Docker is a Linux subsystem - it only runs on Linux laptops, desktops and servers. As we’ll see shortly, if we want to run Docker on Windows or MacOS, we’ll need a hypervisor, a Linux virtual machine and some “glue logic” to provide a Docker user experience equivalent to the one on a Linux system. A.5.2 Containers A container is a set of processes running in an operating system. The host operating system is usually Linux, but other operating systems also can host containers. Unlike a virtual machine, the container has no operating system kernel of its own. If the host is running the Linux kernel, so is the container. And since the container OS is the same as the host OS, there’s no need for a hypervisor or hardware to support the hypervisor. So a container is more efficient than a virtual machine. A container does have its own file system. From inside the container, this file system looks like a Linux file system, but it can use any Linux distro. For example, you can have an Ubuntu 18.04 LTS host running Ubuntu 14.04 LTS or Fedora 28 or CentOS 7 containers. The kernel will always be the host kernel, but the utilities and applications will be those from the container. A.5.3 Docker itself While there are both older (lxc) and newer container tools, the one that has caught on in terms of widespread use is Docker (Docker 2019a). Docker is widely used on cloud providers to deploy services of all kinds. Using Docker on the desktop to deliver standardized packages, as we are doing in this book, is a secondary use case, but a common one. If you’re using a Linux laptop / desktop, all you need to do is install Docker CE (Docker 2018a). However, most laptops and desktops don’t run Linux - they run Windows or MacOS. As noted above, to use Docker on Windows or MacOS, you need a hypervisor and a Linux virtual machine. A.5.4 Docker objects The Docker subsystem manages several kinds of objects - containers, images, volumes and networks. In this book, we are only using the basic command line tools to manage containers, images and volumes. Docker images are files that define a container’s initial file system. You can find pre-built images on Docker Hub and the Docker Store - the base PostgreSQL image we use comes from Docker Hub (https://hub.docker.com/_/postgres/). If there isn’t a Docker image that does exactly what you want, you can build your own by creating a Dockerfile and running docker build. We do this in [Build the pet-sql Docker Image]. Docker volumes – explain mount. A.5.5 Hosting Docker on Windows machines There are two ways to get Docker on Windows. For Windows 10 Home and older versions of Windows, you need Docker Toolbox (Docker 2019e). Note that for Docker Toolbox, you need a 64-bit AMD or Intel processor with the virtualization hardware installed and enabled in the BIOS. For Windows 10 Pro, you have the Hyper-V virtualizer as standard equipment, and can use Docker for Windows (Docker 2019c). A.5.6 Hosting Docker on macOS machines As with Windows, there are two ways to get Docker. For older Intel systems, you’ll need Docker Toolbox (Docker 2019d). Newer systems (2010 or later running at least macOS El Capitan 10.11) can run Docker for Mac (Docker 2019b). A.5.7 Hosting Docker on UNIX machines Unix was the original host for both R and Docker. Unix-like commands show up. A.6 ‘Normal’ and ‘normalized’ data A.6.1 Tidy data Tidy data (Wickham 2014) is well-behaved from the point of view of analysis and tools in the Tidyverse (RStudio 2019). Tidy data is easier to think about and it is usually worthwhile to make the data tidy (Wickham 2018). Tidy data is roughly equivalent to third normal form as discussed below. A.6.2 Design of “normal data” Data in a database is most often optimized to minimize storage space and increase performance while preserving integrity when adding, changing, or deleting data. The Wikipedia article on Database Normalization has a good introduction to the characteristics of “normal” data and the process of re-organizing it to meet those desirable criteria (Wikipedia 2019). The bottom line is that “data normalization is practical” although there are mathematical arguments for normalization based on the preservation of data integrity. A.7 SQL Language SQL stands for Structured Query Language. It is a database language where we can perform certain operations on the existing database and we can use it create a new database. There are four main categories where the SQL commands fall into: DML, DDL, DCL, and TCL. A.7.1 Data Manipulation Langauge (DML) These four SQL commands deal with the manipulation of data in the database. For everyday analytical work, these are the commands that you will use the most. 1. SELECT 2. INSERT 3. UPDATE 4. DELETE A.7.2 Data Definition Langauge (DDL) It consists of the SQL commands that can be used to define a database schema. The DDL commands include: 1. CREATE 2. ALTER 3. TRUNCATE 4. COMMENT 5. RENAME 6. DROP A.7.3 Data Control Language (DCL) The DCL commands deals with user rights, permissions and other controls in database management system. 1. GRANT 2. REVOKE A.7.4 Transaction Control Language (TCL) These commands deal with the control over transaction within the database. Transaction combines a set of tasks into single execution. 1. SET TRANSACTION 2. SAVEPOINT 3. ROLLBACK 4. COMMIT A.8 Enterprise DBMS The organizational context of a database matters just as much as its design characteristics. The design of a database (or data model) may have been purchased from an external vendor or developed in-house. In either case time has a tendency to erode the original design concept so that the data you find in a DBMS may not quite match the original design specification. And the original design may or may not be well reflected in the current naming of tables, columns and other objects. It’s a naive misconception to think that the data you are analyzing just “comes from the database”, although that’s literally true and may be the step that happens before you get your hands on it. In fact it comes from the people who design, enter, manage, protect, and use your organization’s data. In practice, a database administrator (DBA) is often a key point of contact in terms of access and may have stringent criteria for query performance. Make friends with your DBA. A.8.1 SQL databases Although there are ANSI standards for SQL syntax, different implementations vary in enough details that R’s ability to customize queries for those implementations is very helpful. The tables in a DBMS correspond to a data frame in R, so interaction with a DBMS is fairly natural for useRs. SQL code is characterized by the fact that it describes what to retrieve, leaving the DBMS back end to determine how to do it. Therefore it has a batch feel. The pipe operator (%&gt;%, which is read as and then) is inherently procedural when it’s used with dplyr: it can be used to construct queries step-by-step. Once a test dplyr query has been executed, it is easy to inspect the results and add steps with the pipe operator to refine or expand the query. A.8.2 Data mapping between R vs SQL data types The following code shows how different elements of the R bestiary are translated to and from ANSI standard data types. Note that R factors are translated as TEXT so that missing levels are ignored on the SQL side. library(DBI) dbDataType(ANSI(), 1:5) ## [1] &quot;INT&quot; dbDataType(ANSI(), 1) ## [1] &quot;DOUBLE&quot; dbDataType(ANSI(), TRUE) ## [1] &quot;SMALLINT&quot; dbDataType(ANSI(), Sys.Date()) ## [1] &quot;DATE&quot; dbDataType(ANSI(), Sys.time()) ## [1] &quot;TIMESTAMP&quot; dbDataType(ANSI(), Sys.time() - as.POSIXct(Sys.Date())) ## [1] &quot;TIME&quot; dbDataType(ANSI(), c(&quot;x&quot;, &quot;abc&quot;)) ## [1] &quot;TEXT&quot; dbDataType(ANSI(), list(raw(10), raw(20))) ## [1] &quot;BLOB&quot; dbDataType(ANSI(), I(3)) ## [1] &quot;DOUBLE&quot; dbDataType(ANSI(), iris) ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## &quot;DOUBLE&quot; &quot;DOUBLE&quot; &quot;DOUBLE&quot; &quot;DOUBLE&quot; &quot;TEXT&quot; The DBI specification provides extensive documentation that is worth digesting if you intend to work with a DBMS from R. As you work through the examples in this book, you will also want to refer to the following resources: RStudio’s Databases using R site describes many of the technical details involved. The RStudio community is an excellent place to ask questions or study what has been discussed previously. A.8.3 PostgreSQL and connection parameters An important detail: We use a PostgreSQL database server running in a Docker container for the database functions. It is installed inside Docker, so you do not have to download or install it yourself. To connect to it, you have to define some parameters. These parameters are used in two places: When the Docker container is created, they’re used to initialize the database, and Whenever we connect to the database, we need to specify them to authenticate. We define the parameters in an environment file that R reads when starting up. The file is called .Renviron, and is located in your home directory. See the discussion of securing and using dbms credentials. A.8.4 Connecting the R and DBMS environments Although everything happens on one machine in our Docker / PostgreSQL playground, in real life R and PostgreSQL (or other DBMS) will be in different environments on separate machines. How R connects them gives you control over where the work happens. You need to be aware of the differences beween the R and DBMS environments as well as how you can leverage the strengths of each one. Characteristics of local vs. server processing Dimension Local Remote Design purpose The R environment on your local machine is designed to be flexible and easy to use; ideal for data investigation. The DBMS environment is designed for large and complex databases where data integrity is more important than flexibility or ease of use. Processor power Your local machine has less memory, speed, and storage than the typical database server. Database servers are specialized, more expensive, and have more power. Memory constraint In R, query results must fit into memory. Servers have a lot of memory and write intermediate results to disk if needed without you knowing about it. Data crunching Data lives in the DBMS, so crunching it down locally requires you to pull it over the network. A DBMS has powerful data crunching capabilities once you know what you want and moves data over the server backbone to crunch it. Security Local control. Whether it is good or not depends on you. Responsibility of database administrators who set the rules. You play by their rules. Storage of intermediate results Very easy to save a data frame with intermediate results locally. May require extra privileges to save results in the database. Analytical resources Ecosystem of available R packages Extending SQL instruction set involves dbms-specific functions or R pseudo functions Collaboration One person working on a few data.frames. Many people collaborating on many tables. A.8.5 Using SQLite to simulate an enterprise DBMS SQLite engine is embedded in one file, so that many tables are stored together in one object. SQL commands can run against an SQLite database as demonstrated in how many uses of SQLite are in the RStudio dbplyr documentation. References "],
["chapter-appendix-setup-instructions.html", "B - Setup instructions B.1 Sandbox prerequisites B.2 R, RStudio and Git B.3 Install Docker", " B - Setup instructions This appendix explains: Hardware and software prerequisites for setting up the sandbox used in this book Documentation for all of the elements used in this sandbox B.1 Sandbox prerequisites The sandbox environment requires: A computer running Windows (Windows 7 64-bit or later - Windows 10-Pro is recommended), MacOS, or Linux (any Linux distro that will run Docker Community Edition, R and RStudio will work) Current versions of R and RStudio [Vargas (2018)) required. Docker (instructions below) Our companion package sqlpetr (Borasky et al. 2018) The database we use is PostgreSQL 11, but you do not need to install it - it’s installed via a Docker image. In addition to the current version of R and RStudio, you will need current versions of the following packages: DBI (R Special Interest Group on Databases (R-SIG-DB), Wickham, and Müller 2019) DiagrammeR (Iannone 2020) RPostgres (Wickham, Ooms, and Müller 2019) dbplyr (Wickham and Ruiz 2019) devtools (Wickham, Hester, and Chang 2019) downloader (Chang 2015) glue (Hester 2019) here (Müller 2017) knitr (Xie 2020b) skimr (Waring et al. 2019) tidyverse (Wickham 2019) bookdown (Xie 2020a) (for compiling the book, if you want to) B.2 R, RStudio and Git Most readers will probably have these already, but if not: If you do not have R: Go to https://cran.rstudio.com/ (R Core Team 2018). Select the download link for your system. For Linux, choose your distro. We recommend Ubuntu 18.04 LTS “Bionic Beaver”. It’s much easier to find support answers on the web for Ubuntu than other distros. Follow the instructions. Note: if you already have R, make sure it’s upgraded to R 3.5.1. We don’t test on older versions! If you do not have RStudio: go to https://www.rstudio.com/products/rstudio/download/#download. Make sure you have version 1.1.463 or later. If you do not have Git: On Windows, go to https://git-scm.com/download/win and follow instructions. There are a lot of options. Just pick the defaults!!! On MacOS, go to https://sourceforge.net/projects/git-osx-installer/files/ and follow instructions. On Linux, install Git from your distribution. B.3 Install Docker Installation depends on your operating system and we have found that it can be somewhat intricate. You will need Docker Community Edition (Docker CE): For Windows, consider these issues and follow these instructions: Go to https://store.docker.com/editions/community/docker-ce-desktop-windows. If you don’t have a Docker Store log in, you’ll need to create one. Then: If you have Windows 10 Pro, download and install Docker for Windows. If you have an older version of Windows, download and install Docker Toolbox (https://docs.docker.com/toolbox/overview/). Note that both versions require 64-bit hardware and the virtualization needs to be enabled in the firmware. On a Mac (Docker 2018c): Go to https://store.docker.com/editions/community/docker-ce-desktop-mac. If you don’t have a Docker Store login, you’ll need to create one. Then download and install Docker for Mac. Your MacOS must be at least release Yosemite (10.10.3). On UNIX flavors (Docker 2018a): note that, as with Windows and MacOS, you’ll need a Docker Store loin. Although most Linux distros ship with some version of Docker, chances are it’s not the same as the official Docker CE version. Ubuntu: https://store.docker.com/editions/community/docker-ce-server-ubuntu, Fedora: https://store.docker.com/editions/community/docker-ce-server-fedora, Cent OS: https://store.docker.com/editions/community/docker-ce-server-centos, Debian: https://store.docker.com/editions/community/docker-ce-server-debian. Note that on Linux, you will need to be a member of the docker group to use Docker. To do that, execute sudo usermod -aG docker ${USER}. Then, log out and back in again. References "],
["chapter-appendix-postgres-local-db-installation.html", "C Appendix E - Install adventureworks on your own machine C.1 Overview C.2 Resources", " C Appendix E - Install adventureworks on your own machine This appendix demonstrates how to: Setup the adventureworks database locally on your machine Connect to the adventureworks database These instructions should be tested by a Windows user The PostgreSQL tutorial links do not work, despite being pasted from the site C.1 Overview This appendix details the process to download and restore the adventureworks database so that you can work with the database locally on your own machine. This tutorial assumes that (1) you have PostgreSQL installed on your computer, and (2) that you have configured your system to run psql at the command line. Installation of PostgreSQL and configuration of psql are outside the scope of this book. C.1.1 Download the adventureworks database Download the adventureworks database from here. C.1.2 Restore the dvdrental database at the command line Launch the psql tool Enter account information to log into the PostgreSQL database server, if prompted Enter the following command to create a new database CREATE DATABASE adventureworks; Open a new terminal window (not in psql) and navigate to the folder where the adventureworks.sql file is located. Use the cd command in the terminal, followed by the file path to change directories to the location of adventureworks.sql. For example: cd /Users/username/Documents/adventureworks. Enter the following command prompt: pg_restore -d adventureworks -f -U postgres adventureworks.sql C.1.3 Restore the adventureworks database using pgAdmin Another option to restore the adventureworks database locally on your machine is with the pgAdmin graphical user interface. However, we highly recommend using the command line methods detailed above. Installation and configuration of pgAdmin is outside the scope of this book. C.2 Resources Instructions by PostgreSQL Tutorial to load the dvdrental database. (PostgreSQL Tutorial Website 2019). Windows installation of PostgreSQL by PostgreSQL Tutorial. (PostgreSQL Tutorial Website 2019). Installation of PostgreSQL on a Mac using Postgres.app. (Postgres.app 2019). Command line configuration of PosgreSQL on a Mac with Postgres.app. (Postgres.app 2019). Installing PostgreSQL for Linux, Arch Linux, Windows, Mac and other operating systems, by Postgres Guide. (Postgres Guide Website 2019). "],
["chapter-windows-tech-details.html", "D Appendix B - Additional technical details for Windows users D.1 Hardware requirements D.2 Software requirements D.3 Docker for Windows settings D.4 Git, GitHub and line endings", " D Appendix B - Additional technical details for Windows users This chapter explains: How to setup your environment for Windows How to use Git and GitHub effectively on Windows Skip these instructions if your computer has either OSX or a Unix variant. D.1 Hardware requirements You will need an Intel or AMD processor with 64-bit hardware and the hardware virtualization feature. Most machines you buy today will have that, but older ones may not. You will need to go into the BIOS / firmware and enable the virtualization feature. You will need at least 4 gigabytes of RAM! D.2 Software requirements You will need Windows 7 64-bit or later. If you can afford it, I highly recommend upgrading to Windows 10 Pro. D.2.1 Windows 7, 8, 8.1 and Windows 10 Home (64 bit) Install Docker Toolbox. The instructions are here: https://docs.docker.com/toolbox/toolbox_install_windows/. Make sure you try the test cases and they work! D.2.2 Windows 10 Pro Install Docker for Windows stable. The instructions are here: https://docs.docker.com/docker-for-windows/install/#start-docker-for-windows. Again, make sure you try the test cases and they work. D.3 Docker for Windows settings D.3.1 Shared drives If you’re going to mount host files into container file systems (as we do in the following chapters), you need to set up shared drives. Open the Docker settings dialog and select Shared Drives. Check the drives you want to share. In this screenshot, the D: drive is my 1 terabyte hard drive. D.3.2 Kubernetes Kubernetes is a container orchestration / cloud management package that’s a major DevOps tool. It’s heavily supported by Red Hat and Google, and as a result is becoming a required skill for DevOps. However, it’s overkill for this project at the moment. So you should make sure it’s not enabled. Go to the Kubernetes dialog and make sure the Enable Kubernetes checkbox is cleared. D.4 Git, GitHub and line endings Git was originally developed for Linux - in fact, it was created by Linus Torvalds to manage hundreds of different versions of the Linux kernel on different machines all around the world. As usage has grown, Git has achieved a huge following and is the version control system used by most large open source projects, including this one. If you’re on Windows, there are some things about Git and GitHub you need to watch. First of all, there are quite a few tools for running Git on Windows, but the RStudio default and recommended one is Git for Windows (https://git-scm.com/download/win). By default, text files on Linux end with a single linefeed (\\n) character. But on Windows, text files end with a carriage return and a line feed (\\r\\n). See https://en.wikipedia.org/wiki/Newline for the gory details. Git defaults to checking files out in the native mode. So if you’re on Linux, a text file will show up with the Linux convention, and if you’re on Windows, it will show up with the Windows convention. Most of the time this doesn’t cause any problems. But Docker containers usually run Linux, and if you have files from a repository on Windows that you’ve sent to the container, the container may malfunction or give weird results. This kind of situation has caused a lot of grief for contributors to this project, so beware. In particular, executable sh or bash scripts will fail in a Docker container if they have Windows line endings. You may see an error message with \\r in it, which means the shell saw the carriage return (\\r) and gave up. But often you’ll see no hint at all what the problem was. So you need a way to tell Git that some files need to be checked out with Linux line endings. See https://help.github.com/articles/dealing-with-line-endings/ for the details. Summary: You’ll need a .gitattributes file in the root of the repository. In that file, all text files (scripts, program source, data, etc.) that are destined for a Docker container will need to have the designator &lt;spec&gt; text eol=lf, where &lt;spec&gt; is the file name specifier, for example, *.sh. This repo includes a sample: .gitattributes "],
["chapter-appendix-postresql-authentication.html", "E Appendix C - PostgreSQL Authentication E.1 Introduction E.2 Password authentication on the PostgreSQL Docker image E.3 Adding roles", " E Appendix C - PostgreSQL Authentication E.1 Introduction PostgreSQL has a very robust and flexible set of authentication methods (PostgreSQL Global Development Group 2018a). In most production environments, these will be managed by the database administrator (DBA) on a need-to-access basis. People and programs will be granted access only to a minimum set of capabilities required to function, and nothing more. In this book, we are using a PostgreSQL Docker image (Docker 2018d). When we create a container from that image, we use its native mechanism to create the postgres database superuser with a password specified in an R environment file ~/.Renviron. See Securing and using your dbms log-in credentials for how we do this. What that means is that you are the DBA - the database superuser - for the PostgreSQL database cluster running in the container! You can create and destroy databases, schemas, tables, views, etc. You can also create and destroy users - called roles in PostgreSQL, and GRANT or REVOKE their privileges with great precision. You don’t have to do that to use this book. But if you want to experiment with it, feel free! E.2 Password authentication on the PostgreSQL Docker image Of the many PostgreSQL authentication mechanisms, the simplest that’s universallly available is password authentication (PostgreSQL Global Development Group 2018c). That’s what we use for the postgres database superuser, and what we recommend for any roles you may create. Once a role has been created, you need five items to open a connection to the PostgreSQL database cluster: The host. This is a name or IP address that your network can access. In this book, with the database running in a Docker container, that’s usually localhost. The port. This is the port the server is listening on. It’s usually the default, 5439, and that’s what we use. But in a secure environment, it will often be some random number to lower the chances that an attacker can find the database server. And if you have more than one server on the network, you’ll need to use different ports for each of them. The dbname to connect to. This database must exist or the connection attempt will fail. The user. This user must exist in the database cluster and be allowed to access the database. We are using the database superuser postgres in this book. The password. This is set by the DBA for the user. In this book we use the password defined in Securing and using your dbms log-in credentials. E.3 Adding roles As noted above, PostgreSQL has a very flexible fine-grained access permissions system. We can’t cover all of it; see PostgreSQL Global Development Group (2018b) for the full details. But we can give an example. E.3.1 Setting up Docker First, we need to make sure we don’t have any other databases listening on the default port 5439. library(tidyverse) ## ── Attaching packages ─────────────────────────── tidyverse 1.3.0 ── ## ✓ ggplot2 3.2.1 ✓ purrr 0.3.3 ## ✓ tibble 2.1.3 ✓ dplyr 0.8.3 ## ✓ tidyr 1.0.2 ✓ stringr 1.4.0 ## ✓ readr 1.3.1 ✓ forcats 0.4.0 ## ── Conflicts ────────────────────────────── tidyverse_conflicts() ── ## x dplyr::filter() masks stats::filter() ## x dplyr::lag() masks stats::lag() library(DBI) library(RPostgres) library(connections) sqlpetr::sp_check_that_docker_is_up() ## [1] &quot;Docker is up but running no containers&quot; sqlpetr::sp_docker_remove_container(&quot;cattle&quot;) ## [1] 0 # in case you&#39;ve been doing things out of order, stop a container named &#39;adventureworks&#39; if it exists: # sqlpetr::sp_docker_stop(&quot;adventureworks&quot;) E.3.2 Creating a new container We’ll create a “cattle” container with a default PostgreSQL 10 database cluster. sqlpetr::sp_make_simple_pg(&quot;cattle&quot;) # con &lt;- connection_open( # use in an interactive session con &lt;- dbConnect( # use in other settings RPostgres::Postgres(), # without the following (and preceding) lines, # bigint become int64 which is a problem for ggplot bigint = &quot;integer&quot;, host = &quot;localhost&quot;, port = 5439, dbname = &quot;postgres&quot;, user = &quot;postgres&quot;, password = &quot;postgres&quot;) E.3.3 Adding a role Now, let’s add a role. We’ll add a role that can log in and create databases, but isn’t a superuser. Since this is a demo and not a real production database cluster, we’ll specify a password in plaintext. And we’ll create a database for our new user. Create the role: DBI::dbExecute( con, &quot;CREATE ROLE charlie LOGIN CREATEDB PASSWORD &#39;chaplin&#39;;&quot; ) ## [1] 0 Create the database: DBI::dbExecute( con, &quot;CREATE DATABASE charlie OWNER = charlie&quot;) ## [1] 0 E.3.4 Did it work? DBI::dbDisconnect(con) con &lt;- sqlpetr::sp_get_postgres_connection( host = &quot;localhost&quot;, port = 5439, dbname = &quot;postgres&quot;, user = &quot;charlie&quot;, password = &quot;chaplin&quot;, seconds_to_test = 30 ) OK, we can connect. Let’s do some stuff! data(&quot;iris&quot;) dbCreateTable creates the table with columns matching the data frame. But it does not send data to the table. DBI::dbCreateTable(con, &quot;iris&quot;, iris) To send data, we use dbAppendTable. DBI::dbAppendTable(con, &quot;iris&quot;, iris) ## Warning: Factors converted to character ## [1] 150 DBI::dbListTables(con) ## [1] &quot;iris&quot; head(DBI::dbReadTable(con, &quot;iris&quot;)) ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## 1 5.1 3.5 1.4 0.2 setosa ## 2 4.9 3.0 1.4 0.2 setosa ## 3 4.7 3.2 1.3 0.2 setosa ## 4 4.6 3.1 1.5 0.2 setosa ## 5 5.0 3.6 1.4 0.2 setosa ## 6 5.4 3.9 1.7 0.4 setosa E.3.5 Disconnect and remove the container DBI::dbDisconnect(con) # or if using the connections package, use: # connection_close(con) sqlpetr::sp_docker_remove_container(&quot;cattle&quot;) ## [1] 0 References "],
["chapter-appendix-dbi-index.html", "F DBI package functions - INDEX", " F DBI package functions - INDEX Where are these covered and should the by included? DBI 1st time Call Example/Notes DBIConnct 6.3.2 (04) in sp_get_postgres_connection dbAppendTable dbCreateTable dbDisconnect 6.4n (04) dbDisconnect(con) dbExecute 10.4.2 (13) Executes a statement and returns the number of rows affected. dbExecute() comes with a default implementation (which should work with most backends) that calls dbSendStatement(), then dbGetRowsAffected(), ensuring that the result is always free-d by dbClearResult(). dbExistsTable dbExistsTable(con,‘actor’) dbFetch 17.1 (72) dbFetch(rs) dbGetException dbGetInfo dbGetInfo(con) dbGetQuery 10.4.1 (13) dbGetQuery(con,‘select * from store;’) dbIsReadOnly dbIsReadOnly(con) dbIsValid dbIsValid(con) dbListFields 6.3.3 (04) DBI::dbListFields(con, “mtcars”) dbListObjects dbListObjects(con) dbListTables 6.3.2 (04) DBI::dbListTables(con, con) dbReadTable 8.1.2 DBI::dbReadTable(con, “rental”) dbRemoveTable dbSendQuery 17.1 (72) rs &lt;- dbSendQuery(con, “SELECT * FROM mtcars WHERE cyl = 4”) dbSendStatement The dbSendStatement() method only submits and synchronously executes the SQL data manipulation statement (e.g., UPDATE, DELETE, INSERT INTO, DROP TABLE, …) to the database engine. dbWriteTable 6.3.3 (04) dbWriteTable(con, “mtcars”, mtcars, overwrite = TRUE) "],
["chapter-appendix-dplyr-to-postres-translation.html", "G Appendix _ Dplyr to SQL translations G.1 Overview", " G Appendix _ Dplyr to SQL translations You may be interested in exactly how the DBI package translates R functions into their SQL quivalents – and in which functions are translated and which are not. This Appendix answers those questions. It is based on the work of Dewey Dunnington (@paleolimbot) which he published here: https://apps.fishandwhistle.net/archives/1503 https://rud.is/b/2019/04/10/lost-in-sql-translation-charting-dbplyr-mapped-sql-function-support-across-all-backends/ G.1 Overview These packages are called below: library(tidyverse) library(dbplyr) library(gt) library(here) library(sqlpetr) list the DBI functions that are available: names(sql_translate_env(simulate_dbi())) ## [1] &quot;-&quot; &quot;:&quot; &quot;!&quot; &quot;!=&quot; ## [5] &quot;(&quot; &quot;[&quot; &quot;[[&quot; &quot;{&quot; ## [9] &quot;*&quot; &quot;/&quot; &quot;&amp;&quot; &quot;&amp;&amp;&quot; ## [13] &quot;%%&quot; &quot;%&gt;%&quot; &quot;%in%&quot; &quot;^&quot; ## [17] &quot;+&quot; &quot;&lt;&quot; &quot;&lt;=&quot; &quot;==&quot; ## [21] &quot;&gt;&quot; &quot;&gt;=&quot; &quot;|&quot; &quot;||&quot; ## [25] &quot;$&quot; &quot;abs&quot; &quot;acos&quot; &quot;as_date&quot; ## [29] &quot;as_datetime&quot; &quot;as.character&quot; &quot;as.Date&quot; &quot;as.double&quot; ## [33] &quot;as.integer&quot; &quot;as.integer64&quot; &quot;as.logical&quot; &quot;as.numeric&quot; ## [37] &quot;as.POSIXct&quot; &quot;asin&quot; &quot;atan&quot; &quot;atan2&quot; ## [41] &quot;between&quot; &quot;bitwAnd&quot; &quot;bitwNot&quot; &quot;bitwOr&quot; ## [45] &quot;bitwShiftL&quot; &quot;bitwShiftR&quot; &quot;bitwXor&quot; &quot;c&quot; ## [49] &quot;case_when&quot; &quot;ceil&quot; &quot;ceiling&quot; &quot;coalesce&quot; ## [53] &quot;cos&quot; &quot;cosh&quot; &quot;cot&quot; &quot;coth&quot; ## [57] &quot;day&quot; &quot;desc&quot; &quot;exp&quot; &quot;floor&quot; ## [61] &quot;hour&quot; &quot;if&quot; &quot;if_else&quot; &quot;ifelse&quot; ## [65] &quot;is.na&quot; &quot;is.null&quot; &quot;log&quot; &quot;log10&quot; ## [69] &quot;mday&quot; &quot;minute&quot; &quot;month&quot; &quot;na_if&quot; ## [73] &quot;nchar&quot; &quot;now&quot; &quot;paste&quot; &quot;paste0&quot; ## [77] &quot;pmax&quot; &quot;pmin&quot; &quot;qday&quot; &quot;round&quot; ## [81] &quot;second&quot; &quot;sign&quot; &quot;sin&quot; &quot;sinh&quot; ## [85] &quot;sql&quot; &quot;sqrt&quot; &quot;str_c&quot; &quot;str_conv&quot; ## [89] &quot;str_count&quot; &quot;str_detect&quot; &quot;str_dup&quot; &quot;str_extract&quot; ## [93] &quot;str_extract_all&quot; &quot;str_flatten&quot; &quot;str_glue&quot; &quot;str_glue_data&quot; ## [97] &quot;str_interp&quot; &quot;str_length&quot; &quot;str_locate&quot; &quot;str_locate_all&quot; ## [101] &quot;str_match&quot; &quot;str_match_all&quot; &quot;str_order&quot; &quot;str_pad&quot; ## [105] &quot;str_remove&quot; &quot;str_remove_all&quot; &quot;str_replace&quot; &quot;str_replace_all&quot; ## [109] &quot;str_replace_na&quot; &quot;str_sort&quot; &quot;str_split&quot; &quot;str_split_fixed&quot; ## [113] &quot;str_squish&quot; &quot;str_sub&quot; &quot;str_subset&quot; &quot;str_to_lower&quot; ## [117] &quot;str_to_title&quot; &quot;str_to_upper&quot; &quot;str_trim&quot; &quot;str_trunc&quot; ## [121] &quot;str_view&quot; &quot;str_view_all&quot; &quot;str_which&quot; &quot;str_wrap&quot; ## [125] &quot;substr&quot; &quot;switch&quot; &quot;tan&quot; &quot;tanh&quot; ## [129] &quot;today&quot; &quot;tolower&quot; &quot;toupper&quot; &quot;trimws&quot; ## [133] &quot;wday&quot; &quot;xor&quot; &quot;yday&quot; &quot;year&quot; ## [137] &quot;cume_dist&quot; &quot;cummax&quot; &quot;cummean&quot; &quot;cummin&quot; ## [141] &quot;cumsum&quot; &quot;dense_rank&quot; &quot;first&quot; &quot;lag&quot; ## [145] &quot;last&quot; &quot;lead&quot; &quot;max&quot; &quot;mean&quot; ## [149] &quot;median&quot; &quot;min&quot; &quot;min_rank&quot; &quot;n&quot; ## [153] &quot;n_distinct&quot; &quot;nth&quot; &quot;ntile&quot; &quot;order_by&quot; ## [157] &quot;percent_rank&quot; &quot;quantile&quot; &quot;rank&quot; &quot;row_number&quot; ## [161] &quot;sum&quot; &quot;var&quot; &quot;cume_dist&quot; &quot;cummax&quot; ## [165] &quot;cummean&quot; &quot;cummin&quot; &quot;cumsum&quot; &quot;dense_rank&quot; ## [169] &quot;first&quot; &quot;lag&quot; &quot;last&quot; &quot;lead&quot; ## [173] &quot;max&quot; &quot;mean&quot; &quot;median&quot; &quot;min&quot; ## [177] &quot;min_rank&quot; &quot;n&quot; &quot;n_distinct&quot; &quot;nth&quot; ## [181] &quot;ntile&quot; &quot;order_by&quot; &quot;percent_rank&quot; &quot;quantile&quot; ## [185] &quot;rank&quot; &quot;row_number&quot; &quot;sum&quot; &quot;var&quot; sql_translate_env(simulate_dbi()) ## &lt;sql_variant&gt; ## scalar: -, :, !, !=, (, [, [[, {, *, /, &amp;, &amp;&amp;, %%, %&gt;%, %in%, ^, +, ## scalar: &lt;, &lt;=, ==, &gt;, &gt;=, |, ||, $, abs, acos, as_date, as_datetime, ## scalar: as.character, as.Date, as.double, as.integer, as.integer64, ## scalar: as.logical, as.numeric, as.POSIXct, asin, atan, atan2, ## scalar: between, bitwAnd, bitwNot, bitwOr, bitwShiftL, bitwShiftR, ## scalar: bitwXor, c, case_when, ceil, ceiling, coalesce, cos, cosh, ## scalar: cot, coth, day, desc, exp, floor, hour, if, if_else, ifelse, ## scalar: is.na, is.null, log, log10, mday, minute, month, na_if, ## scalar: nchar, now, paste, paste0, pmax, pmin, qday, round, second, ## scalar: sign, sin, sinh, sql, sqrt, str_c, str_conv, str_count, ## scalar: str_detect, str_dup, str_extract, str_extract_all, ## scalar: str_flatten, str_glue, str_glue_data, str_interp, ## scalar: str_length, str_locate, str_locate_all, str_match, ## scalar: str_match_all, str_order, str_pad, str_remove, ## scalar: str_remove_all, str_replace, str_replace_all, ## scalar: str_replace_na, str_sort, str_split, str_split_fixed, ## scalar: str_squish, str_sub, str_subset, str_to_lower, str_to_title, ## scalar: str_to_upper, str_trim, str_trunc, str_view, str_view_all, ## scalar: str_which, str_wrap, substr, switch, tan, tanh, today, ## scalar: tolower, toupper, trimws, wday, xor, yday, year ## aggregate: cume_dist, cummax, cummean, cummin, cumsum, dense_rank, ## aggregate: first, lag, last, lead, max, mean, median, min, min_rank, n, ## aggregate: n_distinct, nth, ntile, order_by, percent_rank, quantile, ## aggregate: rank, row_number, sum, var ## window: cume_dist, cummax, cummean, cummin, cumsum, dense_rank, ## window: first, lag, last, lead, max, mean, median, min, min_rank, n, ## window: n_distinct, nth, ntile, order_by, percent_rank, quantile, ## window: rank, row_number, sum, var source(here(&quot;book-src&quot;, &quot;dbplyr-sql-function-translation.R&quot;)) ## Warning: The `.drop` argument of `unnest()` is deprecated as of tidyr 1.0.0. ## All list-columns are now preserved. ## This warning is displayed once per session. ## Call `lifecycle::last_warnings()` to see where this warning was generated. Each of the following dbplyr back ends may have a slightly different translation: translations %&gt;% filter(!is.na(sql)) %&gt;% count(variant) ## # A tibble: 11 x 2 ## variant n ## &lt;chr&gt; &lt;int&gt; ## 1 access 193 ## 2 dbi 183 ## 3 hive 187 ## 4 impala 190 ## 5 mssql 196 ## 6 mysql 194 ## 7 odbc 186 ## 8 oracle 184 ## 9 postgres 204 ## 10 sqlite 183 ## 11 teradata 196 Only one postgres translation produces an output: psql &lt;- translations %&gt;% filter(!is.na(sql), variant == &quot;postgres&quot;) %&gt;% select(r, n_args, sql) %&gt;% arrange(r) # sp_print_df(head(psql, n = 40)) sp_print_df(psql) "],
["chapter-appendix-additional-resources.html", "H Appendix Additional resources H.1 Editing this book H.2 Docker alternatives H.3 Docker and R H.4 Documentation for Docker and PostgreSQL H.5 SQL and dplyr H.6 More Resources", " H Appendix Additional resources H.1 Editing this book Here are instructions for editing this book H.2 Docker alternatives Choosing between Docker and Vagrant (Zait 2017) H.3 Docker and R Noam Ross’ talk on Docker for the UseR (Ross 2018b) and his Slides (Ross 2018a) give a lot of context and tips. Good Docker tutorials An introductory Docker tutorial (Srivastav 2018) A Docker curriculum (Hall 2018) Scott Came’s materials about Docker and R on his website (Came 2018) and at the 2018 UseR Conference focus on R inside Docker. It’s worth studying the ROpensci Docker tutorial (ROpenSciLabs 2018) H.4 Documentation for Docker and PostgreSQL The Postgres image documentation (Docker 2018d) PostgreSQL &amp; Docker documentation (Docker 2018d) Dockerize PostgreSQL (Docker 2018b) Usage examples of PostgreSQL with Docker WARNING-EXPIRED CERTIFICATE 2018-12-20 H.5 SQL and dplyr Why SQL is not for analysis but dplyr is (Nishida 2016) Data Manipulation with dplyr (With 50 Examples) (ListenData.com 2016) H.6 More Resources David Severski describes some key elements of connecting to databases with R for MacOS users (Severski 2018) This tutorial picks up ideas and tips from Ed Borasky’s Data Science pet containers (Borasky 2018), which creates a framework based on that Hack Oregon example and explains why this repo is named pet-sql. References "],
["references.html", "References", " References "]
]
