[
["index.html", "R, Databases, and Docker Chapter 1 Introduction 1.1 Using R to query a DBMS in your organization 1.2 Docker as a tool for UseRs 1.3 Alternatives to Docker 1.4 Who are we? 1.5 How did this project come about? 1.6 Navigation", " R, Databases, and Docker John David Smith, Sophie Yang, M. Edward (Ed) Borasky, Jim Tyhurst, Scott Came, Mary Anne Thygesen, and Ian Frantz 2019-07-12 Chapter 1 Introduction This chapter introduces: The motivation for this book and the strategies we have adopted How Docker can be used to set up a dbms to demonstrate access to a service like PostgreSQL from R Our team and how this project came about 1.1 Using R to query a DBMS in your organization Many R users (or useRs) live a dual life: in the vibrant open-source R community where R is created, improved, discussed, and taught. And then they go to work in a secured, complex, closed organizational environment where they may be on their own. Here is a request on the Rstudio community site for help that has been lightly edited to emphasize the generality that we see: I’m trying to migrate some inherited scripts that […] to connect to a […] database to […] instead. I’ve reviewed the https://db.rstudio.com docs and tried a number of configurations but haven’t been able to connect. I’m in uncharted territory within my org, so haven’t been able to get much help internally. This book will help you create a hybrid environment on your machine that can mimic some of the uncharted territory in your organization. It goes far beyond the basic connection issues and covers issues that you face when you are finding your way around or writing queries to your organization’s databases, not just when maintaining inherited scripts. Technology hurdles. The interfaces (passwords, packages, etc.) and gaps between R and a back end database are hidden from public view as a matter of security, so pinpointing exactly where a problem is can be difficult. A simulated environment such as we offer here can be an important learning resource. Scale issues. We see at least two types of scale issues. Handling large volumes of data so that performance issues must be a consideration requires a basic understanding of what’s happening in “the back end” (which is necessarily hidden from view). Therefore mastering techniques for drawing samples or small batches of data are essential. In addition to their size, your organization’s databases will often have structural characteristics that are complex and obscure. Data documentation is often incomplete and emphasizes operational characteristics, rather than analytic opportunities. A careful useR often needs to confirm the documentation on the fly and de-normalize data carefully. Use cases. R users frequently need to make sense of an organization’s complex data structures and coding schemes to address incompletely formed questions so that informal exploratory data analysis has to be intuitive and fast. The technology details should not get in the way. Sharing and discussing exploratory and diagnostic retrieval techniquesis best in public, but is constrained by organizational requirements. We have found that PostgreSQL in a Docker container solves many of the foregoing problems. 1.2 Docker as a tool for UseRs Noam Ross’s “Docker for the UseR” (Ross 2018a) suggests that there are four distinct Docker use-cases for useRs. Make a fixed working environment for reproducible analysis Access a service outside of R (e.g., PostgreSQL) Create an R based service (e.g., with plumber) Send our compute jobs to the cloud with minimal reconfiguration or revision This book explores #2 because it allows us to work on the database access issues described above and to practice on an industrial-scale DBMS. Docker is a comparatively easy way to simulate the relationship between an R/RStudio session and a database – all on on your machine (provided you have Docker installed and running). Running PostgreSQL on a Docker container avoids OS or system dependencies or conflicts that cause confusion and limit reproducibility. A Docker environment consumes relatively few resources. Our sandbox does much less but only includes PostgreSQL and sample data, so it takes up about 5% of the space taken up by the Vagrant environment that inspired this project. (Makubuya 2018) A simple Docker container such as the one used in our sandbox is easy to use and could be extended for other uses. Docker is a widely used technology for deploying applications in the cloud, so for many useRs it’s worth mastering. 1.3 Alternatives to Docker We have found Docker to be a great tool for simulating the complexities of an enterprise environment. However, installing Docker can be challenging, especially for Windows users. Therefore the code in this book depends on PostgreSQL(2019) in a Docker container, but it can all be readily adapted to either SQLite(“SQLite” 2019), PostgreSQL running natively on your computer, or even PostgreSQL running in the cloud. The technical details of these alternatives are all in separate chapters. 1.4 Who are we? We have been collaborating on this book since the Summer of 2018, each of us chipping into the project as time permits: Ian Franz - @ianfrantz Jim Tyhurst - @jimtyhurst John David Smith - @smithjd M. Edward (Ed) Borasky - @znmeb Maryanne Thygesen @maryannet Scott Came - @scottcame Sophie Yang - @SophieMYang 1.5 How did this project come about? We trace this book back to the June 2, 2018 Cascadia R Conf where Aaron Makubuya gave a presentation using Vagrant hosting (Makubuya 2018). After that John Smith, Ian Franz, and Sophie Yang had discussions after the monthly Data Discussion Meetups about the difficulties around setting up Vagrant (a virtual environment), connecting to an enterprise database, and having realistic public environment to demo or practice the issues that come up behind corporate firewalls. Scott Came’s tutorial on R and Docker (Came 2018) (an alternative to Vagrant) at the 2018 UseR Conference in Melbourne was provocative and it turned out he lived nearby. We re-connected with M. Edward (Ed) Borasky who had done extensive development for a Hack Oregon data science containerization project (Borasky 2018). 1.6 Navigation If this is the first bookdown (Xie 2016) book you’ve read, here’s how to navigate the website. The controls on the upper left: there are four controls on the upper left. A “hamburger” menu: this toggles the table of contents on the left side of the page on or off. A magnifying glass: this toggles a search box on or off. A letter “A”: this lets you pick how you want the site to display. You have your choice of small or large text, a serif or sans-serif font, and a white, sepia or night theme. A pencil: this is the “Edit” button. This will take you to a GitHub edit dialog for the chapter you’re reading. If you’re a committer to the repository, you’ll be able to edit the source directly. If not, GitHub will fork a copy of the repository to your own account and you’ll be able to edit that version. Then you can make a pull request. The share buttons in the upper right hand corner. There’s one for Twitter, one for Facebook, and one that gives a menu of options, including LinkedIn. References "],
["chapter-basic-concepts.html", "Chapter 2 Basic Concepts 2.1 The big picture: R and the Docker / PostgreSQL playground on your machine 2.2 Your computer and its operating system 2.3 R 2.4 Our sqlpetr package 2.5 Docker 2.6 ‘Normal’ and ‘normalized’ data 2.7 Enterprise DBMS", " Chapter 2 Basic Concepts This chapter introduces: The overall structure of our Docker-based PostgreSQL sandbox Basic concepts around each of the elements that make up our sandbox: tidy data, pipes, Docker, PostgreSQL, data representation, and our petsqlr package. 2.1 The big picture: R and the Docker / PostgreSQL playground on your machine Here is an overview of how R and Docker fit on your operating system in this book’s sandbox: R and Docker You run R from RStudio to set up Docker, launch PostgreSQL inside it and then send queries directly to PostgreSQL from R. (We provide more details about our sandbox environment in the chapter on mapping your environment. 2.2 Your computer and its operating system The playground that we construct in this book is designed so that some of the mysteries of accessing a corporate database are more visible – it’s all happening on your computer. The challenge, however, is that we know very little about your computer and its operating system. In the workshops we’ve given about this book, the details of individual computers have turned out to be diverse and difficult to pin down in advance. So there can be many issues, but not many basic concepts that we can highlight in advance. 2.3 R We assume a general familiarity with R and RStudio. RStudio’s Big Data workshop at the 2019 RStudio has an abundance of introductory material (Ruiz 2019). This book is Tidyverse-oriented, so we assume familiarity with the pipe operator, tidy data (Wickham 2014), dplyr, and techniques for tidying data (Wickham 2018). R connects to a database by means of a series of packages that work together. The following diagram from a big data workshop at the 2019 RStudio conference shows the big picture. The biggest difference in terms of retrieval strategies is between writing dplyr and native SQL code. Dplyr generates SQL-92 standard code; whereas you can write SQL code that leverages the specific language features of your DBMS when you write SQL code yourself. Rstudio’s DBMS architecture - slide # 33 2.4 Our sqlpetr package The sqlpetr package is the companion R package for this database tutorial. It has two classes of functions: Functions to install the dependencies needed to build the book and perform the operations covered in the tutorial, and Utilities for dealing with Docker and the PostgreSQL Docker image we use. sqlpetr has a pkgdown site at https://smithjd.github.io/sqlpetr/. 2.5 Docker Docker and the DevOps tools surrounding it have fostered a revolution in the way services are delivered over the internet. In this book, we’re piggybacking on a small piece of that revolution, Docker on the desktop. 2.5.1 Virtual machines and hypervisors A virtual machine is a machine that is running purely as software hosted by another real machine. To the user, a virtual machine looks just like a real one. But it has no processors, memory or I/O devices of its own - all of those are supplied and managed by the host. A virtual machine can run any operating system that will run on the host’s hardware. A Linux host can run a Windows virtual machine and vice versa. A hypervisor is the component of the host system software that manages virtual machines, usually called guests. Linux systems have a native hypervisor called Kernel Virtual Machine (kvm). And laptop, desktop and server processors from Intel and Advanced Micro Devices (AMD) have hardware that makes this hypervisor more efficient. Windows servers and Windows 10 Pro have a hypervisor called Hyper-V. Like kvm, Hyper-V can take advantage of the hardware in Intel and AMD processors. On Macintosh, there is a Hypervisor Framework (https://developer.apple.com/documentation/hypervisor) and other tools build on that. If this book is about Docker, why do we care about virtual machines and hypervisors? Docker is a Linux subsystem - it only runs on Linux laptops, desktops and servers. As we’ll see shortly, if we want to run Docker on Windows or MacOS, we’ll need a hypervisor, a Linux virtual machine and some “glue logic” to provide a Docker user experience equivalent to the one on a Linux system. 2.5.2 Containers A container is a set of processes running in an operating system. The host operating system is usually Linux, but other operating systems also can host containers. Unlike a virtual machine, the container has no operating system kernel of its own. If the host is running the Linux kernel, so is the container. And since the container OS is the same as the host OS, there’s no need for a hypervisor or hardware to support the hypervisor. So a container is more efficient than a virtual machine. A container does have its own file system. From inside the container, this file system looks like a Linux file system, but it can use any Linux distro. For example, you can have an Ubuntu 18.04 LTS host running Ubuntu 14.04 LTS or Fedora 28 or CentOS 7 containers. The kernel will always be the host kernel, but the utilities and applications will be those from the container. 2.5.3 Docker itself While there are both older (lxc) and newer container tools, the one that has caught on in terms of widespread use is Docker (Docker 2019a). Docker is widely used on cloud providers to deploy services of all kinds. Using Docker on the desktop to deliver standardized packages, as we are doing in this book, is a secondary use case, but a common one. If you’re using a Linux laptop / desktop, all you need to do is install Docker CE (Docker 2018a). However, most laptops and desktops don’t run Linux - they run Windows or MacOS. As noted above, to use Docker on Windows or MacOS, you need a hypervisor and a Linux virtual machine. 2.5.4 Docker objects The Docker subsystem manages several kinds of objects - containers, images, volumes and networks. In this book, we are only using the basic command line tools to manage containers, images and volumes. Docker images are files that define a container’s initial file system. You can find pre-built images on Docker Hub and the Docker Store - the base PostgreSQL image we use comes from Docker Hub (https://hub.docker.com/_/postgres/). If there isn’t a Docker image that does exactly what you want, you can build your own by creating a Dockerfile and running docker build. We do this in [Build the pet-sql Docker Image]. Docker volumes – explain mount. 2.5.5 Hosting Docker on Windows machines There are two ways to get Docker on Windows. For Windows 10 Home and older versions of Windows, you need Docker Toolbox (Docker 2019e). Note that for Docker Toolbox, you need a 64-bit AMD or Intel processor with the virtualization hardware installed and enabled in the BIOS. For Windows 10 Pro, you have the Hyper-V virtualizer as standard equipment, and can use Docker for Windows (Docker 2019c). 2.5.6 Hosting Docker on macOS machines As with Windows, there are two ways to get Docker. For older Intel systems, you’ll need Docker Toolbox (Docker 2019d). Newer systems (2010 or later running at least macOS El Capitan 10.11) can run Docker for Mac (Docker 2019b). 2.5.7 Hosting Docker on UNIX machines Unix was the original host for both R and Docker. Unix-like commands show up. 2.6 ‘Normal’ and ‘normalized’ data 2.6.1 Tidy data Tidy data (Wickham 2014) is well-behaved from the point of view of analysis and tools in the Tidyverse (RStudio 2019). Tidy data is easier to think about and it is usually worthwhile to make the data tidy (Wickham 2018). Tidy data is roughly equivalent to third normal form as discussed below. 2.6.2 Design of “normal data” Data in a database is most often optimized to minimize storage space and increase performance while preserving integrity when adding, changing, or deleting data. The Wikipedia article on Database Normalization has a good introduction to the characteristics of “normal” data and the process of re-organizing it to meet those desirable criteria (Wikipedia 2019). The bottom line is that “data normalization is practical” although there are mathematical arguments for normalization based on the preservation of data integrity. 2.7 Enterprise DBMS The organizational context of a database matters just as much as its design characteristics. The design of a database (or data model) may have been purchased from an external vendor or developed in-house. In either case time has a tendency to erode the original design concept so that the data you find in a DBMS may not quite match the original design specification. And the original design may or may not be well reflected in the current naming of tables, columns and other objects. It’s a naive misconception to think that the data you are analyzing just “comes from the database”, although that’s literally true and may be the step that happens before you get your hands on it. In fact it comes from the people who design, enter, manage, protect, and use your organization’s data. In practice, a database administrator (DBA) is often a key point of contact in terms of access and may have stringent criteria for query performance. Make friends with your DBA. 2.7.1 SQL databases Although there are ANSI standards for SQL syntax, different implementations vary in enough details that R’s ability to customize queries for those implementations is very helpful. The tables in a DBMS correspond to a data frame in R, so interaction with a DBMS is fairly natural for useRs. SQL code is characterized by the fact that it describes what to retrieve, leaving the DBMS back end to determine how to do it. Therefore it has a batch feel. The pipe operator (%&gt;%, which is read as and then) is inherently procedural when it’s used with dplyr: it can be used to construct queries step-by-step. Once a test dplyr query has been executed, it is easy to inspect the results and add steps with the pipe operator to refine or expand the query. APPENDIX D - Quick Guide to SQL lists the different elements of the SQL language. 2.7.2 Data mapping between R vs SQL data types The following code shows how different elements of the R bestiary are translated to and from ANSI standard data types. Note that R factors are translated as TEXT so that missing levels are ignored on the SQL side. library(DBI) dbDataType(ANSI(), 1:5) ## [1] &quot;INT&quot; dbDataType(ANSI(), 1) ## [1] &quot;DOUBLE&quot; dbDataType(ANSI(), TRUE) ## [1] &quot;SMALLINT&quot; dbDataType(ANSI(), Sys.Date()) ## [1] &quot;DATE&quot; dbDataType(ANSI(), Sys.time()) ## [1] &quot;TIMESTAMP&quot; dbDataType(ANSI(), Sys.time() - as.POSIXct(Sys.Date())) ## [1] &quot;TIME&quot; dbDataType(ANSI(), c(&quot;x&quot;, &quot;abc&quot;)) ## [1] &quot;TEXT&quot; dbDataType(ANSI(), list(raw(10), raw(20))) ## [1] &quot;BLOB&quot; dbDataType(ANSI(), I(3)) ## [1] &quot;DOUBLE&quot; dbDataType(ANSI(), iris) ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## &quot;DOUBLE&quot; &quot;DOUBLE&quot; &quot;DOUBLE&quot; &quot;DOUBLE&quot; &quot;TEXT&quot; The DBI specification provides extensive documentation that is worth digesting if you intend to work with a DBMS from R. As you work through the examples in this book, you will also want to refer to the following resources: RStudio’s Databases using R site describes many of the technical details involved. The RStudio community is an excellent place to ask questions or study what has been discussed previously. 2.7.3 PostgreSQL and connection parameters An important detail: We use a PostgreSQL database server running in a Docker container for the database functions. It is installed inside Docker, so you do not have to download or install it yourself. To connect to it, you have to define some parameters. These parameters are used in two places: When the Docker container is created, they’re used to initialize the database, and Whenever we connect to the database, we need to specify them to authenticate. We define the parameters in an environment file that R reads when starting up. The file is called .Renviron, and is located in your home directory. See the discussion of securing and using dbms credentials. 2.7.4 Connecting the R and DBMS environments Although everything happens on one machine in our Docker / PostgreSQL playground, in real life R and PostgreSQL (or other DBMS) will be in different environments on separate machines. How R connects them gives you control over where the work happens. You need to be aware of the differences beween the R and DBMS environments as well as how you can leverage the strengths of each one. Characteristics of local vs. server processing Dimension Local Remote Design purpose The R environment on your local machine is designed to be flexible and easy to use; ideal for data investigation. The DBMS environment is designed for large and complex databases where data integrity is more important than flexibility or ease of use. Processor power Your local machine has less memory, speed, and storage than the typical database server. Database servers are specialized, more expensive, and have more power. Memory constraint In R, query results must fit into memory. Servers have a lot of memory and write intermediate results to disk if needed without you knowing about it. Data crunching Data lives in the DBMS, so crunching it down locally requires you to pull it over the network. A DBMS has powerful data crunching capabilities once you know what you want and moves data over the server backbone to crunch it. Security Local control. Whether it is good or not depends on you. Responsibility of database administrators who set the rules. You play by their rules. Storage of intermediate results Very easy to save a data frame with intermediate results locally. May require extra privileges to save results in the database. Analytical resources Ecosystem of available R packages Extending SQL instruction set involves dbms-specific functions or R pseudo functions Collaboration One person working on a few data.frames. Many people collaborating on many tables. 2.7.5 Using SQLite to simulate an enterprise DBMS SQLite engine is embedded in one file, so that many tables are stored together in one object. SQL commands can run against an SQLite database as demonstrated in how many uses of SQLite are in the RStudio dbplyr documentation. References "],
["chapter-how-to-use-this-book.html", "Chapter 3 How to use this book 3.1 Retrieve the code from GitHub 3.2 Read along, experiment as you go 3.3 Participating", " Chapter 3 How to use this book This chapter explains: Getting the code used in this book How you can contribute to the book project This book is full of examples that you can replicate on your computer. 3.1 Retrieve the code from GitHub The code to generate the book and the exercises it contains can be downloaded from this repo. 3.2 Read along, experiment as you go We have never been sure whether we’re writing an expository book or a massive tutorial. You may use it either way. The best way to learn the material we cover is to experiment. After the introductory chapters and the chapter that creates the persistent database, you can jump around and each chapter stands on its own. 3.3 Participating 3.3.1 Browsing the book If you just want to read the book and copy / paste code into your working environment, simply browse to https://smithjd.github.io/sql-pet. If you get stuck, or find things aren’t working, open an issue at https://github.com/smithjd/sql-pet/issues/new/. 3.3.2 Diving in If you want to experiment with the code in the book, run it in RStudio and interact with it, you’ll need to do two more things: Install the sqlpetr R package (Borasky et al. 2018). See https://smithjd.github.io/sqlpetr for the package documentation. Installation may take some time if it has to install or update packages not available on your computer. Clone the Git repository https://github.com/smithjd/sql-pet.git and open the project file sql-pet.Rproj in RStudio. Enjoy! References "],
["chapter-learning-goals.html", "Chapter 4 Learning Goals and Use Cases 4.1 Challenge: goals, context and expectations 4.2 The Challenge: Investigating a question using an organization’s database 4.3 Ask yourself, what are you aiming for? 4.4 AdventureWorks", " Chapter 4 Learning Goals and Use Cases This chapter sets the context for the book by: Describing our assumptions about your goals, context, and expectations Describing what the book offers in terms of: Problems that are addressed Learning objectives R packages used Describing the sample database used in the book Posing some imaginary but realistic use cases that frame the exercises and discussions An R analyst…. 4.1 Challenge: goals, context and expectations 4.2 The Challenge: Investigating a question using an organization’s database Need both familiarity with the data and a focus question An iterative process where the data resource can shape your understanding of the question the question you need to answer will frame how you see the data resource You need to go back and forth between the two, asking do I understand the question? do I understand the data? How well do you understand the data resource (in the DBMS)? Use all available documentation and understand its limits Use your own tools and skills to examine the data resource What is missing from the database: (columns, records, cells) Why is the data missing? How well do you understand the question you seek to answer? How general or specific is your question? How aligned is it with the purpose for which the database was designed and is being operated? How different are your assumptions and concerns from those of the people who enter and use the data on a day to day basis? 4.3 Ask yourself, what are you aiming for? Differences between production and data warehouse environments. Learning to keep your DBAs happy: You are your own DBA in this simulation, so you can wreak havoc and learn from it, but you can learn to be DBA-friendly here. In the end it’s the subject-matter experts that understand your data, but you have to work with your DBAs first. 4.3.1 Problems that are addressed Database exploration Wisdom from Sophie 4.3.2 Learning Objectives After working through the code in this book, you can expect to be able to: R, SQL and PostgreSQL Run queries against PostgreSQL in an environment that simulates what is found in a corporate setting. Understand techniques and some of the trade-offs between: queries aimed at exploration or informal investigation using dplyr (Wickham 2018); and queries that should be written in SQL, because performance is important due to the size of the database or the frequency with which a query is to be run. Understand the equivalence between dplyr and SQL queries, and how R translates one into the other. Gain familiarity with techniques that help you explore a database and verify its documentation. Gain familiarity with the standard metadata that a SQL database contains to describe its own contents. Understand some advanced SQL techniques. Gain some understanding of techniques for assessing query structure and performance. Docker related Set up a PostgreSQL database in a Docker environment. Gain familiarity with the various ways of interacting with the Docker and PostgreSQL environments Understand enough about Docker to swap databases, e.g. Sports DB for the DVD rental database used in this tutorial. Or swap the database management system (DBMS), e.g. MySQL for PostgreSQL. 4.3.3 R Packages These R packages are discussed or used in exercises: DBI dbplyr devtools downloader glue gt here knitr RPostgres skimr sqlpetr (installs with: remotes::install_github(&quot;smithjd/sqlpetr&quot;, force = TRUE, quiet = TRUE, build = TRUE, build_opts = &quot;&quot;)) tidyverse In addition, these are used to render the book: * bookdown * DiagrammeR 4.4 AdventureWorks In this book we have adopted the Microsoft AdventureWorks online transaction processing database for our examples. It is https://docs.microsoft.com/en-us/previous-versions/sql/sql-server-2008/ms124438(v=sql.100) See Sections 3 and 4 Journal of Information Systems Education, Vol. 26(3) Summer 2015. “Teaching Tip Active Learning via a Sample Database: The Case of Microsoft’s Adventure Works” by Michel Mitri http://jise.org/Volume26/n3/JISEv26n3p177.pdf See the AdventureWorks Data Dictionary and a sample table (employee). Here is a (link to an ERD diagram)[https://i.stack.imgur.com/LMu4W.gif] References "],
["chapter-connect-docker-postgresql-r.html", "Chapter 5 Connecting Docker, PostgreSQL, and R 5.1 Verify that Docker is running 5.2 Remove previous containers if they exist 5.3 Connecting, reading and writing to PostgreSQL from R 5.4 Clean up", " Chapter 5 Connecting Docker, PostgreSQL, and R This chapter demonstrates how to: Run, clean-up and close PostgreSQL in Docker containers. Keep necessary credentials secret while being available to R when it executes. Interact with PostgreSQL when it’s running inside a Docker container. Read and write to PostgreSQL from R. Please install the sqlpetr package if not already installed: library(devtools) if (!require(sqlpetr)) { remotes::install_github( &quot;smithjd/sqlpetr&quot;, force = TRUE, build = FALSE, quiet = TRUE) } Note that when you install the package the first time, it will ask you to update the packages it uses and that can take some time. The following packages are used in this chapter: library(tidyverse) library(DBI) library(RPostgres) require(knitr) library(sqlpetr) 5.1 Verify that Docker is running Docker commands can be run from a terminal (e.g., the Rstudio Terminal pane) or with a system2() command. (We discuss the diffeent ways of interacting with Docker and other elements in your environment in a separate chapter.) The necessary functions to start, stop Docker containers and do other busy work are provided in the sqlpetr package. As time permits and curiosity dictates, feel free to look at those functions to see how they work. 5.1.1 Check that Docker is up and running Note: The sqlpetr package is written to accompany this book. The functions in the package are designed to help you focus on interacting with a dbms from R. You can ignore how they work until you are ready to delve into the details. They are all named to begin with sp_. The first time a function is called in the book, we provide a note explaining its use. The sp_check_that_docker_is_up function from the sqlpetr package checks whether Docker is up and running. If it’s not, then you need to install, launch or re-install Docker. sp_check_that_docker_is_up() ## [1] &quot;Docker is up, running these containers:&quot; ## [2] &quot;CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES&quot; ## [3] &quot;0d58355689d5 postgres:10 \\&quot;docker-entrypoint.s…\\&quot; About a minute ago Up About a minute 5432/tcp, 0.0.0.0:5439-&gt;5439/tcp cattle&quot; 5.2 Remove previous containers if they exist Force remove the cattle and sql-pet containers if they exist (e.g., from prior experiments). The sp_docker_remove_container function from the sqlpetr package forcibly removes a Docker container. If it is running it will be forcibly terminated and removed. If it doesn’t exist you won’t get an error message. Note that the images out of which a container is built will still exist on your system. sp_docker_remove_container(&quot;cattle&quot;) ## [1] 0 We name containers cattle for “throw-aways” and pet for ones we treasure and keep around. :-) The sp_docker_remove_container function from the sqlpetr package creates a container and runs the PostgreSQL 10 image (docker.io/postgres:10) in it. The image will be downloaded if it doesn’t exist locally. sp_make_simple_pg(&quot;cattle&quot;) The first time you run this, Docker downloads the PostgreSQL image, which takes a bit of time. Did it work? The following command should show that a container named cattle is running postgres:10. sp_check_that_docker_is_up() ## [1] &quot;Docker is up, running these containers:&quot; ## [2] &quot;CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES&quot; ## [3] &quot;738f564ff025 postgres:10 \\&quot;docker-entrypoint.s…\\&quot; 4 seconds ago Up 2 seconds 5432/tcp, 0.0.0.0:5439-&gt;5439/tcp cattle&quot; The sp_docker_containers_tibble function from the sqlpetr package provides more on the containers that Docker is running. Basically this function creates a tibble of containers using docker ps. sp_docker_containers_tibble() ## # A tibble: 1 x 12 ## container_id image command created_at created ports status size names ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 738f564ff025 post… docker… 2019-07-1… 4 seco… 5432… Up 2 … 63B … catt… ## # … with 3 more variables: labels &lt;chr&gt;, mounts &lt;chr&gt;, networks &lt;chr&gt; 5.3 Connecting, reading and writing to PostgreSQL from R 5.3.1 Connecting to PostgreSQL The sp_make_simple_pg function we called above created a container from the postgres:10 library image downloaded from Docker Hub. As part of the process, it set the password for the PostgreSQL database superuser postgres to the value “postgres”. For simplicity, we are using a weak password at this point and it’s shown here and in the code in plain text. That is bad practice because user credentials should not be shared in open code like that. A subsequent chapter demonstrates how to store and use credentials to access the DBMS so that they are kept private. The sp_get_postgres_connection function from the sqlpetr package gets a DBI connection string to a PostgreSQL database, waiting if it is not ready. This function connects to an instance of PostgreSQL and we assign it to a symbol, con, for subsequent use. The connctions_tab = TRUE parameter opens a connections tab that’s useful for navigating a database. Note that we are using port 5439 for PostgreSQL inside the container and published to localhost. Why? If you have PostgreSQL already running on the host or another container, it probably claimed port 5432, since that’s the default. So we need to use a different port for our PostgreSQL container. con &lt;- sp_get_postgres_connection( host = &quot;localhost&quot;, port = 5439, user = &quot;postgres&quot;, password = &quot;postgres&quot;, dbname = &quot;postgres&quot;, seconds_to_test = 30, connection_tab = TRUE ) If you have been executing the code from this tutorial, the database will not contain any tables yet, but you will be connected to the database: DBI::dbListTables(con) ## character(0) The Connections tab shows that you are connected but that the database has no tables in it: Connections tab - no tables 5.3.2 Interact with PostgreSQL Write mtcars to PostgreSQL DBI::dbWriteTable(con, &quot;mtcars&quot;, mtcars, overwrite = TRUE) List the tables in the PostgreSQL database to show that mtcars is now there: DBI::dbListTables(con) ## [1] &quot;mtcars&quot; The Connections tab has not been updated, so it still shows no tables. When the code to connect to the database is executed again, the connections tab is updated. con &lt;- sp_get_postgres_connection( host = &quot;localhost&quot;, port = 5439, user = &quot;postgres&quot;, password = &quot;postgres&quot;, dbname = &quot;postgres&quot;, seconds_to_test = 30, connection_tab = TRUE ) The Connections tab now shows: Clicking on the triangle on the left next to mtcars will list the table’s fields. That’s equivalent to listing the fields with: DBI::dbListFields(con, &quot;mtcars&quot;) ## [1] &quot;mpg&quot; &quot;cyl&quot; &quot;disp&quot; &quot;hp&quot; &quot;drat&quot; &quot;wt&quot; &quot;qsec&quot; &quot;vs&quot; &quot;am&quot; &quot;gear&quot; ## [11] &quot;carb&quot; Here is the same information on the Connections tab: Clicking on the rectangle on the right of mtcars opens a View tab on the first 1,000 rows of a table. That’s equivalent to excuting this code to download the table from the DBMS to a local data frame: mtcars_df &lt;- DBI::dbReadTable(con, &quot;mtcars&quot;) The sp_print_df function from the sqlpetr package shows (or print) a data frame depending on appropriate output type. That is when running interactively or generating HTML it prints a DT::datatable() while it prints a knitr::kable() otherwise. sp_print_df(head(mtcars_df)) Interactively, you can also click on the mtcars table in the Connections tab to see: The number of rows and columns shown in the View pane depends on the size of the window. 5.4 Clean up Afterwards, always disconnect from the dbms: DBI::dbDisconnect(con) The Connection tab equivalent is to click on the connection icon next to Help on the Connctions tab. The sp_docker_stop function from the sqlpetr package stops the container given by the container_name parameter. Tell Docker to stop the cattle container: sp_docker_stop(&quot;cattle&quot;) The sp_docker_remove_container function from the sqlpetr package removes the container given by the container_name parameter. Tell Docker to remove the cattle container from it’s library of active containers: sp_docker_remove_container(&quot;cattle&quot;) ## [1] 0 Verify that cattle is gone: sp_docker_containers_tibble() ## # A tibble: 0 x 0 If we just stop the Docker container but don’t remove it (as we did with the sp_docker_remove_container(&quot;cattle&quot;) command), the cattle container will persist and we can start it up again later with sp_docker_start(&quot;cattle&quot;). In that case, mtcars would still be there and we could retrieve it from PostgreSQL again. Since sp_docker_remove_container(&quot;cattle&quot;) has removed it, the updated database has been deleted. (There are enough copies of mtcars in the world, so no great loss.) "],
["chapter-setup-adventureworks-db.html", "Chapter 6 Create the adventureworks database in PostgreSQL in Docker 6.1 Overview 6.2 Verify that Docker is up and running 6.3 Clean up if appropriate 6.4 Build the adventureworks Docker image 6.5 Run the adventureworks Docker Image 6.6 Connect to PostgreSQL with R 6.7 Stop and start to demonstrate persistence 6.8 Cleaning up 6.9 Using the adventureworks container in the rest of the book", " Chapter 6 Create the adventureworks database in PostgreSQL in Docker NOTE: This chapter doesn’t go into the details of creating or restoring the adventureworks database. For more detail on what’s going on behind the scenes, you can examine the step-by-step code in: source('book-src/restore-adventureworks-postgres-on-docker.R') This chapter demonstrates how to: Setup the adventureworks database in Docker Stop and start Docker container to demonstrate persistence Connect to and disconnect R from the adventureworks database Set up the environment for subsequent chapters 6.1 Overview In the last chapter we connected to PostgreSQL from R. Now we set up a “realistic” database named adventureworks. There are different approaches to doing this: this chapter sets it up in a way that doesn’t show all the Docker details. These packages are called in this Chapter: library(tidyverse) library(DBI) library(RPostgres) library(glue) require(knitr) library(dbplyr) library(sqlpetr) library(bookdown) library(here) 6.2 Verify that Docker is up and running sp_check_that_docker_is_up() ## [1] &quot;Docker is up but running no containers&quot; 6.3 Clean up if appropriate Force-remove the adventureworks container if it exist (e.g., from a prior runs): sp_docker_remove_container(&quot;adventureworks&quot;) ## [1] 0 6.4 Build the adventureworks Docker image UPDATE: For the rest of the book we will be using a Docker image called adventureworks. To save space here in the book, we’ve created a function in sqlpetr to build this image, called sp_make_dvdrental_image. Vignette Building the hsrample Docker Image describes the build process. # sp_make_dvdrental_image(&quot;postgres-dvdrental&quot;) source(here(&quot;book-src&quot;, &quot;restore-adventureworks-postgres-on-docker.R&quot;)) ## docker run --detach --name adventureworks --publish 5432:5432 --mount type=bind,source=&quot;/Users/jds/Documents/Library/R/r-system/sql-pet&quot;,target=/petdir postgres:10 UPDATE: Did it work? We have a function that lists the images into a tibble! sp_docker_start(&quot;adventureworks&quot;) sp_docker_images_tibble() # Doesn&#39;t produce the expected output. ## # A tibble: 6 x 7 ## image_id repository tag digest created created_at size ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 1523f751… adventurewo… latest &lt;none&gt; 3 week… 2019-06-19 … 475MB ## 2 602a8e50… &lt;none&gt; &lt;none&gt; &lt;none&gt; 3 week… 2019-06-19 … 365MB ## 3 4e045cb8… postgres latest sha256:1518027f… 4 week… 2019-06-10 … 312MB ## 4 aff06852… postgres-dv… latest &lt;none&gt; 2 mont… 2019-04-26 … 294MB ## 5 c149455a… &lt;none&gt; &lt;none&gt; &lt;none&gt; 3 mont… 2019-03-18 … 252MB ## 6 3e016ba4… postgres 10 sha256:5c702997… 4 mont… 2019-03-04 … 230MB 6.5 Run the adventureworks Docker Image UPDATE: Now we can run the image in a container and connect to the database. To run the image we use an sqlpetr function called sp_pg_docker_run # sp_pg_docker_run( # container_name = &quot;adventureworks&quot;, # image_tag = &quot;adventureworks&quot;, # postgres_password = &quot;postgres&quot; # ) UPDATE: Did it work? sp_docker_containers_tibble() ## # A tibble: 1 x 12 ## container_id image command created_at created ports status size names ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 7f011ef99a46 post… docker… 2019-07-1… 15 sec… 0.0.… Up 13… 63B … adve… ## # … with 3 more variables: labels &lt;chr&gt;, mounts &lt;chr&gt;, networks &lt;chr&gt; 6.6 Connect to PostgreSQL with R Use the DBI package to connect to the adventureworks database in PostgreSQL. Remember the settings discussion about [keeping passwords hidden][Pause for some security considerations] con &lt;- sp_get_postgres_connection( host = &quot;localhost&quot;, port = 5432, user = &quot;postgres&quot;, password = &quot;postgres&quot;, dbname = &quot;adventureworks&quot;, seconds_to_test = 20, connection_tab = TRUE ) For the moment we by-pass some complexity that results from the fact that the adventureworks has multiple schemas and that we are interested in only one of them, named adventureworks. tbl(con, in_schema(&quot;information_schema&quot;, &quot;schemata&quot;)) %&gt;% select(catalog_name, schema_name, schema_owner) %&gt;% collect() ## Warning: `overscope_eval_next()` is deprecated as of rlang 0.2.0. ## Please use `eval_tidy()` with a data mask instead. ## This warning is displayed once per session. ## Warning: `overscope_clean()` is deprecated as of rlang 0.2.0. ## This warning is displayed once per session. ## # A tibble: 16 x 3 ## catalog_name schema_name schema_owner ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 adventureworks pg_toast postgres ## 2 adventureworks pg_temp_1 postgres ## 3 adventureworks pg_toast_temp_1 postgres ## 4 adventureworks pg_catalog postgres ## 5 adventureworks public postgres ## 6 adventureworks information_schema postgres ## 7 adventureworks hr postgres ## 8 adventureworks humanresources postgres ## 9 adventureworks pe postgres ## 10 adventureworks person postgres ## 11 adventureworks pr postgres ## 12 adventureworks production postgres ## 13 adventureworks pu postgres ## 14 adventureworks purchasing postgres ## 15 adventureworks sa postgres ## 16 adventureworks sales postgres Schemas will be discussed later on because multiple schemas are the norm in an enterprise database environment, but they are a side issue at this point. So we switch the order in which PostgreSQL searches for objects with the following SQL code: dbExecute(con, &quot;set search_path to humanresources, public;&quot;) ## [1] 0 With the custom search_path, the following command works, but it will fail without out it. dbListTables(con) ## [1] &quot;shift&quot; &quot;employee&quot; ## [3] &quot;jobcandidate&quot; &quot;vemployee&quot; ## [5] &quot;vemployeedepartment&quot; &quot;vemployeedepartmenthistory&quot; ## [7] &quot;vjobcandidate&quot; &quot;vjobcandidateeducation&quot; ## [9] &quot;vjobcandidateemployment&quot; &quot;department&quot; ## [11] &quot;employeedepartmenthistory&quot; &quot;employeepayhistory&quot; Same for dbListFields: dbListFields(con, &quot;employee&quot;) ## [1] &quot;businessentityid&quot; &quot;nationalidnumber&quot; &quot;loginid&quot; ## [4] &quot;jobtitle&quot; &quot;birthdate&quot; &quot;maritalstatus&quot; ## [7] &quot;gender&quot; &quot;hiredate&quot; &quot;salariedflag&quot; ## [10] &quot;vacationhours&quot; &quot;sickleavehours&quot; &quot;currentflag&quot; ## [13] &quot;rowguid&quot; &quot;modifieddate&quot; &quot;organizationnode&quot; Thus with this search order, the following two produce identical results: tbl(con, in_schema(&quot;humanresources&quot;, &quot;employee&quot;)) %&gt;% head() ## # Source: lazy query [?? x 15] ## # Database: postgres [postgres@localhost:5432/adventureworks] ## businessentityid nationalidnumber loginid jobtitle birthdate ## &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;date&gt; ## 1 1 295847284 &quot;adven… Chief E… 1969-01-29 ## 2 2 245797967 &quot;adven… Vice Pr… 1971-08-01 ## 3 3 509647174 &quot;adven… Enginee… 1974-11-12 ## 4 4 112457891 &quot;adven… Senior … 1974-12-23 ## 5 5 695256908 &quot;adven… Design … 1952-09-27 ## 6 6 998320692 &quot;adven… Design … 1959-03-11 ## # … with 10 more variables: maritalstatus &lt;chr&gt;, gender &lt;chr&gt;, ## # hiredate &lt;date&gt;, salariedflag &lt;lgl&gt;, vacationhours &lt;int&gt;, ## # sickleavehours &lt;int&gt;, currentflag &lt;lgl&gt;, rowguid &lt;chr&gt;, ## # modifieddate &lt;dttm&gt;, organizationnode &lt;chr&gt; tbl(con, &quot;employee&quot;) %&gt;% head() ## # Source: lazy query [?? x 15] ## # Database: postgres [postgres@localhost:5432/adventureworks] ## businessentityid nationalidnumber loginid jobtitle birthdate ## &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;date&gt; ## 1 1 295847284 &quot;adven… Chief E… 1969-01-29 ## 2 2 245797967 &quot;adven… Vice Pr… 1971-08-01 ## 3 3 509647174 &quot;adven… Enginee… 1974-11-12 ## 4 4 112457891 &quot;adven… Senior … 1974-12-23 ## 5 5 695256908 &quot;adven… Design … 1952-09-27 ## 6 6 998320692 &quot;adven… Design … 1959-03-11 ## # … with 10 more variables: maritalstatus &lt;chr&gt;, gender &lt;chr&gt;, ## # hiredate &lt;date&gt;, salariedflag &lt;lgl&gt;, vacationhours &lt;int&gt;, ## # sickleavehours &lt;int&gt;, currentflag &lt;lgl&gt;, rowguid &lt;chr&gt;, ## # modifieddate &lt;dttm&gt;, organizationnode &lt;chr&gt; Disconnect from the database: dbDisconnect(con) 6.7 Stop and start to demonstrate persistence Stop the container: sp_docker_stop(&quot;adventureworks&quot;) sp_docker_containers_tibble() ## # A tibble: 0 x 0 When we stopped adventureworks, it no longer appeared in the tibble. But the container is still there. sp_docker_containers_tibble by default only lists the running containers. But we can use the list_all option and see it: sp_docker_containers_tibble(list_all = TRUE) ## # A tibble: 1 x 12 ## container_id image command created_at created ports status size names ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 7f011ef99a46 post… docker… 2019-07-1… 16 sec… &lt;NA&gt; Exite… 0B (… adve… ## # … with 3 more variables: labels &lt;chr&gt;, mounts &lt;chr&gt;, networks &lt;chr&gt; Restart the container and verify that the adventureworks tables are still there: sp_docker_start(&quot;adventureworks&quot;) sp_docker_containers_tibble() ## # A tibble: 1 x 12 ## container_id image command created_at created ports status size names ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 7f011ef99a46 post… docker… 2019-07-1… 17 sec… 0.0.… Up Le… 63B … adve… ## # … with 3 more variables: labels &lt;chr&gt;, mounts &lt;chr&gt;, networks &lt;chr&gt; Connect to the adventureworks database in PostgreSQL: con &lt;- sp_get_postgres_connection( host = &quot;localhost&quot;, port = 5432, user = &quot;postgres&quot;, password = &quot;postgres&quot;, dbname = &quot;adventureworks&quot;, seconds_to_test = 30 ) Check that you can still see the first few rows of the employeeinfo table: tbl(con, in_schema(&quot;humanresources&quot;, &quot;employee&quot;)) %&gt;% head() ## # Source: lazy query [?? x 15] ## # Database: postgres [postgres@localhost:5432/adventureworks] ## businessentityid nationalidnumber loginid jobtitle birthdate ## &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;date&gt; ## 1 1 295847284 &quot;adven… Chief E… 1969-01-29 ## 2 2 245797967 &quot;adven… Vice Pr… 1971-08-01 ## 3 3 509647174 &quot;adven… Enginee… 1974-11-12 ## 4 4 112457891 &quot;adven… Senior … 1974-12-23 ## 5 5 695256908 &quot;adven… Design … 1952-09-27 ## 6 6 998320692 &quot;adven… Design … 1959-03-11 ## # … with 10 more variables: maritalstatus &lt;chr&gt;, gender &lt;chr&gt;, ## # hiredate &lt;date&gt;, salariedflag &lt;lgl&gt;, vacationhours &lt;int&gt;, ## # sickleavehours &lt;int&gt;, currentflag &lt;lgl&gt;, rowguid &lt;chr&gt;, ## # modifieddate &lt;dttm&gt;, organizationnode &lt;chr&gt; 6.8 Cleaning up Always have R disconnect from the database when you’re done. dbDisconnect(con) Stop the adventureworks container: sp_docker_stop(&quot;adventureworks&quot;) Show that the container still exists even though it’s not running sp_show_all_docker_containers() ## CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES ## 7f011ef99a46 postgres:10 &quot;docker-entrypoint.s…&quot; 18 seconds ago Exited (0) Less than a second ago adventureworks Next time, you can just use this command to start the container: sp_docker_start(&quot;adventureworks&quot;) And once stopped, the container can be removed with: sp_check_that_docker_is_up(&quot;adventureworks&quot;) 6.9 Using the adventureworks container in the rest of the book After this point in the book, we assume that Docker is up and that we can always start up our adventureworks database with: sp_docker_start(&quot;adventureworks&quot;) "],
["chapter-dbms-login-credentials.html", "Chapter 7 Securing and using your dbms log-in credentials 7.1 Set up the adventureworks Docker container 7.2 Storing your dbms credentials 7.3 Clean up", " Chapter 7 Securing and using your dbms log-in credentials This chapter demonstrates how to: Keep necessary credentials secret or at least invisible Interact with PostgreSQL using your stored dbms credentials Connecting to a dbms can be very frustrating at first. In many organizations, simply getting access credentials takes time and may involve jumping through multiple hoops. In addition, a dbms is terse or deliberately inscrutable when your credetials are incorrect. That’s a security strategy, not a limitation of your understanding or of your software. When R can’t log you on to a dbms, you usually will have no information as to what went wrong. There are many different strategies for managing credentials. See Securing Credentials in RStudio’s Databases using R documentation for some alternatives to the method we adopt in this book. We provide more details about PostgreSQL Authentication in our sandbox environment in an appendix. The following packages are used in this chapter: library(tidyverse) library(DBI) library(RPostgres) require(knitr) library(sqlpetr) 7.1 Set up the adventureworks Docker container 7.1.1 Verify that Docker is running Check that Docker is up and running: sp_check_that_docker_is_up() ## [1] &quot;Docker is up but running no containers&quot; 7.1.2 Start the Docker container: Start the adventureworks Docker container: sp_docker_start(&quot;adventureworks&quot;) 7.2 Storing your dbms credentials In previous chapters the connection string for connecting to the dbms has used default credentials specified in plain text as follows: user= 'postgres', password = 'postgres' When we call sp_get_postgres_connection below we’ll use environment variables that R obtains from reading the .Renviron file when R starts up. This approach has two benefits: that file is not uploaded to GitHub and R looks for it in your default directory every time it loads. To see whether you have already created that file, use the R Studio Files tab to look at your home directory: That file should contain lines that look like the example below. Although in this example it contains the PostgreSQL default values for the username and password, they are obviously not secret. But this approach demonstrates where you should put secrets that R needs while not risking accidental uploaded to GitHub or some other public location.. Open your .Renviron file with this command: file.edit(&quot;~/.Renviron&quot;) Or you can execute define_postgresql_params.R to create the file or you could copy / paste the following into your .Renviron file: DEFAULT_POSTGRES_PASSWORD=postgres DEFAULT_POSTGRES_USER_NAME=postgres Once that file is created, restart R, and after that R reads it every time it comes up. 7.2.1 Connect with Postgres using the Sys.getenv function Connect to the postgrSQL using the sp_get_postgres_connection function: con &lt;- sp_get_postgres_connection(user = Sys.getenv(&quot;DEFAULT_POSTGRES_USER_NAME&quot;), password = Sys.getenv(&quot;DEFAULT_POSTGRES_PASSWORD&quot;), port = 5432, dbname = &quot;adventureworks&quot;, seconds_to_test = 30, connection_tab = TRUE) Once the connection object has been created, you can list all of the tables in one of the schemas: dbExecute(con, &quot;set search_path to humanresources, public;&quot;) # watch for duplicates! ## [1] 0 dbListTables(con) ## [1] &quot;shift&quot; &quot;employee&quot; ## [3] &quot;jobcandidate&quot; &quot;vemployee&quot; ## [5] &quot;vemployeedepartment&quot; &quot;vemployeedepartmenthistory&quot; ## [7] &quot;vjobcandidate&quot; &quot;vjobcandidateeducation&quot; ## [9] &quot;vjobcandidateemployment&quot; &quot;department&quot; ## [11] &quot;employeedepartmenthistory&quot; &quot;employeepayhistory&quot; 7.3 Clean up Afterwards, always disconnect from the dbms: dbDisconnect(con) Tell Docker to stop the adventureworks container: sp_docker_stop(&quot;adventureworks&quot;) "],
["chapter-dbms-queries-intro.html", "Chapter 8 Introduction to DBMS queries 8.1 Setup 8.2 Getting data from the database 8.3 Mixing dplyr and SQL 8.4 Examining a single table with R 8.5 Additional reading", " Chapter 8 Introduction to DBMS queries This chapter demonstrates how to: Get a glimpse of what tables are in the database and what fields a table contains Download all or part of a table from the dbms See how dplyr code is translated into SQL commands Get acquainted with some useful tools for investigating a single table Begin thinking about how to divide the work between your local R session and the dbms 8.1 Setup The following packages are used in this chapter: library(tidyverse) library(DBI) library(RPostgres) library(dbplyr) require(knitr) library(bookdown) library(sqlpetr) Assume that the Docker container with PostgreSQL and the adventureworks database are ready to go. If not go back to [Chapter 6][#chapter_setup-adventureworks-db] sqlpetr::sp_docker_start(&quot;adventureworks&quot;) Connect to the database: con &lt;- sqlpetr::sp_get_postgres_connection( user = Sys.getenv(&quot;DEFAULT_POSTGRES_USER_NAME&quot;), password = Sys.getenv(&quot;DEFAULT_POSTGRES_PASSWORD&quot;), dbname = &quot;adventureworks&quot;, port = 5432, seconds_to_test = 20, connection_tab = TRUE ) 8.2 Getting data from the database As we show later on, the database serves as a store of data and as an engine for sub-setting, joining, and computation on the data. We begin with getting data from the dbms, or “downloading” data. 8.2.1 Finding out what’s there We’ve already seen how the connections tab is the easiest way to explore a database. Adventureworks connections tab It’s a little more complex than the cattle example because we have additional structure: the adventureworks database contains schemas, which contain tables. The hr schema is the same as the humanresources schema, but with nicknames (the d table in hr is the same as the department table in the humanresources schema). Schemas are used to control access and to set up shortcuts. Clicking on the the right opens up the table view of the d table: Adventureworks connections tab Exploring a databse using R code is a littlel more complicated. The following command does not give you a list of tables as it did in the simpler case when there were no schemas other than the public schema: tables &lt;- DBI::dbListTables(con) tables ## character(0) We need to to tell the database which schemas to search dbExecute(con, &quot;set search_path to hr, humanresources;&quot;) ## [1] 0 DBI::dbListTables(con) ## [1] &quot;d&quot; &quot;shift&quot; ## [3] &quot;e&quot; &quot;employee&quot; ## [5] &quot;edh&quot; &quot;eph&quot; ## [7] &quot;jc&quot; &quot;s&quot; ## [9] &quot;jobcandidate&quot; &quot;vemployee&quot; ## [11] &quot;vemployeedepartment&quot; &quot;vemployeedepartmenthistory&quot; ## [13] &quot;vjobcandidate&quot; &quot;vjobcandidateeducation&quot; ## [15] &quot;vjobcandidateemployment&quot; &quot;department&quot; ## [17] &quot;employeedepartmenthistory&quot; &quot;employeepayhistory&quot; Notice the way the database designers have abbreviated table names for your convenience. Once DBI::dbListFields(con, &quot;d&quot;) ## [1] &quot;id&quot; &quot;departmentid&quot; &quot;name&quot; &quot;groupname&quot; ## [5] &quot;modifieddate&quot; 8.2.2 Listing all the fields for all the tables The first example, DBI::dbListTables(con) returned 22 tables and the second example, DBI::dbListFields(con, &quot;employee&quot;) returns 7 fields. Here we combine the two calls to return a list of tables which has a list of all the fields in the table. The code block just shows the first two tables. table_columns &lt;- purrr::map(tables, ~ dbListFields(.,conn = con) ) Rename each list [[1]] … [[43]] to meaningful table name names(table_columns) &lt;- tables head(table_columns) ## named list() Later on we’ll discuss how to get more extensive data about each table and column from the database’s own store of metadata using a similar technique. As we go further the issue of scale will come up again and again: you need to be careful about how much data a call to the dbms will return, whether it’s a list of tables or a table that could have millions of rows. It’s important to connect with people who own, generate, or are the subjects of the data. A good chat with people who own the data, generate it, or are the subjects can generate insights and set the context for your investigation of the database. The purpose for collecting the data or circumstances where it was collected may be buried far afield in an organization, but usually someone knows. The metadata discussed in a later chapter is essential but will only take you so far. There are different ways of just looking at the data, which we explore below. 8.2.3 Downloading an entire table There are many different methods of getting data from a DBMS, and we’ll explore the different ways of controlling each one of them. DBI::dbReadTable will download an entire table into an R tibble. employee_tibble &lt;- DBI::dbReadTable(con, &quot;employee&quot;) str(employee_tibble) ## &#39;data.frame&#39;: 290 obs. of 15 variables: ## $ businessentityid: int 1 2 3 4 5 6 7 8 9 10 ... ## $ nationalidnumber: chr &quot;295847284&quot; &quot;245797967&quot; &quot;509647174&quot; &quot;112457891&quot; ... ## $ loginid : chr &quot;adventure-works\\\\ken0&quot; &quot;adventure-works\\\\terri0&quot; &quot;adventure-works\\\\roberto0&quot; &quot;adventure-works\\\\rob0&quot; ... ## $ jobtitle : chr &quot;Chief Executive Officer&quot; &quot;Vice President of Engineering&quot; &quot;Engineering Manager&quot; &quot;Senior Tool Designer&quot; ... ## $ birthdate : Date, format: &quot;1969-01-29&quot; &quot;1971-08-01&quot; ... ## $ maritalstatus : chr &quot;S&quot; &quot;S&quot; &quot;M&quot; &quot;S&quot; ... ## $ gender : chr &quot;M&quot; &quot;F&quot; &quot;M&quot; &quot;M&quot; ... ## $ hiredate : Date, format: &quot;2009-01-14&quot; &quot;2008-01-31&quot; ... ## $ salariedflag : logi TRUE TRUE TRUE FALSE TRUE TRUE ... ## $ vacationhours : int 99 1 2 48 5 6 61 62 63 16 ... ## $ sickleavehours : int 69 20 21 80 22 23 50 51 51 64 ... ## $ currentflag : logi TRUE TRUE TRUE TRUE TRUE TRUE ... ## $ rowguid : chr &quot;f01251e5-96a3-448d-981e-0f99d789110d&quot; &quot;45e8f437-670d-4409-93cb-f9424a40d6ee&quot; &quot;9bbbfb2c-efbb-4217-9ab7-f97689328841&quot; &quot;59747955-87b8-443f-8ed4-f8ad3afdf3a9&quot; ... ## $ modifieddate : POSIXct, format: &quot;2014-06-30 00:00:00&quot; &quot;2014-06-30 00:00:00&quot; ... ## $ organizationnode: chr &quot;/&quot; &quot;/1/&quot; &quot;/1/1/&quot; &quot;/1/1/1/&quot; ... That’s very simple, but if the table is large it may not be a good idea, since R is designed to keep the entire table in memory. Note that the first line of the str() output reports the total number of observations. 8.2.4 A table object that can be reused The dplyr::tbl function gives us more control over access to a table by enabling control over which columns and rows to download. It creates an object that might look like a data frame, but it’s actually a list object that dplyr uses for constructing queries and retrieving data from the DBMS. employee_table &lt;- dplyr::tbl(con, &quot;employee&quot;) class(employee_table) ## [1] &quot;tbl_PqConnection&quot; &quot;tbl_dbi&quot; &quot;tbl_sql&quot; ## [4] &quot;tbl_lazy&quot; &quot;tbl&quot; 8.2.5 Controlling the number of rows returned The collect function triggers the creation of a tibble and controls the number of rows that the DBMS sends to R. For more complex queries, the dplyr::collect() function provides a mechanism to indicate what’s processed on on the dbms server and what’s processed by R on the local machine. The chapter on Lazy Evaluation and Execution Environment discusses this issue in detail. employee_table %&gt;% dplyr::collect(n = 3) %&gt;% dim ## [1] 3 15 employee_table %&gt;% dplyr::collect(n = 500) %&gt;% dim ## [1] 290 15 8.2.6 Random rows from the dbms When the dbms contains many rows, a sample of the data may be plenty for your purposes. Although dplyr has nice functions to sample a data frame that’s already in R (e.g., the sample_n and sample_frac functions), to get a sample from the dbms we have to use dbGetQuery to send native SQL to the database. To peek ahead, here is one example of a query that retrieves 20 rows from a 1% sample: one_percent_sample &lt;- DBI::dbGetQuery( con, &quot;SELECT businessentityid, jobtitle, birthdate FROM employee TABLESAMPLE BERNOULLI(3) LIMIT 20; &quot; ) one_percent_sample ## businessentityid jobtitle birthdate ## 1 15 Design Engineer 1961-05-02 ## 2 65 Production Technician - WC60 1970-04-28 ## 3 102 Production Supervisor - WC10 1983-10-26 ## 4 177 Production Technician - WC30 1982-02-11 ## 5 180 Production Supervisor - WC20 1984-11-18 ## 6 235 Human Resources Manager 1976-02-11 ## 7 239 Benefits Specialist 1984-11-20 Exact sample of 100 records This technique depends on knowing the range of a record index, such as the businessentityid in the employee table of our adventureworks database. Start by finding the min and max values. DBI::dbListFields(con, &quot;employee&quot;) ## [1] &quot;businessentityid&quot; &quot;nationalidnumber&quot; &quot;loginid&quot; ## [4] &quot;jobtitle&quot; &quot;birthdate&quot; &quot;maritalstatus&quot; ## [7] &quot;gender&quot; &quot;hiredate&quot; &quot;salariedflag&quot; ## [10] &quot;vacationhours&quot; &quot;sickleavehours&quot; &quot;currentflag&quot; ## [13] &quot;rowguid&quot; &quot;modifieddate&quot; &quot;organizationnode&quot; employee_df &lt;- DBI::dbReadTable(con, &quot;employee&quot;) max(employee_df$businessentityid) ## [1] 290 min(employee_df$businessentityid) ## [1] 1 Set the random number seed and draw the sample. set.seed(123) sample_rows &lt;- sample(1:max(employee_df$businessentityid), 10) employee_table &lt;- dplyr::tbl(con, &quot;employee&quot;) Run query with the filter verb listing the randomly sampled rows to be retrieved: employee_sample &lt;- employee_table %&gt;% dplyr::filter(businessentityid %in% sample_rows) %&gt;% dplyr::collect() ## Warning: `overscope_eval_next()` is deprecated as of rlang 0.2.0. ## Please use `eval_tidy()` with a data mask instead. ## This warning is displayed once per session. ## Warning: `overscope_clean()` is deprecated as of rlang 0.2.0. ## This warning is displayed once per session. str(employee_sample) ## Classes &#39;tbl_df&#39;, &#39;tbl&#39; and &#39;data.frame&#39;: 10 obs. of 15 variables: ## $ businessentityid: int 13 84 118 129 150 156 228 253 254 269 ## $ nationalidnumber: chr &quot;486228782&quot; &quot;947029962&quot; &quot;222400012&quot; &quot;502058701&quot; ... ## $ loginid : chr &quot;adventure-works\\\\janice0&quot; &quot;adventure-works\\\\frank3&quot; &quot;adventure-works\\\\don0&quot; &quot;adventure-works\\\\gary0&quot; ... ## $ jobtitle : chr &quot;Tool Designer&quot; &quot;Production Technician - WC40&quot; &quot;Production Technician - WC50&quot; &quot;Production Technician - WC40&quot; ... ## $ birthdate : Date, format: &quot;1989-05-28&quot; &quot;1952-03-02&quot; ... ## $ maritalstatus : chr &quot;M&quot; &quot;M&quot; &quot;M&quot; &quot;S&quot; ... ## $ gender : chr &quot;F&quot; &quot;M&quot; &quot;M&quot; &quot;M&quot; ... ## $ hiredate : Date, format: &quot;2010-12-23&quot; &quot;2010-02-05&quot; ... ## $ salariedflag : logi FALSE FALSE FALSE FALSE FALSE FALSE ... ## $ vacationhours : int 8 66 88 69 35 11 92 56 57 72 ## $ sickleavehours : int 24 53 64 54 37 25 66 48 48 56 ## $ currentflag : logi TRUE TRUE TRUE TRUE TRUE TRUE ... ## $ rowguid : chr &quot;954b91b6-5aa7-48c2-8685-6e11c6e5c49a&quot; &quot;9af24acc-ea3e-4efe-b5e3-4762c496d57c&quot; &quot;e720053d-922e-4c91-b81a-a1ca4ef8bb0e&quot; &quot;a03d6052-1f85-4ebe-aac9-b67cfdcd91a6&quot; ... ## $ modifieddate : POSIXct, format: &quot;2014-06-30&quot; &quot;2014-06-30&quot; ... ## $ organizationnode: chr &quot;/1/1/5/2/&quot; &quot;/3/1/7/6/&quot; &quot;/3/1/11/10/&quot; &quot;/3/1/13/2/&quot; ... 8.2.7 Sub-setting variables A table in the dbms may not only have many more rows than you want, but also many more columns. The select command controls which columns are retrieved. employee_table %&gt;% dplyr::select(businessentityid, jobtitle, birthdate) %&gt;% head() ## # Source: lazy query [?? x 3] ## # Database: postgres [postgres@localhost:5432/adventureworks] ## businessentityid jobtitle birthdate ## &lt;int&gt; &lt;chr&gt; &lt;date&gt; ## 1 1 Chief Executive Officer 1969-01-29 ## 2 2 Vice President of Engineering 1971-08-01 ## 3 3 Engineering Manager 1974-11-12 ## 4 4 Senior Tool Designer 1974-12-23 ## 5 5 Design Engineer 1952-09-27 ## 6 6 Design Engineer 1959-03-11 That’s exactly equivalent to submitting the following SQL commands dirctly: DBI::dbGetQuery( con, &#39;SELECT &quot;businessentityid&quot;, &quot;jobtitle&quot;, &quot;birthdate&quot; FROM &quot;employee&quot; LIMIT 6&#39;) ## businessentityid jobtitle birthdate ## 1 1 Chief Executive Officer 1969-01-29 ## 2 2 Vice President of Engineering 1971-08-01 ## 3 3 Engineering Manager 1974-11-12 ## 4 4 Senior Tool Designer 1974-12-23 ## 5 5 Design Engineer 1952-09-27 ## 6 6 Design Engineer 1959-03-11 We won’t discuss dplyr methods for sub-setting variables, deriving new ones, or sub-setting rows based on the values found in the table, because they are covered well in other places, including: Comprehensive reference: https://dplyr.tidyverse.org/ Good tutorial: https://suzan.rbind.io/tags/dplyr/ In practice we find that, renaming variables is often quite important because the names in an SQL database might not meet your needs as an analyst. In “the wild”, you will find names that are ambiguous or overly specified, with spaces in them, and other problems that will make them difficult to use in R. It is good practice to do whatever renaming you are going to do in a predictable place like at the top of your code. The names in the adventureworks database are simple and clear, but if they were not, you might rename them for subsequent use in this way: tbl(con, &quot;employee&quot;) %&gt;% dplyr::rename(businessentity_id_number = businessentityid, employee_job_title = jobtitle) %&gt;% dplyr::select(businessentity_id_number, employee_job_title, birthdate) %&gt;% # head() show_query() ## &lt;SQL&gt; ## SELECT &quot;businessentityid&quot; AS &quot;businessentity_id_number&quot;, &quot;jobtitle&quot; AS &quot;employee_job_title&quot;, &quot;birthdate&quot; ## FROM &quot;employee&quot; That’s equivalent to the following SQL code: DBI::dbGetQuery( con, &#39;SELECT &quot;businessentityid&quot; AS &quot;businessentity_id_number&quot;, &quot;jobtitle&quot; AS &quot;employee_job_title&quot;, &quot;birthdate&quot; FROM &quot;employee&quot; LIMIT 6&#39; ) ## businessentity_id_number employee_job_title birthdate ## 1 1 Chief Executive Officer 1969-01-29 ## 2 2 Vice President of Engineering 1971-08-01 ## 3 3 Engineering Manager 1974-11-12 ## 4 4 Senior Tool Designer 1974-12-23 ## 5 5 Design Engineer 1952-09-27 ## 6 6 Design Engineer 1959-03-11 The one difference is that the SQL code returns a regular data frame and the dplyr code returns a tibble. Notice that the seconds are greyed out in the tibble display. 8.2.8 Translating dplyr code to SQL queries Where did the translations we’ve shown above come from? The show_query function shows how dplyr is translating your query to the dialect of the target dbms: employee_table %&gt;% dplyr::tally() %&gt;% dplyr::show_query() ## &lt;SQL&gt; ## SELECT COUNT(*) AS &quot;n&quot; ## FROM &quot;employee&quot; Here is an extensive discussion of how dplyr code is translated into SQL: https://dbplyr.tidyverse.org/articles/sql-translation.html If you prefer to use SQL directly, rather than dplyr, you can submit SQL code to the DBMS through the DBI::dbGetQuery function: DBI::dbGetQuery( con, &#39;SELECT COUNT(*) AS &quot;n&quot; FROM &quot;employee&quot; &#39; ) ## n ## 1 290 When you create a report to run repeatedly, you might want to put that query into R markdown. That way you can also execute that SQL code in a chunk with the following header: {sql, connection=con, output.var = &quot;query_results&quot;} SELECT COUNT(*) AS &quot;n&quot; FROM &quot;employee&quot;; Rmarkdown stores that query result in a tibble which can be printed by referring to it: query_results ## n ## 1 290 8.3 Mixing dplyr and SQL When dplyr finds code that it does not know how to translate into SQL, it will simply pass it along to the dbms. Therefore you can interleave native commands that your dbms will understand in the middle of dplyr code. Consider this example that’s derived from (Ruiz 2019): employee_table %&gt;% dplyr::select_at(vars(jobtitle, contains(&quot;hours&quot;))) %&gt;% dplyr::mutate(today = now()) %&gt;% dplyr::show_query() ## &lt;SQL&gt; ## SELECT &quot;jobtitle&quot;, &quot;vacationhours&quot;, &quot;sickleavehours&quot;, CURRENT_TIMESTAMP AS &quot;today&quot; ## FROM &quot;employee&quot; That is native to PostgreSQL, not ANSI standard SQL. Verify that it works: employee_table %&gt;% dplyr::select_at(vars(jobtitle, contains(&quot;hours&quot;))) %&gt;% head() %&gt;% dplyr::mutate(today = now()) %&gt;% dplyr::collect() ## # A tibble: 6 x 4 ## jobtitle vacationhours sickleavehours today ## &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;dttm&gt; ## 1 Chief Executive Officer 99 69 2019-07-12 17:37:06 ## 2 Vice President of Engin… 1 20 2019-07-12 17:37:06 ## 3 Engineering Manager 2 21 2019-07-12 17:37:06 ## 4 Senior Tool Designer 48 80 2019-07-12 17:37:06 ## 5 Design Engineer 5 22 2019-07-12 17:37:06 ## 6 Design Engineer 6 23 2019-07-12 17:37:06 8.4 Examining a single table with R Dealing with a large, complex database highlights the utility of specific tools in R. We include brief examples that we find to be handy: Base R structure: str Printing out some of the data: datatable, kable, and View Summary statistics: summary glimpse in the tibble package, which is included in the tidyverse skim in the skimr package 8.4.1 str - a base package workhorse str is a workhorse function that lists variables, their type and a sample of the first few variable values. str(employee_tibble) ## &#39;data.frame&#39;: 290 obs. of 15 variables: ## $ businessentityid: int 1 2 3 4 5 6 7 8 9 10 ... ## $ nationalidnumber: chr &quot;295847284&quot; &quot;245797967&quot; &quot;509647174&quot; &quot;112457891&quot; ... ## $ loginid : chr &quot;adventure-works\\\\ken0&quot; &quot;adventure-works\\\\terri0&quot; &quot;adventure-works\\\\roberto0&quot; &quot;adventure-works\\\\rob0&quot; ... ## $ jobtitle : chr &quot;Chief Executive Officer&quot; &quot;Vice President of Engineering&quot; &quot;Engineering Manager&quot; &quot;Senior Tool Designer&quot; ... ## $ birthdate : Date, format: &quot;1969-01-29&quot; &quot;1971-08-01&quot; ... ## $ maritalstatus : chr &quot;S&quot; &quot;S&quot; &quot;M&quot; &quot;S&quot; ... ## $ gender : chr &quot;M&quot; &quot;F&quot; &quot;M&quot; &quot;M&quot; ... ## $ hiredate : Date, format: &quot;2009-01-14&quot; &quot;2008-01-31&quot; ... ## $ salariedflag : logi TRUE TRUE TRUE FALSE TRUE TRUE ... ## $ vacationhours : int 99 1 2 48 5 6 61 62 63 16 ... ## $ sickleavehours : int 69 20 21 80 22 23 50 51 51 64 ... ## $ currentflag : logi TRUE TRUE TRUE TRUE TRUE TRUE ... ## $ rowguid : chr &quot;f01251e5-96a3-448d-981e-0f99d789110d&quot; &quot;45e8f437-670d-4409-93cb-f9424a40d6ee&quot; &quot;9bbbfb2c-efbb-4217-9ab7-f97689328841&quot; &quot;59747955-87b8-443f-8ed4-f8ad3afdf3a9&quot; ... ## $ modifieddate : POSIXct, format: &quot;2014-06-30 00:00:00&quot; &quot;2014-06-30 00:00:00&quot; ... ## $ organizationnode: chr &quot;/&quot; &quot;/1/&quot; &quot;/1/1/&quot; &quot;/1/1/1/&quot; ... 8.4.2 Always look at your data with head, View, or kable There is no substitute for looking at your data and R provides several ways to just browse it. The head function controls the number of rows that are displayed. Note that tail does not work against a database object. In every-day practice you would look at more than the default 6 rows, but here we wrap head around the data frame: sqlpetr::sp_print_df(head(employee_tibble)) 8.4.3 The summary function in base The base package’s summary function provides basic statistics that serve a unique diagnostic purpose in this context. For example, the following output shows that: * `businessentityid` is a number from 1 to 16,049. In a previous section, we ran the `str` function and saw that there are 16,044 observations in this table. Therefore, the `businessentityid` seems to be sequential from 1:16049, but there are 5 values missing from that sequence. _Exercise for the Reader_: Which 5 values from 1:16049 are missing from `businessentityid` values in the `employee` table? (_Hint_: In the chapter on SQL Joins, you will learn the functions needed to answer this question.) * The number of NA&#39;s in the `return_date` column is a good first guess as to the number of DVDs rented out or lost as of 2005-09-02 02:35:22. summary(employee_tibble) ## businessentityid nationalidnumber loginid jobtitle ## Min. : 1.00 Length:290 Length:290 Length:290 ## 1st Qu.: 73.25 Class :character Class :character Class :character ## Median :145.50 Mode :character Mode :character Mode :character ## Mean :145.50 ## 3rd Qu.:217.75 ## Max. :290.00 ## birthdate maritalstatus gender ## Min. :1951-10-17 Length:290 Length:290 ## 1st Qu.:1973-09-21 Class :character Class :character ## Median :1978-10-19 Mode :character Mode :character ## Mean :1978-07-04 ## 3rd Qu.:1986-05-27 ## Max. :1991-05-31 ## hiredate salariedflag vacationhours sickleavehours ## Min. :2006-06-30 Mode :logical Min. : 0.00 Min. :20.00 ## 1st Qu.:2008-12-26 FALSE:238 1st Qu.:26.25 1st Qu.:33.00 ## Median :2009-02-02 TRUE :52 Median :51.00 Median :46.00 ## Mean :2009-05-19 Mean :50.61 Mean :45.31 ## 3rd Qu.:2009-10-09 3rd Qu.:75.00 3rd Qu.:58.00 ## Max. :2013-05-30 Max. :99.00 Max. :80.00 ## currentflag rowguid modifieddate ## Mode:logical Length:290 Min. :2014-06-30 00:00:00 ## TRUE:290 Class :character 1st Qu.:2014-06-30 00:00:00 ## Mode :character Median :2014-06-30 00:00:00 ## Mean :2014-07-01 20:32:52 ## 3rd Qu.:2014-06-30 00:00:00 ## Max. :2014-12-26 09:17:08 ## organizationnode ## Length:290 ## Class :character ## Mode :character ## ## ## So the summary function is surprisingly useful as we first start to look at the table contents. 8.4.4 The glimpse function in the tibble package The tibble package’s glimpse function is a more compact version of str: tibble::glimpse(employee_tibble) ## Observations: 290 ## Variables: 15 ## $ businessentityid &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, … ## $ nationalidnumber &lt;chr&gt; &quot;295847284&quot;, &quot;245797967&quot;, &quot;509647174&quot;, &quot;1124578… ## $ loginid &lt;chr&gt; &quot;adventure-works\\\\ken0&quot;, &quot;adventure-works\\\\terr… ## $ jobtitle &lt;chr&gt; &quot;Chief Executive Officer&quot;, &quot;Vice President of E… ## $ birthdate &lt;date&gt; 1969-01-29, 1971-08-01, 1974-11-12, 1974-12-23… ## $ maritalstatus &lt;chr&gt; &quot;S&quot;, &quot;S&quot;, &quot;M&quot;, &quot;S&quot;, &quot;M&quot;, &quot;M&quot;, &quot;M&quot;, &quot;S&quot;, &quot;M&quot;, &quot;M… ## $ gender &lt;chr&gt; &quot;M&quot;, &quot;F&quot;, &quot;M&quot;, &quot;M&quot;, &quot;F&quot;, &quot;M&quot;, &quot;M&quot;, &quot;F&quot;, &quot;F&quot;, &quot;M… ## $ hiredate &lt;date&gt; 2009-01-14, 2008-01-31, 2007-11-11, 2007-12-05… ## $ salariedflag &lt;lgl&gt; TRUE, TRUE, TRUE, FALSE, TRUE, TRUE, TRUE, TRUE… ## $ vacationhours &lt;int&gt; 99, 1, 2, 48, 5, 6, 61, 62, 63, 16, 7, 9, 8, 3,… ## $ sickleavehours &lt;int&gt; 69, 20, 21, 80, 22, 23, 50, 51, 51, 64, 23, 24,… ## $ currentflag &lt;lgl&gt; TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE,… ## $ rowguid &lt;chr&gt; &quot;f01251e5-96a3-448d-981e-0f99d789110d&quot;, &quot;45e8f4… ## $ modifieddate &lt;dttm&gt; 2014-06-30, 2014-06-30, 2014-06-30, 2014-06-30… ## $ organizationnode &lt;chr&gt; &quot;/&quot;, &quot;/1/&quot;, &quot;/1/1/&quot;, &quot;/1/1/1/&quot;, &quot;/1/1/2/&quot;, &quot;/1/… 8.4.5 The skim function in the skimr package The skimr package has several functions that make it easy to examine an unknown data frame and assess what it contains. It is also extensible. library(skimr) ## ## Attaching package: &#39;skimr&#39; ## The following object is masked from &#39;package:knitr&#39;: ## ## kable ## The following object is masked from &#39;package:stats&#39;: ## ## filter # skimr::skim(employee_tibble) # skimr::skim_to_wide(employee_tibble) #skimr doesn&#39;t like certain kinds of columns 8.4.6 Close the connection and shut down adventureworks Where you place the collect function matters. DBI::dbDisconnect(con) sqlpetr::sp_docker_stop(&quot;adventureworks&quot;) 8.5 Additional reading (Wickham 2018) (Baumer 2018) References "],
["chapter-appendix-postresql-authentication.html", "A Appendix C - PostgreSQL Authentication A.1 Introduction A.2 Password authentication on the PostgreSQL Docker image A.3 Adding roles", " A Appendix C - PostgreSQL Authentication A.1 Introduction PostgreSQL has a very robust and flexible set of authentication methods (PostgreSQL Global Development Group 2018a). In most production environments, these will be managed by the database administrator (DBA) on a need-to-access basis. People and programs will be granted access only to a minimum set of capabilities required to function, and nothing more. In this book, we are using a PostgreSQL Docker image (Docker 2018c). When we create a container from that image, we use its native mechanism to create the postgres database superuser with a password specified in an R environment file ~/.Renviron. See Securing and using your dbms log-in credentials for how we do this. What that means is that you are the DBA - the database superuser - for the PostgreSQL database cluster running in the container! You can create and destroy databases, schemas, tables, views, etc. You can also create and destroy users - called roles in PostgreSQL, and GRANT or REVOKE their privileges with great precision. You don’t have to do that to use this book. But if you want to experiment with it, feel free! A.2 Password authentication on the PostgreSQL Docker image Of the many PostgreSQL authentication mechanisms, the simplest that’s universallly available is password authentication (PostgreSQL Global Development Group 2018c). That’s what we use for the postgres database superuser, and what we recommend for any roles you may create. Once a role has been created, you need five items to open a connection to the PostgreSQL database cluster: The host. This is a name or IP address that your network can access. In this book, with the database running in a Docker container, that’s usually localhost. The port. This is the port the server is listening on. It’s usually the default, 5439, and that’s what we use. But in a secure environment, it will often be some random number to lower the chances that an attacker can find the database server. And if you have more than one server on the network, you’ll need to use different ports for each of them. The dbname to connect to. This database must exist or the connection attempt will fail. The user. This user must exist in the database cluster and be allowed to access the database. We are using the database superuser postgres in this book. The password. This is set by the DBA for the user. In this book we use the password defined in Securing and using your dbms log-in credentials. A.3 Adding roles As noted above, PostgreSQL has a very flexible fine-grained access permissions system. We can’t cover all of it; see PostgreSQL Global Development Group (2018b) for the full details. But we can give an example. A.3.1 Setting up Docker First, we need to make sure we don’t have any other databases listening on the default port 5439. sqlpetr::sp_check_that_docker_is_up() ## [1] &quot;Docker is up but running no containers&quot; sqlpetr::sp_docker_remove_container(&quot;cattle&quot;) ## [1] 0 # in case you&#39;ve been doing things out of order, stop a container named &#39;adventureworks&#39; if it exists: sqlpetr::sp_docker_stop(&quot;adventureworks&quot;) A.3.2 Creating a new container We’ll create a “cattle” container with a default PostgreSQL 10 database cluster. sqlpetr::sp_make_simple_pg(&quot;cattle&quot;) con &lt;- sqlpetr::sp_get_postgres_connection( host = &quot;localhost&quot;, port = 5439, dbname = &quot;postgres&quot;, user = &quot;postgres&quot;, password = &quot;postgres&quot;, seconds_to_test = 30 ) A.3.3 Adding a role Now, let’s add a role. We’ll add a role that can log in and create databases, but isn’t a superuser. Since this is a demo and not a real production database cluster, we’ll specify a password in plaintext. And we’ll create a database for our new user. Create the role: DBI::dbExecute( con, &quot;CREATE ROLE charlie LOGIN CREATEDB PASSWORD &#39;chaplin&#39;;&quot; ) ## [1] 0 Create the database: DBI::dbExecute( con, &quot;CREATE DATABASE charlie OWNER = charlie&quot;) ## [1] 0 A.3.4 Did it work? DBI::dbDisconnect(con) con &lt;- sqlpetr::sp_get_postgres_connection( host = &quot;localhost&quot;, port = 5439, dbname = &quot;postgres&quot;, user = &quot;charlie&quot;, password = &quot;chaplin&quot;, seconds_to_test = 30 ) OK, we can connect. Let’s do some stuff! data(&quot;iris&quot;) dbCreateTable creates the table with columns matching the data frame. But it does not send data to the table. DBI::dbCreateTable(con, &quot;iris&quot;, iris) To send data, we use dbAppendTable. DBI::dbAppendTable(con, &quot;iris&quot;, iris) ## Warning: Factors converted to character ## [1] 150 DBI::dbListTables(con) ## [1] &quot;iris&quot; head(DBI::dbReadTable(con, &quot;iris&quot;)) ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## 1 5.1 3.5 1.4 0.2 setosa ## 2 4.9 3.0 1.4 0.2 setosa ## 3 4.7 3.2 1.3 0.2 setosa ## 4 4.6 3.1 1.5 0.2 setosa ## 5 5.0 3.6 1.4 0.2 setosa ## 6 5.4 3.9 1.7 0.4 setosa DBI::dbDisconnect(con) A.3.5 Remove the container sqlpetr::sp_docker_remove_container(&quot;cattle&quot;) ## [1] 0 References "],
["chapter-appendix-dbi-index.html", "B DBI package functions - INDEX", " B DBI package functions - INDEX Where are these covered and should the by included? DBI 1st time Call Example/Notes DBIConnct 6.3.2 (04) in sp_get_postgres_connection dbAppendTable dbCreateTable dbDisconnect 6.4n (04) dbDisconnect(con) dbExecute 10.4.2 (13) Executes a statement and returns the number of rows affected. dbExecute() comes with a default implementation (which should work with most backends) that calls dbSendStatement(), then dbGetRowsAffected(), ensuring that the result is always free-d by dbClearResult(). dbExistsTable dbExistsTable(con,‘actor’) dbFetch 17.1 (72) dbFetch(rs) dbGetException dbGetInfo dbGetInfo(con) dbGetQuery 10.4.1 (13) dbGetQuery(con,‘select * from store;’) dbIsReadOnly dbIsReadOnly(con) dbIsValid dbIsValid(con) dbListFields 6.3.3 (04) DBI::dbListFields(con, “mtcars”) dbListObjects dbListObjects(con) dbListTables 6.3.2 (04) DBI::dbListTables(con, con) dbReadTable 8.1.2 DBI::dbReadTable(con, “rental”) dbRemoveTable dbSendQuery 17.1 (72) rs &lt;- dbSendQuery(con, “SELECT * FROM mtcars WHERE cyl = 4”) dbSendStatement The dbSendStatement() method only submits and synchronously executes the SQL data manipulation statement (e.g., UPDATE, DELETE, INSERT INTO, DROP TABLE, …) to the database engine. dbWriteTable 6.3.3 (04) dbWriteTable(con, “mtcars”, mtcars, overwrite = TRUE) "],
["chapter-appendix-dplyr-to-postres-translation.html", "C Appendix _ Dplyr to SQL translations C.1 Overview", " C Appendix _ Dplyr to SQL translations You may be interested in exactly how the DBI package translates R functions into their SQL quivalents – and in which functions are translated and which are not. This Appendix answers those questions. It is based on the work of Dewey Dunnington ((???)(http://twitter.com/paleolimbot)) which he published here: https://apps.fishandwhistle.net/archives/1503 https://rud.is/b/2019/04/10/lost-in-sql-translation-charting-dbplyr-mapped-sql-function-support-across-all-backends/ C.1 Overview These packages are called below: library(tidyverse) library(dbplyr) library(gt) library(here) library(sqlpetr) list the DBI functions that are available: names(sql_translate_env(simulate_dbi())) ## [1] &quot;-&quot; &quot;:&quot; &quot;!&quot; ## [4] &quot;!=&quot; &quot;(&quot; &quot;[&quot; ## [7] &quot;[[&quot; &quot;{&quot; &quot;*&quot; ## [10] &quot;/&quot; &quot;&amp;&quot; &quot;&amp;&amp;&quot; ## [13] &quot;%%&quot; &quot;%&gt;%&quot; &quot;%in%&quot; ## [16] &quot;^&quot; &quot;+&quot; &quot;&lt;&quot; ## [19] &quot;&lt;=&quot; &quot;==&quot; &quot;&gt;&quot; ## [22] &quot;&gt;=&quot; &quot;|&quot; &quot;||&quot; ## [25] &quot;$&quot; &quot;abs&quot; &quot;acos&quot; ## [28] &quot;as_date&quot; &quot;as_datetime&quot; &quot;as.character&quot; ## [31] &quot;as.Date&quot; &quot;as.double&quot; &quot;as.integer&quot; ## [34] &quot;as.integer64&quot; &quot;as.logical&quot; &quot;as.numeric&quot; ## [37] &quot;as.POSIXct&quot; &quot;asin&quot; &quot;atan&quot; ## [40] &quot;atan2&quot; &quot;between&quot; &quot;bitwAnd&quot; ## [43] &quot;bitwNot&quot; &quot;bitwOr&quot; &quot;bitwShiftL&quot; ## [46] &quot;bitwShiftR&quot; &quot;bitwXor&quot; &quot;c&quot; ## [49] &quot;case_when&quot; &quot;ceil&quot; &quot;ceiling&quot; ## [52] &quot;coalesce&quot; &quot;cos&quot; &quot;cosh&quot; ## [55] &quot;cot&quot; &quot;coth&quot; &quot;day&quot; ## [58] &quot;desc&quot; &quot;exp&quot; &quot;floor&quot; ## [61] &quot;hour&quot; &quot;if&quot; &quot;if_else&quot; ## [64] &quot;ifelse&quot; &quot;is.na&quot; &quot;is.null&quot; ## [67] &quot;log&quot; &quot;log10&quot; &quot;mday&quot; ## [70] &quot;minute&quot; &quot;month&quot; &quot;na_if&quot; ## [73] &quot;nchar&quot; &quot;now&quot; &quot;paste&quot; ## [76] &quot;paste0&quot; &quot;pmax&quot; &quot;pmin&quot; ## [79] &quot;qday&quot; &quot;round&quot; &quot;second&quot; ## [82] &quot;sign&quot; &quot;sin&quot; &quot;sinh&quot; ## [85] &quot;sql&quot; &quot;sqrt&quot; &quot;str_c&quot; ## [88] &quot;str_conv&quot; &quot;str_count&quot; &quot;str_detect&quot; ## [91] &quot;str_dup&quot; &quot;str_extract&quot; &quot;str_extract_all&quot; ## [94] &quot;str_flatten&quot; &quot;str_glue&quot; &quot;str_glue_data&quot; ## [97] &quot;str_interp&quot; &quot;str_length&quot; &quot;str_locate&quot; ## [100] &quot;str_locate_all&quot; &quot;str_match&quot; &quot;str_match_all&quot; ## [103] &quot;str_order&quot; &quot;str_pad&quot; &quot;str_remove&quot; ## [106] &quot;str_remove_all&quot; &quot;str_replace&quot; &quot;str_replace_all&quot; ## [109] &quot;str_replace_na&quot; &quot;str_sort&quot; &quot;str_split&quot; ## [112] &quot;str_split_fixed&quot; &quot;str_squish&quot; &quot;str_sub&quot; ## [115] &quot;str_subset&quot; &quot;str_to_lower&quot; &quot;str_to_title&quot; ## [118] &quot;str_to_upper&quot; &quot;str_trim&quot; &quot;str_trunc&quot; ## [121] &quot;str_view&quot; &quot;str_view_all&quot; &quot;str_which&quot; ## [124] &quot;str_wrap&quot; &quot;substr&quot; &quot;switch&quot; ## [127] &quot;tan&quot; &quot;tanh&quot; &quot;today&quot; ## [130] &quot;tolower&quot; &quot;toupper&quot; &quot;trimws&quot; ## [133] &quot;wday&quot; &quot;xor&quot; &quot;yday&quot; ## [136] &quot;year&quot; &quot;cume_dist&quot; &quot;cummax&quot; ## [139] &quot;cummean&quot; &quot;cummin&quot; &quot;cumsum&quot; ## [142] &quot;dense_rank&quot; &quot;first&quot; &quot;lag&quot; ## [145] &quot;last&quot; &quot;lead&quot; &quot;max&quot; ## [148] &quot;mean&quot; &quot;median&quot; &quot;min&quot; ## [151] &quot;min_rank&quot; &quot;n&quot; &quot;n_distinct&quot; ## [154] &quot;nth&quot; &quot;ntile&quot; &quot;order_by&quot; ## [157] &quot;percent_rank&quot; &quot;quantile&quot; &quot;rank&quot; ## [160] &quot;row_number&quot; &quot;sum&quot; &quot;var&quot; ## [163] &quot;cume_dist&quot; &quot;cummax&quot; &quot;cummean&quot; ## [166] &quot;cummin&quot; &quot;cumsum&quot; &quot;dense_rank&quot; ## [169] &quot;first&quot; &quot;lag&quot; &quot;last&quot; ## [172] &quot;lead&quot; &quot;max&quot; &quot;mean&quot; ## [175] &quot;median&quot; &quot;min&quot; &quot;min_rank&quot; ## [178] &quot;n&quot; &quot;n_distinct&quot; &quot;nth&quot; ## [181] &quot;ntile&quot; &quot;order_by&quot; &quot;percent_rank&quot; ## [184] &quot;quantile&quot; &quot;rank&quot; &quot;row_number&quot; ## [187] &quot;sum&quot; &quot;var&quot; sql_translate_env(simulate_dbi()) ## &lt;sql_variant&gt; ## scalar: -, :, !, !=, (, [, [[, {, *, /, &amp;, &amp;&amp;, %%, %&gt;%, %in%, ## scalar: ^, +, &lt;, &lt;=, ==, &gt;, &gt;=, |, ||, $, abs, acos, as_date, ## scalar: as_datetime, as.character, as.Date, as.double, ## scalar: as.integer, as.integer64, as.logical, as.numeric, ## scalar: as.POSIXct, asin, atan, atan2, between, bitwAnd, ## scalar: bitwNot, bitwOr, bitwShiftL, bitwShiftR, bitwXor, c, ## scalar: case_when, ceil, ceiling, coalesce, cos, cosh, cot, ## scalar: coth, day, desc, exp, floor, hour, if, if_else, ifelse, ## scalar: is.na, is.null, log, log10, mday, minute, month, na_if, ## scalar: nchar, now, paste, paste0, pmax, pmin, qday, round, ## scalar: second, sign, sin, sinh, sql, sqrt, str_c, str_conv, ## scalar: str_count, str_detect, str_dup, str_extract, ## scalar: str_extract_all, str_flatten, str_glue, str_glue_data, ## scalar: str_interp, str_length, str_locate, str_locate_all, ## scalar: str_match, str_match_all, str_order, str_pad, ## scalar: str_remove, str_remove_all, str_replace, ## scalar: str_replace_all, str_replace_na, str_sort, str_split, ## scalar: str_split_fixed, str_squish, str_sub, str_subset, ## scalar: str_to_lower, str_to_title, str_to_upper, str_trim, ## scalar: str_trunc, str_view, str_view_all, str_which, str_wrap, ## scalar: substr, switch, tan, tanh, today, tolower, toupper, ## scalar: trimws, wday, xor, yday, year ## aggregate: cume_dist, cummax, cummean, cummin, cumsum, dense_rank, ## aggregate: first, lag, last, lead, max, mean, median, min, ## aggregate: min_rank, n, n_distinct, nth, ntile, order_by, ## aggregate: percent_rank, quantile, rank, row_number, sum, var ## window: cume_dist, cummax, cummean, cummin, cumsum, dense_rank, ## window: first, lag, last, lead, max, mean, median, min, ## window: min_rank, n, n_distinct, nth, ntile, order_by, ## window: percent_rank, quantile, rank, row_number, sum, var source(here(&quot;book-src&quot;, &quot;dbplyr-sql-function-translation.R&quot;)) ## Warning: `overscope_eval_next()` is deprecated as of rlang 0.2.0. ## Please use `eval_tidy()` with a data mask instead. ## This warning is displayed once per session. ## Warning: `overscope_clean()` is deprecated as of rlang 0.2.0. ## This warning is displayed once per session. ## Warning: `.drop` is deprecated. All list-columns are now preserved. Each of the following dbplyr back ends may have a slightly different translation: translations %&gt;% filter(!is.na(sql)) %&gt;% count(variant) ## # A tibble: 11 x 2 ## variant n ## &lt;chr&gt; &lt;int&gt; ## 1 access 193 ## 2 dbi 183 ## 3 hive 156 ## 4 impala 190 ## 5 mssql 194 ## 6 mysql 194 ## 7 odbc 186 ## 8 oracle 184 ## 9 postgres 204 ## 10 sqlite 134 ## 11 teradata 196 Only one postgres translation produces an output: psql &lt;- translations %&gt;% filter(!is.na(sql), variant == &quot;postgres&quot;) %&gt;% select(r, n_args, sql) %&gt;% arrange(r) # sp_print_df(head(psql, n = 40)) sp_print_df(psql) "],
["chapter-appendix-additional-resources.html", "D Appendix Additional resources D.1 Editing this book D.2 Docker alternatives D.3 Docker and R D.4 Documentation for Docker and PostgreSQL D.5 SQL and dplyr D.6 More Resources", " D Appendix Additional resources D.1 Editing this book Here are instructions for editing this book D.2 Docker alternatives Choosing between Docker and Vagrant (Zait 2017) D.3 Docker and R Noam Ross’ talk on Docker for the UseR (Ross 2018b) and his Slides (Ross 2018a) give a lot of context and tips. Good Docker tutorials An introductory Docker tutorial (Srivastav 2018) A Docker curriculum (Hall 2018) Scott Came’s materials about Docker and R on his website (Came 2018) and at the 2018 UseR Conference focus on R inside Docker. It’s worth studying the ROpensci Docker tutorial (ROpenSciLabs 2018) D.4 Documentation for Docker and PostgreSQL The Postgres image documentation (Docker 2018c) PostgreSQL &amp; Docker documentation (Docker 2018c) Dockerize PostgreSQL (Docker 2018b) Usage examples of PostgreSQL with Docker WARNING-EXPIRED CERTIFICATE 2018-12-20 D.5 SQL and dplyr Why SQL is not for analysis but dplyr is (Nishida 2016) Data Manipulation with dplyr (With 50 Examples) (ListenData.com 2016) D.6 More Resources David Severski describes some key elements of connecting to databases with R for MacOS users (Severski 2018) This tutorial picks up ideas and tips from Ed Borasky’s Data Science pet containers (Borasky 2018), which creates a framework based on that Hack Oregon example and explains why this repo is named pet-sql. References "],
["references.html", "References", " References "]
]
