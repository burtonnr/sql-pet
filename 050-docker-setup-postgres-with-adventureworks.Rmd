# Create the adventureworks database in PostgreSQL in Docker {#chapter_setup-adventureworks-db}

> NOTE: This chapter doesn't go into the details of *creating* or *restoring* the `adventureworks` database.  For more detail on what's going on behind the scenes, you can examine the step-by-step code in:
>
> ` source('book-src/restore-adventureworks-postgres-on-docker.R') `

> This chapter demonstrates how to:
>
>  * Setup the `adventureworks` database in Docker
>  * Stop and start Docker container to demonstrate persistence
>  * Connect to and disconnect R from the `adventureworks` database
>  * Set up the environment for subsequent chapters

## Overview

In the last chapter we connected to PostgreSQL from R.  Now we set up a "realistic" database named `adventureworks`. There are different approaches to doing this: this chapter sets it up in a way that doesn't show all the Docker details.

These packages are called in this Chapter:
```{r setup, echo=TRUE, message=FALSE, warning=FALSE}
library(tidyverse)
library(DBI)
library(RPostgres)
library(glue)
require(knitr)
library(dbplyr)
library(sqlpetr)
library(bookdown)
library(here)
```

## Verify that Docker is up and running
```{r docker verify}
sp_check_that_docker_is_up()
```

## Clean up if appropriate
Force-remove the `adventureworks` container if it exist (e.g., from a prior runs):
```{r}
sp_docker_remove_container("adventureworks")
```
## Build the adventureworks Docker image

**UPDATE:** For the rest of the book we will be using a Docker image called
`adventureworks`. To save space here in the book, we've created a function
in `sqlpetr` to build this image, called [`sp_make_dvdrental_image`](https://smithjd.github.io/sqlpetr/reference/sp_make_dvdrental_image.html). Vignette [Building the `hsrample` Docker Image
](https://smithjd.github.io/sqlpetr/articles/building-the-dvdrental-docker-image.html) describes the build process.

```{r}
# sp_make_dvdrental_image("postgres-dvdrental")
source(here("book-src", "restore-adventureworks-postgres-on-docker.R"))
```

**UPDATE:** Did it work? We have a function that lists the images into a tibble!

```{r}
sp_docker_start("adventureworks")
sp_docker_images_tibble()  # Doesn't produce the expected output.

```

## Run the adventureworks Docker Image
**UPDATE:** Now we can run the image in a container and connect to the database. To run the
image we use an `sqlpetr` function called [`sp_pg_docker_run`](https://smithjd.github.io/sqlpetr/reference/sp_pg_docker_run.html)

```{r eval= FALSE}
# sp_pg_docker_run(
#   container_name = "adventureworks",
#   image_tag = "adventureworks",
#   postgres_password = "postgres"
# )
```

**UPDATE:** Did it work?
```{r}

sp_docker_containers_tibble()

```

## Connect to PostgreSQL with R

Use the DBI package to connect to the `adventureworks` database in PostgreSQL.  Remember the settings discussion about [keeping passwords hidden][Pause for some security considerations]

```{r }
con <- sp_get_postgres_connection(
  host = "localhost",
  port = 5432,
  user = "postgres",
  password = "postgres",
  dbname = "adventureworks",
  seconds_to_test = 20, connection_tab = TRUE
)
```
For the moment we by-pass some complexity that results from the fact that the `adventureworks` database has multiple *schemas* and that we are interested in only one of them, named `information_schema`.  
```{r}
tbl(con, in_schema("information_schema", "schemata")) %>%
  select(catalog_name, schema_name, schema_owner) %>%
  collect()

```

Schemas will be discussed later on because multiple schemas are the norm in an enterprise database environment, but they are a side issue at this point.  So we switch the order in which PostgreSQL searches for objects with the following SQL code:
```{r}
dbExecute(con, "set search_path to sales, public;")

```
With the custom `search_path`, the following command works, but it will fail without out it.
```{r}
dbListTables(con)
```
Same for `dbListFields`:
```{r }
dbListFields(con, "salesperson")
```

Thus with this search order, the following two produce identical results:
```{r }
tbl(con, in_schema("sales", "salesperson")) %>%
  head()

tbl(con, "salesperson") %>%
  head()

```

## `dplyr` connection objects
As introduced in the previous chapter, the `dplyr::tbl` function creates an object that might **look** like a data frame in that when you enter it on the command line, it prints a bunch of rows from the dbms table.  But it is actually a **list** object that `dplyr` uses for constructing queries and retrieving data from the DBMS.  

The following code illustrates these issues.  The `dplyr::tbl` function creates the connection object that we store in an object named `person_table`:
```{r}
person_table <- dplyr::tbl(con, in_schema("person", "person")) %>% 
  select(-rowguid) %>% 
  rename(personal_details_updated = modifieddate)
```

At first glance, it _acts_ like a data frame when you print it, although it only prints 10 of the table's 1,000 rows:
```{r}
person_table
```

However, notice that the first output line shows `??`, rather than providing the number of rows in the table. Similarly, the next to last line shows:
```
    … with more rows, and 8 more variables
```
whereas the output for a normal `tbl` of this film data would say:
```
    … with more 1,000, and 8 more variables
```

So even though `person_table` is a `tbl`, it's **also** a `tbl_PqConnection`:
```{r}
class(person_table)
```

It is not just a normal `tbl` of data. We can see that from the structure of `person_table`:
```{r}
str(person_table)
```

It has only _two_ rows!  The first row contains all the information in the `con` object, which contains information about all the tables and objects in the database:
```{r}
person_table$src$con@typnames$typname[380:437]
```
The second row contains a list of the columns in the `film` table, among other things:
```{r}
person_table$ops$vars
```
`person_table` holds information needed to get the data from the 'film' table, but `person_table` does not hold the data itself. In the following sections, we will examine more closely this relationship between the `person_table` object and the data in the database's 'film' table.

Disconnect from the database:
```{r }
dbDisconnect(con)

```
## Stop and start to demonstrate persistence

Stop the container:
```{r}
sp_docker_stop("adventureworks")
sp_docker_containers_tibble()
```

When we stopped `adventureworks`, it no longer appeared in the tibble. But the
container is still there. `sp_docker_containers_tibble` by default only lists
the *running* containers. But we can use the `list_all` option and see it:

```{r}
sp_docker_containers_tibble(list_all = TRUE)
```


Restart the container and verify that the adventureworks tables are still there:
```{r}
sp_docker_start("adventureworks")
sp_docker_containers_tibble()
```
Connect to the `adventureworks` database in PostgreSQL:
```{r}
con <- sp_get_postgres_connection(
  host = "localhost",
  port = 5432,
  user = "postgres",
  password = "postgres",
  dbname = "adventureworks",
  seconds_to_test = 30
)
```

Check that you can still see the first few rows of the `employeeinfo` table:
```{r}
tbl(con, in_schema("sales", "salesperson")) %>%
  head()

```

## Cleaning up

Always have R disconnect from the database when you're done.
```{r}

dbDisconnect(con)

```

Stop the `adventureworks` container:
```{r}
sp_docker_stop("adventureworks")
```
Show that the container still exists even though it's not running

```{r}
sp_show_all_docker_containers()

```

Next time, you can just use this command to start the container: 

> `sp_docker_start("adventureworks")`

And once stopped, the container can be removed with:

> `sp_check_that_docker_is_up("adventureworks")`

## Using the `adventureworks` container in the rest of the book

After this point in the book, we assume that Docker is up and that we can always start up our *adventureworks database* with:

> `sp_docker_start("adventureworks")`

